## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation 


# KNIME Documentation
Read or download documentation for KNIME Software
KNIME Analytics Platform
KNIME Extensions
KNIME Hub
KNIME Edge
KNIME Server
Version 3.6Version 3.7Version 4.0Version 4.1Version 4.2Version 4.3Version 4.4Version 4.5Version 4.6Version 4.7Version 5.1Version 5.2Version 5.3Version 5.4 (latest)Version 5.4 (latest)
KNIME Analytics Platform Installation Guide
ReadDownload
KNIME Analytics Platform User Guide
ReadDownload
KNIME Best Practices Guide
ReadDownload
KNIME Extensions Guide
ReadDownload
KNIME Flow Control Guide
ReadDownload
KNIME Components Guide
ReadDownload
KNIME File Handling Guide
ReadDownload
KNIME Reporting Guide
ReadDownload
KNIME Integrated Deployment Guide
ReadDownload
KNIME Workflow Invocation Guide
ReadDownload
KNIME Expressions Guide
ReadDownload
Create a New Java based KNIME Extension
ReadDownload
Create a New Python based KNIME Extension
ReadDownload
Publish Your Extension on KNIME Community Hub
ReadDownload
Kerberos User Guide
ReadDownload
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Documentation
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. KNIME Best Practices Guide 


KNIME Analytics Platform 5.4
# KNIME Best Practices Guide
IntroductionWhat is KNIME?KNIME Analytics PlatformKNIME HubWorkflow Design Process: From KNIME Analytics Platform to KNIME HubProject prerequisites: Before building a KNIME workflowAims of best practices when working with KNIMEMake your workflow reusableNaming conventionsUse relative file pathsDocument your workflow with node labels and annotationsStructure your workflow with components and metanodesShare components on the KNIME HubComponents for Data appsCapture and invoke workflowsTesting and data validationMake your workflow efficientEfficient file readingEfficient workflow designMeasure performance with KNIME nodesProcess in memory optionStreaming executionColumnar table backendParallelize branch execution with loopsEfficient database usageMake your workflow secureHandle credentials accordinglyHandling sensitive dataData anonymizationSecrets: Secure information on the KNIME HubGlossaryWorkflow annotationNode labelWorkflow descriptionHub spacesJob(Shared) ComponentData app deploymentSchedule deploymentService deploymentWorkflow ServiceIntroduction
  * Introduction
  * What is KNIME?
    * KNIME Analytics Platform
    * KNIME Hub
  * Workflow Design Process: From KNIME Analytics Platform to KNIME Hub
  * Project prerequisites: Before building a KNIME workflow
  * Aims of best practices when working with KNIME
  * Make your workflow reusable
    * Naming conventions
    * Use relative file paths
    * Document your workflow with node labels and annotations
    * Structure your workflow with components and metanodes
    * Share components on the KNIME Hub
    * Components for Data apps
    * Capture and invoke workflows
    * Testing and data validation
  * Make your workflow efficient
    * Efficient file reading
    * Efficient workflow design
    * Measure performance with KNIME nodes
    * Process in memory option
    * Streaming execution
    * Columnar table backend
    * Parallelize branch execution with loops
    * Efficient database usage
  * Make your workflow secure
    * Handle credentials accordingly
    * Handling sensitive data
    * Data anonymization
    * Secrets: Secure information on the KNIME Hub
  * Glossary
    * Workflow annotation
    * Node label
    * Workflow description
    * Hub spaces
    * Job
    * (Shared) Component
    * Data app deployment
    * Schedule deployment
    * Service deployment
    * Workflow Service


Download PDF
## Introduction 
This guide provides best practices to work efficiently with KNIME Analytics Platform and on KNIME Hub. It shows you how to accelerate, reuse and secure your KNIME workflows. It also provides best practices for updating and monitoring workflows that are shared with your end users.
## What is KNIME? 
KNIME is a platform for end to end data science. It offers two complementary products: KNIME Analytics Platform and KNIME Hub.
### KNIME Analytics Platform 
KNIME Analytics Platform is an open source software with a visual interface, which is shown below. It allows users to build data science analyses. Users can access, blend, analyze, and visualize their data, and integrate third-party tools and libraries.
![04 knime modern ui general layout](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/04_knime_modern_ui_general_layout.png)
Figure 1. The KNIME Analytics Platform user interface
If you haven’t downloaded KNIME Analytics Platform yet, you can do so on the download page.
### KNIME Hub 
KNIME Hub is the commercial software for collaborating and scaling data science, offered online as KNIME Community Hub or installed into a company’s private infrastructure as KNIME Business Hub.
#### KNIME Community Hub 
On KNIME Community Hub, users can browse working examples workflows of data science solutions, or find nodes and components to use in their KNIME projects. On the Community Hub, collections provide selected workflows, nodes, and links about a specific, common topic. Open source extensions provide additional functionalities such as access to and processing of complex data types, as well as the addition of advanced machine learning algorithms.
Additionally, with Team plan paid services small groups or teams of users can share and collaborate on solutions in private spaces, and run workflows ad hoc as data apps or automate their execution.
#### KNIME Business Hub 
KNIME Business Hub provides the same capabilities for collaboration and sharing – but within a company’s dedicated infrastructure. Teams can share knowledge publicly across their organization or privately with selected users. Additionally, data experts can deploy and monitor their workflows, as well as share analytic models as data apps and services with data consumers.
## Workflow Design Process: From KNIME Analytics Platform to KNIME Hub 
As you start your data project with KNIME, you will create a workflow in the KNIME Analytics Platform that can then be uploaded to KNIME Hub. The KNIME Analytics Platform is how you’ll design your data process via a workflow. Once your workflow is ready, you can upload it to your KNIME Hub instance. There, you can easily automate or deploy it.
![image2](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/image2.png)
Figure 2. The data science lifecycle
## Project prerequisites: Before building a KNIME workflow 
As with any project, prior to building your workflow, it’s important to understand the scope of the project or use case you’re working with. This checklist can guide you:
  * Define a goal. It should be measurable and achievable within a certain time limit. For example: _Reduce the churn rate by 15% in the upcoming quarter._
  * Access your data: What sources are required? Which nodes are essential to access them?
  * Define your audience. A workflow is rarely just accessed by the person who built it. Ask yourself who consumes the final results. What should their takeaway from your insights? Define a method to share your insights. It should be suitable to convey the data and your conclusions drawn from it. For example, a method could be:
    * A scheduled workflow that regularly creates and sends a report
    * A scheduled workflow that gets executed regularly to process new data or apply a predictive model?
    * A Data App that gets deployed to KNIME Hub
    * A predictive model that is accessible via REST API
  * Outline the structure of your workflow(s). Consider the following questions:
    * What is the task of each individual workflow that you want to build? If you identify multiple tasks, limit one workflow to one self-contained task.
    * What is the input and output of each individual workflow?
    * Do your workflows share some parts?
  * Define the requirements for your workflows. Consider the following questions:
    * How fast should the workflow be executed?
    * How often should it be executed, etc.?
  * Define the framework conditions. What hardware and software do you need to meet your workflow’s requirements?


## Aims of best practices when working with KNIME 
Now that you have defined your project prerequisites, you want to find the best method to handle your workflow.
You will learn what best practices are, why they matter, and how to implement them in your projects using KNIME Analytics Platform and KNIME Hub. This guide will help you get an intuition about what to consider when building, running, or sharing a workflow.
However, if you are taking your first steps with the KNIME Analytics Platform, please start by following the Getting Started Guide.
## Make your workflow reusable 
### Naming conventions 
Workflows created in your local KNIME Analytics Platform installation are just like any other file on your computer. You can uniquely identify them by their file name and sort them into folders. Before you dive into the data, consider the following when saving your workflows:
  * Use a clear naming convention for your workflows.The file name should describe the task of the workflow. For example: _"Read and Preprocess Customer Data" instead of "Project_1"_. You can also introduce numbers as prefixes. A consistent naming is shown in Figure 3.


![image3](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/image3.png)
Figure 3. Consistently named workflows
  * Remember to limit one workflow to one coherent task. If your use case requires multiple tasks to achieve the final result, group the respective workflows into folders. Choose a folder name that indicates your final goal.


### Use relative file paths 
KNIME workflows are made to operate on data files. If such a file is saved on your local computer, its absolute path uniquely identifies the location of your data file within your file system, for example, `C:\Users\username\Desktop` on Windows or `/home/user` on Linux. However, if you share your workflow with others, e.g. using the KNIME Hub as a repository they will execute it in their local KNIME Analytics Platform installation.
Thus, the link with which KNIME Analytics Platform accesses the data file you provide along with your workflow should be a relative path. This can mean that your path is relative to one of the following:
  * Your current workflow: The data is stored in the same folder as the workflow file, e.g. `/workflow_group/my_workflow`.
  * Your current workflow data area: The data is dedicated to and accessible by the currently executing workflow. Thus, the data is stored in a folder that is located inside the workflow folder. For example, a relative path could look as follows: `<knime-workspace>/workflow_group/my_workflow/data/`. The `data` folder contains the files that contain the data. e.g., a `.csv` file. For example, `<knime-workspace>/workflow_group/my_workflow/data/adult.csv`.
  * Your current Hub space (given that you are signed in to the respective Hub instance from your space explorer).
  * A connected file system that you can access with the respective connector node


Alternatively, you can provide a Custom URL along with your data for everyone that you share your workflow with. For example,
```
https://example.com/mydata/datafile.csv
```

|  Using this option, you can read and write single files, but you cannot move or copy files or folders. However, listing files in a folder, i.e. browsing, is not supported.   
---|---  
Find out more about the KNIME file handling infrastructure in the KNIME File Handling Guide.
Find out more about storing and sharing items on the KNIME Hub in the KNIME Community Hub User Guide.
### Document your workflow with node labels and annotations 
To make your workflow reusable, you and your team need to be able to quickly comprehend what each part and each node are doing. KNIME node names are self-explanatory. However, to describe how each node operates on your specific data, you can change the node label. Select a node in the workflow editor and double-click below the node to change its label. Here, you can describe what operation it performs on your data.
To capture an operation performed by multiple nodes, use a workflow annotation. Right-click anywhere in the workflow editor and select _New workflow annotation_. You can observe a workflow with both workflow annotations, nodes with node labels, and one node which is selected and does not carry a label yet, in Figure 4.
![img workflow annotation](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/img-workflow-annotation.png)
Figure 4. Node labels describe data operations performed by one node. Workflow annotations do the same for multiple nodes.
### Structure your workflow with components and metanodes 
Workflow annotations subsume smaller sub-groups of nodes within a workflow. The more nodes you add to your workflow, the less comprehensible it gets. In larger workflows, you may want to compress nodes into groups represented by a single node. Those can either be components or metanodes.
Metanodes are used solely to structure your workflows more clearly by subsuming many nodes into one. You can take parts of a larger workflow and collapse them into a metanode, hiding that part of the workflow’s functionality.
In addition to the functions of a metanode, a component can be reused on different data that require the same operations performed on them. For example, if you often connect to a particular database, you can create a connector component that contains all the settings options.
To create a component or metanode select at least two nodes, right-click anywhere in the workflow editor, and select either _Create metanode_ or _Create component_ , as explained in this section of the KNIME Components Guide.
To make a component comprehensible to others, document it by adding a description for the expected input and the different setting options (similar to a KNIME node), as explained in this section of the KNIME Components Guide.
To properly exhaust the flexibility of a component, it should:
  * Use a small set of consistent inputs and outputs.
  * Never rely on sources specific to one data set.
  * Not require its users to change any node settings on the inside when adopting it.


You can have your component adjust to different data by adding custom configuration dialogs to access the flow variables inside the component. This allows you and other users of your component to easily customize a component to take variable names from different datasets as inputs. In this respect, components work just like regular KNIME nodes: You provide them with data to which you adjust the node settings. Ultimately, components make frequent data operations faster and easier.
|  Some components are executed repeatedly, e.g. because they are data apps. To accelerate their execution, perform heavy computations, like processing large amounts of data, outside the component.   
---|---  
As node sub-groups, components and metanodes should be treated like small workflows with respect to their scope. You should assign one single, self-contained and coherent function to one component or metanode.
Learn more about components and metanodes in the KNIME Components Guide.
### Share components on the KNIME Hub 
After you create a component in a workflow, if you want to reuse it in another workflow, you can copy and paste it into the new workflow. However, in this way, changes to the component in one workflow are not picked up by the others. To make full use of a component’s reusability potential, you can share it on the KNIME Hub. To share a component, right-click it, select _Component_ → _Share_. By doing so, you can let your collaborators access your changes to a component and also see theirs.
Additionally, by linking a shared component, you can incorporate changes made by others in your local installation. Whenever your collaborators update the component on the KNIME Hub, your KNIME Analytics Platform installation prompts you to update the component. To adapt the update policy, navigate to _Preferences_ → _KNIME_. Under _Component updates_ , you can set the default update policy to _Never_ , _Always_ , or _Prompt_.
Find out more about sharing components in the KNIME Components Guide.
|  Please be aware that if input data are included, they will be accessible to everyone who has access to the component itself. Thoroughly check which type of Hub space you save your components to and whether your data contains sensitive information.   
---|---  
Find out how to handle workflows that involve confidential data, in the security section of this guide.
Some components allow Streaming Execution. Read more about how this feature makes your workflows more efficient in the Streaming Execution section of this guide.
### Components for Data apps 
You can use components and their composite views to define pages in web application workflows. Once uploaded to KNIME Hub, they can be deployed as Data Apps.
To directly interact with the Data App UI in the browser, you need to implement components with widgets. Widgets allow consumers of your Data App to navigate between its pages. Each page corresponds to a component at the root level of the workflow, containing Widget nodes and View nodes. This means that when you inspect the composite view in KNIME Analytics Platform, you can adjust the parameters in the window that opens.
|  A component that constitutes a data app is meant to be executed repeatedly. To accelerate its execution and to avoid undesirable consequences, perform heavy computations outside of the component.   
---|---  
Find out more about data apps in the respective section of the KNIME Components Guide, or learn how to create a data app step-by-step in the KNIME Data Apps Beginners Guide.
### Capture and invoke workflows 
#### Capture workflow segments 
Having many sub-processes in one workflow makes it slow and difficult to maintain and test. In the Structure your workflow with components and metanodes section the advantages of reusable workflow segments are described. Another option for automatically capturing a selection of nodes from a workflow is the KNIME Integrated Deployment Extension.
This feature allows you to:
  * Capture core segments of your workflow for reuse
  * Save captured segments automatically as workflows with all the relevant settings and transformations included As a consequence, captured segments do not require manual work or changes to be reused. This facilitates deploying and testing relevant parts of a workflow, e.g. custom data preprocessing.


You can use the KNIME Integrated Deployment Extension for variety of use cases, such as:
  * Data pipeline deployment
  * Model deployment
  * Ad-hoc testing
  * Workflow summary extraction


In general, you will benefit from:
  * Saving time
  * Reducing errors
  * Increasing compliance
  * Optimizing processes


If you want to learn more ways to optimize workflow performance, please refer to the Efficiency section.
To capture a workflow segment, i.e. a selection of nodes, place a _Capture Workflow Start_ node before the first node, and a _Capture Workflow End_ after the last node is to be captured, as shown in Figure 5.
![img integrated deployment capture](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/img-integrated-deployment-capture.png)
Figure 5. A workflow segment encapsulated by _Capture Workflow Start_ and _Capture Workflow End_ nodes.
The entire workflow segment within the scope of these two nodes is then available at the workflow output port of the _Capture Workflow End_ node. In the configuration dialog of the _Capture Workflow End_ node, you can store input data that will be used by default when running the captured workflow segment unless you provide a different input. You can also propagate variables defined within the captured segment.
|  Nodes that have out-going connections to a node that is part of the scope but are not part of the scope themselves are represented as static inputs but not captured.   
---|---  
Finally, to save a captured workflow, you have the following options:
  1. Write the workflow to a local folder with the Workflow Writer node, as described in the respective section in the KNIME Integrated Deployment Guide.
  2. Write the workflow to a connected file system by adding a database connector node. See the respective section in the KNIME Integrated Deployment Guide for details.
  3. Read the workflow inside another workflow with the _Workflow Reader_ node. It reads a single workflow into a workflow port object. Since the workflow will not be executed by this node, add a _Workflow Executor_ node to do so. See the respective section of the KNIME Integrated Deployment Guide for details.


The _Workflow Reader_ node allows you to read a workflow object, even if you do not want to execute it right away. This is useful to analyze the workflow, e.g. with the _Workflow Summary Extractor_ node or, if you want to provide said workflow object to a subsequent node or component as input. The treatment of the workflow object is delegated to the node or component. However, the _Workflow Reader_ cannot access the data area of the workflow that is read, nor does it read partially executed workflows. The whole workflow will be reset. The _Workflow Executor_ node can be useful to execute a workflow object that is the output of a node or component. For example, you could connect the output port of the _Workflow Combiner_ node, which facilitates configuring input/output port connections between workflow segments, to the _Workflow Executor_. However, the _Workflow Executor_ can only execute a workflow that is captured from the same workflow that the Workflow Executor node is part of.
|  It is not possible to read KNIME components with the Workflow Reader node.   
---|---  
Finally, if you want to use captured workflows immediately after reading them into another workflow, consider using the _Call Workflow Services_ nodes instead. See the Workflow Services section for details.
Learn more about capturing segments of a workflow in the KNIME Integrated Deployment Guide.
#### Workflow Services: Invoke workflows within each other 
Workflow Services provide nodes that allow a caller workflow to invoke other workflows. Those workflows being called are callee workflows.
Since an ideal workflow performs only one task, this feature allows you to modularize your workflows. Ultimately, it makes workflows faster to execute and easier to understand and maintain. You can use Workflow Services for:
  * Sharing data tables, text, models, and other objects between workflows
  * Create modular workflows, each of which creates a chart for a report, that are orchestrated by a caller workflow that generates a report that includes all of them


Rather than invoking standardized (but limiting) JSON-based APIs, it is possible to create and connect directly to KNIME native API endpoints. This means that it is possible to share more KNIME data types, beyond what is possible with JSON, e.g., text documents, decision trees, deep learning models, and many more. This is especially useful in workflow orchestration.
|  To call workflows from external REST clients, please use _Container Input/Output_ nodes. See the respective section in the KNIME Workflow Invocation Guide for details.   
---|---  
The most important nodes regarding Workflow Services are the following from the KNIME Personal Productivity Tools Extension:
  * _Workflow Service Input_ node: It receives a table or any port object from a workflow that calls this workflow, i.e. a caller workflow. Various port types are available.
  * _Workflow Service Output_ node: It sends an object to a caller workflow. Various port types are available.
  * _Call Workflow Service_ node: It calls other local and remote workflows. Multiple ports and various ports are available.


In conclusion, the _Workflow Input_ and _Output_ nodes are used to build workflows intended to be called from within another workflow (i.e., using the Call Workflow Service node).
The _Call Workflow Service_ node, in turn, obtains the results for further processing in the workflow. It can be used any time we want a caller workflow to invoke a callee workflow. The workflow will be able to receive inputs via _Workflow Service Input_ nodes and return outputs which the Workflow Service Output nodes then collect. The ports of the Call Workflow Service node are adjusted automatically in accordance with the callee workflow selected in the configuration dialog.
The advantages of Workflow Services are:
  * They can adequately access the workflow data area of the callee workflow, which distinguishes them from the _Workflow Reader_ node.
  * They leverage the infrastructure of KNIME Business Hub, leading to better scalability.
  * They are more efficient than using _Workflow Reader_ and _Workflow Executor_ nodes, when direct workflow execution is the main goal. If you want to learn more ways to optimize workflow performance, please refer to the Efficiency section.


Find out more about Workflow Services in the KNIME Workflow Invocation Guide.
### Testing and data validation 
#### Get started with why and how to test 
To guarantee that a KNIME workflow, component or workflow segment performs as expected, you should test your workflows regularly. Ultimately, this makes your workflows less error-prone and easier to maintain. This section explains how to leverage KNIME as a testing framework.
Your workflows will not always be faced with the ideal input, nor will they be configured in the right way. Because of that, your workflows should be robust to inputs deviating from norm by breaking in a controllable way. This helps ensure that any future changes continue to pass all of the same tests.
Before testing, you should clearly define your goals. In the case of workflow testing, you can formulate test cases, which state any given situation encountered by your workflow, i.e., what happens when valid data is injected into the workflow, and what errors will be raised, when invalid data is provided.
Possible issues that may occur while testing include:
  * Failing for unexpected reasons
  * Unexpected error messages
  * Wrong output


The reasons could be:
  * Invalid inputs, e.g. provided by end users
  * Configuration changes
  * Changes caused by an update
  * Changes in external services


#### Test and validate in KNIME Analytics Platform 
In KNIME Analytics Platform you can create and automate a testing framework for future re-deployments. You can create so-called testflows, which are KNIME workflows that provide controlled test inputs and outputs to a workflow, component, or workflow segment. Test in- and outputs are gold standard data, i.e., they represent small samples of the inputs and results that you expect your workflow to handle.
When test outputs are provided, the testflows then check whether the outputs align with the expected results generated from the test inputs. If they do not, the testflows should raise an alert, which is a response to deviating inputs or unexpected cases. You have already defined those cases as your error test cases, e.g., missing or invalid inputs, unexpected failure, etc.
As a best practice you should never:
  * Process the golden data outside your test flow as if it were regular data. This may lead to more unknown errors that you need to identify.
  * Make any changes to the golden data.


It is best to keep golden data in a separate folder and to restrict permissions if possible. KNIME Analytics Platform provides nodes that allow you to test the output of your testflows. Although there exist many more, the most commonly used nodes are listed below.
  * _Table Validator_


Ensures a certain table structure and table content (schema). As a result, the output table structure should be identical to the specification defined by the user in the configuration dialog. This node forms part of the KNIME Base nodes.
  * _JSON Schema Validator_


Validates JSON values based on a specified schema. It collects mismatching values so that you can inspect them.
  * _Table Difference Checker_


Determines if there is a difference between two tables. It compares the table created by your testflow with a reference table, e.g., a golden table.
  * _Table Difference Finder_


Identifies and returns domain and value differences for each row and column in the tables. As opposed to the Table Difference Checker, the node does not immediately fail if the tables differ, but the configuration dialog provides an option to enable failing, too.
  * _File Difference Checker_


The node will compare two files given by flow variable and fails with an exception if the files are not equal. Many similar nodes exist for other types of files, too, e.g., the Image Difference Checker. You can find them on the KNIME Community Hub by searching for the keywords "difference checker".
In the following, you will read about two approaches to create testflows in KNIME Analytics Platform. The nodes listed above can be used in both of them.
In KNIME Analytics Platform you can either use the KNIME Integrated Deployment Extension or the KNIME Testing Framework UI Extension. Both approaches are described in the following. You can choose which one fits best your use case.
##### KNIME Integrated Deployment 
The first option is to create testflows by capturing workflow segments and executing them using the KNIME Integrated Deployment Extension, which is described in the Capture and invoke workflows section.
This option is useful for ad-hoc testing, i.e., executing the captured workflow segments in the same workflow while testing against new data. For execution, you may use the Workflow Executor. First, enclose the segment you want to test into the _Capture Workflow Start_ and _Capture Workflow End_ nodes, as described in the Capture and invoke workflows section of this guide. Then, in the same workflow create multiple workflow branches. Each of them represents a different test case by injecting different data inputs into the captured workflow segments to be tested, as shown in Figure 6.
![img test branches](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/img-test-branches.png)
Figure 6. A workflow containing workflow segments representing different test cases.
|  If you only want to test whether your workflow is executing or not, you may insert a _Workflow Executor_ node without any additional input but the captured workflow segment.   
---|---  
To perform an application test, i.e. to test an entire workflow we recommend using the _Workflow Services_ nodes instead.
##### KNIME Testing Framework UI 
The second option for testing your workflows is the KNIME Testing Framework UI Extension. It is a more complex method of creating and executing testflows. You can generate reports which you can view inside of KNIME Analytics Platform.
Your testflow can be as simple or complex as you choose to. The test itself is performed by a validator node such as the _Table Validator_ , which compares the output generated by your workflow against a gold standard. To establish which conditions must be met for the workflow to fail or to succeed execution, you can use the _Testflow Configuration_ node.
#### Testflow automation 
To execute multiple testflows automatically, you can execute a _Call Workflow Service_ node in a loop. You simply iterate over the testflow names, concatenate the results of the reported errors and write them into a report, as shown in Figure 7.
![img testflow automation](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/img-testflow-automation.png)
Figure 7. A workflow executing testflow files in a loop and writing their execution time to a report.
Find out more about creating reports in the KNIME Reporting Guide.
This is necessary, because identifying errors with testflows does not automatically mean that the people responsible are informed. The testflow could either fail silently, or the process would even be successfully completed whilst having processed faulty input data.
To be made aware of these errors, you can embed failing nodes in a try-catch framework to catch all the errors. It captures information about failing nodes which can then be stated in the report. On the KNIME Community Hub you can find various nodes to implement this framework.
Try nodes simply indicate the beginning of the try-catch framework. Place them in front of the first node you want to include and connect them to its input port. The _Catch Errors_ node, on the other hand, will be placed after the last node you want to include in your try-catch structure. By connecting the last node to the input port of the _Catch Errors_ node, it will forward the input from the first port if the execution was successful. If execution on the top branch failed (and a matching try node was connected before the failing node) then the input from the second port of the _Catch Errors_ port will be forwarded, and the second variable outport will contain information about the observed error.
After having collected the errors, you can choose from a variety of KNIME nodes to message the responsible people about the issues you identified.
## Make your workflow efficient 
### Efficient file reading 
To speed-up your workflow, keep the following guidelines in mind:
  * Avoid reading the same file multiple times, i.e., do not use a Reader node each time you need the same file for different data operations. You can use multiple connections to connect the same Reader node to more than one subsequent node, as shown in Figure 8.


![img double file reading](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/img-double-file-reading.png)
Figure 8. One node reading a file can have multiple connections at its output port
  * Read multiple files with the same structure with one Reader node. Open the configuration of a Reader node and select the “Files in folder” option in the Settings tab.
  * Remove unnecessary data. KNIME Source nodes, like the Reader nodes, offer an option for removing unused or redundant columns. Open the configuration of a Reader node and select the _Transformation_ tab.


### Efficient workflow design 
When designing a workflow, there are only a few mistakes to absolutely avoid, but many convenient practices that facilitate its usage. Both will be introduced in the following.
An efficient workflow never contains:
  * Disconnected nodes or components. Especially, if you want to run that workflow on a KNIME Hub instance, or if you want to call it from another workflow.
  * Workflow branches that are not actively needed.
  * Large, unused files in the form of components, nodes, or metanodes. They slow down loading, executing, and saving the workflow. In some cases, they even cause the workflow to fail.


To make your workflow run faster, keep the following in mind:
  * Some data manipulation operations are more computationally expensive than others. KNIME Analytics Platform offers nodes to avoid unnecessarily expensive operations. For example:
    * **Joins** : Joining two tables with the Joiner node to then filter for criteria is slow. Instead, replace a few rows with the _Value Lookup_ node, that compares and matches your data to a dictionary containing your criteria.
    * **Loops** : Depending on your data and iterations, loops may slow down your workflow. Thus, use loops only when absolutely needed. Instead, try Multi Column nodes to operate on strings in multiple columns at once.
  * Some data types use more memory space on your computer than others. For example, strings occupy more space than integers. Check for columns with the wrong data type, e.g., where numbers are saved as strings.
  * Redundant data serves no purpose in your data operations. Filter out columns that contain constant values or any information that you will not be using later.
  * The fastest way to run recurring processes, is to capture, save them and replicate your workflow on (new) data. To learn how, refer to the Capture and invoke workflows section of this guide section of this guide.


### Measure performance with KNIME nodes 
Like any good project, a good workflow is scalable.
To measure which nodes take the longest to execute in your workflow you can use the _Timer Info_ node. It reports individual and aggregate timing and execution information for all nodes. This allows you to identify weak points in your workflow and track your progress over time.
The _Timer Info_ node is included in the KNIME base installation. Simply drag it anywhere into the workflow that you want to evaluate, and execute it.
### Process in memory option 
In KNIME Analytics Platform, the _GroupBy_ and _Pivot_ nodes offer the "Process in memory" option. While it does use significant memory, it also brings a notable performance improvement by reducing disk swaps when these nodes are executed. To activate the "Process in memory" option, open the configuration dialog of the _GroupBy_ or _Pivot_ node. Under the tabs _Settings > Groups_ navigate to _Advanced settings_ and select the checkbox _Process in memory_.
### Streaming execution 
Some KNIME nodes and components allow streaming execution. You can implement it with the KNIME Streaming Execution (Beta) extension on KNIME Hub.
While the default execution operates node-by-node, in streaming execution nodes are executed concurrently. Each node passes data to the next as soon as it is available, i.e., before the node is fully executed. In particular, all data are divided into a number of batches which are streamed one by one. Especially, when reading or preprocessing data, streaming execution accelerates runtime.
|  In Streaming Execution intermediate results are not available because no data is cached.   
---|---  
To switch from default to streaming execution, create a component and open its configuration dialog. Open the _Job Manager Selection_ tab in the component configuration dialog. If the streaming execution option is not available for the node you will only see the `default` option in the menu. In components that allow streaming execution, select the "Simple Streaming" option. The streamable component is now marked with an arrow in its lower left corner.
![img streaming execution](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/img-streaming-execution.png)
Figure 9. A component with Simple Streaming execution enabled
|  Streaming execution is not yet optimized to operate with the Columnar Backend. You can adjust your default table backend as described in the Columnar Table Backend section of this guide.   
---|---  
For further reading on Streaming Execution, please consult the respective section in the KNIME Components Guide.
Find out more about how to structure your workflow with components in the section on reusability of this guide.
### Columnar table backend 
You can use the KNIME Columnar Table Backend extension to accelerate workflow execution. It optimizes the use of main memory in KNIME Analytics Platform by reviewing the underlying data representation, where cell elements in a table are represented by Java objects. Its now different underlying data layer is backed by Apache Arrow which is based on a columnar representation.
The advantages are:
  * **Simplicity** : Simple cell elements (e.g. integers) are not represented as memory-intensive objects.
  * **Compactness** : The data is kept in a more compact form. Thus, more data fits into main memory.
  * **Controllability** : Smaller chunks of data are held (“cached”) in-memory, making the memory footprint of KNIME much more stable. Also, the data lives "off-heap" (a region of memory) where, for you as the user, it is more controllable.
  * **Compatibility** : It is a first step in enabling shared memory with other programming languages such as Python, leading to more performance improvements.and thus, more performance improvements.


You can activate the Columnar Backend by navigating to _Preferences > Table Backend_.
|  Setting your table backend to columnar in _Preferences_ will only affect workflows created after making the adjustment. Existing workflows, including the workflow(s) currently opened in your KNIME Analytics Platform, will need to be manually updated.   
---|---  
To adjust the table backend for one specific workflow, e.g., a workflow created before changing your default table backend, navigate to the node description tab in the side panel navigation. Click on the ![16](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/icons/cog.svg) icon. In the window that opens, in the _Table Backend_ tab, select _Columnar Backend_ from the dropdown menu. The parameters relative to memory usage of the Columnar Backend can be configured under _Preferences > Table Backend > Columnar Backend_.
|  The Columnar Backend is not yet optimized to operate with streaming execution. You can adjust your default table backend as described in the Streaming execution section of this guide.   
---|---  
For more details, please refer to the Preferences section of the KNIME Analytics Platform User Guide.
### Parallelize branch execution with loops 
Another way to speed up workflow execution is parallel computing. You can implement this with loops. They iterate over a certain part of the workflow, e.g. over a workflow branch. Each iteration, a repeated procedure, has different input. Among others, this input can consist of chunks of the data you want to process in KNIME Analytics Platform. These chunks will each be processed in parallel on separate workflow branches.
To get started, you need to have the KNIME Parallel Chunk Loop Nodes Extension installed.
|  To make use of workflow execution parallelization, you need to first execute your workflow on the KNIME Hub.   
---|---  
After reading the data, and before your first data manipulation, you can add the _Chunk Loop Start_ node. It splits data into equally sized chunks for each iteration. Then it executes the manipulations in the loop body on each chunk in parallel. You can configure either the number of chunks or the number of rows per chunk. At the end of a parallel chunked block, add the Parallel Chunk End node. It collects and concatenates the results from the parallel workflow branches. If the data segment in the loop generates tables with repeated RowIDs, the _Parallel Chunk_ End node adds a chunk index to the RowID.
For more details about loops, please refer to the Loops section of the KNIME Flow Control Guide.
### Efficient database usage 
KNIME Analytics Platform allows you to access, manipulate and share large amounts of data. The most secure and effective way to store this data is in a (relational) database.
With the KNIME Database Extension, which is already part of every KNIME Analytics Platform installation, you can use the full functionality of any JDBC-compliant database within KNIME. This includes the following database operations:
  * Connecting to the database
  * Reading from the database
  * Generating queries in the database
  * Manipulating the database structure, i.e. creating and deleting tables in the database
  * Manipulating elements contained in a database table, e.g. deleting or updating values
  * Type Mapping, i.e. defining rules for mapping database types to KNIME types and vice versa. Read more about this in the respective section of the KNIME Database Extension Guide.


To connect to the database of your choice, KNIME provides Connector nodes that can be found in different extensions. There are three ways to connect to a database in KNIME:
  1. Connect to a predefined database that has its own dedicated node in the KNIME Database Extension. Among others, PostgreSQL, MySQL, Microsoft SQL Server, or SQLite have dedicated nodes.
  2. Connect to a predefined database that has its own dedicated node in another dedicated KNIME extension, which is the case for e.g. the KNIME Amazon Athena Connector[Amazon Athena Connector Extension] or the KNIME Snowflake Integration
  3. Connect to any other JBDC-compliant database using the generic DB Connector node.


|  The database-specific connector nodes already contain the necessary JDBC drivers and provide a configuration dialog that is tailored to the specific database. It is recommended to use these nodes instead of the generic DB Connector node, if possible.Find out more about database connectivity in the respective section of the KNIME Extension Guide.   
---|---  
Once connected, you can select relations with the _DB Table Selector_ node and read from the database using the _DB Reader_ node.
In addition to reading from the database, you can perform in-database processing, i.e. data manipulation directly on the database, using the KNIME Database nodes, e.g. _DB Joiner_ or _DB GroupBy_ and formulating your query.
Their configuration is similar to the KNIME manipulation nodes. The DB Query Nodes require no SQL coding, since the nodes themselves construct and output a SQL query.
Finally, Database Writing Nodes let you create, update and delete data on the database of your choice. The _DB Connection Table Writer_ node is useful for efficiency and data lineage maintenance, because it creates a new database table based on the input on the SQL query. As a result, there is no need to actually read the data into KNIME Analytics Platform itself. Additionally, the _DB Loader_ node lets you insert rows into an existing DB table, although this is not supported by all databases.
KNIME Analytics Platform also supports NoSQL and non-relational databases like MongoDB(document-oriented), DynamoDB (document-oriented and key-value-based) and Neo4j (graph-based).
To speed-up your workflow with respect to databases, keep the following guidelines in mind:
  * Avoid using the _DB Connector_ node more than once when connecting to the same database. Unless you need to open up parallel connections to speed up execution of parallel branches, this practice consumes more computing resources.
  * Do not process your data locally. Do it directly on the database whenever possible.
  * Always use the _DB Connection Closer_ node at the end of your database processing. It closes a DB Session during workflow execution and will free up database resources as soon as they are no longer needed by the workflow.


The KNIME Database Extension includes nodes for simulating database transactions, such as _DB Transaction Start_ and _DB Transaction End_. These nodes help group multiple database operations into a single unit, ensuring that either all operations execute or none do. If the transaction is successful, it ends with a commit to the database. If it is not successful, it is rolled back, returning the database to its state at the beginning of the transaction.
This enhances efficiency by:
  1. Enabling reliable recovery from failures, maintaining database consistency even if execution halts prematurely, avoiding incomplete operations and unclear statuses.
  2. Ensuring isolation between concurrent programs accessing the database, preventing potential errors in program outcomes.


Find out more about database capabilities with KNIME and what to consider for different database providers in the KNIME Database Extension Guide.
## Make your workflow secure 
### Handle credentials accordingly 
When working with confidential data, it’s critical to have the best security measures in place to protect it from breaches and misuse. Protecting sensitive information is essential for regulatory compliance and maintaining the trust of customers and stakeholders.
KNIME Analytics Platform helps by providing various security features. It includes tools to encrypt data, manage user access, and ensure data integrity so you can handle sensitive information.
#### Credential nodes 
To make sure confidential data are not accidentally saved or shared, make sure that you do not save any hard coded information in your nodes and workflows. In detail, this means the following:
  * Do not save passwords in plaintext as they are easy to copy.
  * Do not hardcode your credentials into nodes.
  * Reset your workflow before sharing it anywhere.


You can avoid storing passwords as a part of your workflow on disk by default. Navigate to the `knime.ini` file in the root of your local KNIME installation, and set `Dknime.settings.passwords.forbidden=true`.
Another option to secure credentials is to encrypt them. The _Credentials Configuration_ or _Credentials Widget_ nodes allow the user to specify credentials at runtime.
Configuration nodes provide input parameters to other nodes in the workflow. They do this through flow variables. This means that if the flow variable output of the Configuration node is connected to another node, that node can access the input parameters set in the Configuration node.
Widget nodes, on the other hand, are displayed as widgets only in the composite views. This means that when you inspect the composite view in KNIME Analytics Platform, you can adjust the parameters in the window that opens.
Using Widget and Configuration nodes to handle credentials means that the user does not need to access the node that performs the action for which the credentials are needed. Instead, the user can access the necessary settings from the outside, without having to change the configuration of the node itself.
You can use Configuration and Widget nodes to securely use database connector, authenticator, and REST nodes. Below, is an example of how a database connector node accesses credentials encrypted in a _Credentials Configuration_ node.
![img credentials configuration](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/img-credentials-configuration.png)
Figure 10. Handling credentials with the _Credentials Configuration_ node
Find out more about Configuration nodes in the respective section in the KNIME Components Guide. The same guide also provides a section on Widget nodes, too.
#### Cloud credentials 
To make your workflows flexible, you can access remote file systems. When connecting to a cloud service such as Microsoft, Google, etc., you should use appropriate connector nodes to securely authenticate. In the configuration dialog of an authenticator node, you can limit the scopes of access that will be granted for this connection. You should select the scopes based on the level of access that you need. The _Add scope_ button allows you to add additional scopes within the cloud service. In addition to the Standard list of scopes, you can add several custom scopes.
|  In order to see the corresponding Scope under the Standard list of scopes, you need to have installed the extension first.   
---|---  
### Handling sensitive data 
When you share a workflow, you can choose to save data that was processed along with it. Intermediate results of data manipulation operations are then available even after the workflow is closed. Particularly, when processing confidential data inside your workflows, you should make sure not to include any information that you do not want to share. When working with sensitive data, keep the following in mind:
  * Do not save confidential data within your workflow
  * Reset your workflow before sharing it unless you have a specific reason not to. That is described in the respective section of the KNIME Community Hub User Guide.


|  Items that you upload to a space on the KNIME Hub will be available to anyone with access to that space. Therefore, be very careful with the data you share.   
---|---  
### Data anonymization 
Sensitive data needs to be handled with caution. GDPR (General Data Protection Regulation) stipulates that only anonymized data may be used extensively. As a consequence, you may want to anonymize your confidential data while later being able to re-identify the individual data points in the dataset. This is referred to as pseudo-anonymization. To additionally assess the risks of de-anonymization, you can use the Redfield Privacy Nodes extension for KNIME which uses different algorithms for anonymization and the assessment of re-identification risks.
For basic anonymization (pseudo-anonymization) you can use the _Anonymization_ node. In the configuration dialog, simply choose the identifying attributes, i.e. the columns in your table that contain sensitive information. The _Anonymization_ node hashes the values in the columns using SHA-1. You can also choose between different salting strategies. An advantage of this rather uncomplicated approach is that you can go back to the original data, since the translation table is available at the second output port.
For advanced anonymization, you should rely on hierarchies. You can build them by defining complex binning rules, which convert the unmodified data to completely suppressed data. You can create different types of hierarchies with the _Create Hierarchy_ node. Multiple hierarchies may be applied to the data subsequently. To read more about the hierarchy types, please refer to the _Building hierarchies_ section of the following article: Remain Anonymous with Redfield’s Privacy Extension.
To now apply the rules set to the original data, you need to combine them in the _Hierarchical Anonymization_ node.
To finally assess which risks are posed by your anonymization techniques, add the _Anonymity Assessment_ node. To find out more about the types of risks that the node identifies, please refer to the node page of the Anonymity Assessment node on the KNIME Community Hub.
### Secrets: Secure information on the KNIME Hub 
#### Manage and use secrets 
Secrets provide a way to centrally store and manage logins to other systems. For example, a secret could be credentials to log into an external database, file system or service. Secrets are owned and managed by a user or team. User secrets are intended for managing personal logins, e.g., john.smith. Team secrets on the other hand are intended for shared logins sometimes referred to as technical or service users, e.g., hr_read_only, that are shared with multiple users.
To manage your personal secrets navigate to your account page and select _Secrets_ from the menu on the left. On your Secrets page you can create, edit and delete your personal secrets. Similarly, to manage your team’s secrets, navigate to the team’s page and select _Secrets_ from the menu on the left. On your _Secrets_ page you can create, edit and delete your personal secrets, as shown in the respective section of the KNIME Secrets User Guide.
|  Team secrets can only be created by team admins.   
---|---  
Secrets allow you to manage sensitive information like passwords and API keys. Each secret has a unique name, optional description, secret type, and authentication type. Editing a secret allows changes to its values but not its secret nor authentication type; to change the type, a new secret must be created. Deleting a secret requires confirming the secret’s name to avoid accidental deletions. Some secret types, such as OAuth2, require you to log into your account to acquire a session and refresh token. If you are not logged in, these secrets are marked in the secrets table as _Not consumable_ in the status column. Team secrets can be shared with others by managing access rights, which can be set to "Use" or "Edit." These access rights control whether users can only use the secret in workflows or also modify and delete it. Personal secrets, however, cannot be shared with others for security reasons.
Find out more about Secrets in the KNIME Secrets User Guide.
You can access Secrets in your KNIME workflows through the _Secrets Retriever_ node, part of the KNIME Hub Additional Connectivity (Labs) extension.
|  The _Secrets Retriever_ node requires a connection to a KNIME Business Hub.   
---|---  
The node allows selection of multiple secrets, adjusting its output ports based on the secret types. For security, secrets are not saved with the workflow and must be re-executed each time. The node retrieves secrets using the executing user’s permissions, and if the user lacks rights, an error occurs. Secrets are referenced by internal identifiers, ensuring name changes do not disrupt connections.
#### Secret types 
Secret types in KNIME cater to diverse authentication needs, enhancing workflow security and flexibility. Credentials can be of various types like usernames, passwords or password-only. Thus, secrets accommodate different authentication scenarios such as database logins, etc. The Box secret type facilitates connection to Box for file management, supporting OAuth2-based user authentication. Similarly, the Google secret type enables connection to Google services like Drive or BigQuery. Microsoft secret types extend compatibility to Microsoft/Azure cloud platforms, empowering users to access Office 365, SharePoint, Azure services, and more. Each secret type offers specific authentication methods specific to different use cases. Make sure to choose the suitable secret type for interactive logins or automated workflows. Additionally, the flexibility to specify scopes of access ensures fine-grained control over resource permissions, enhancing security.
Find out more about leveraging Secrets on the KNIME Hub in the KNIME Secrets User Guide. It offers diverse how-to guides which provide systematic procedures to incorporate authentication and access permissions in Microsoft Azure services into KNIME workflows.
## Glossary 
### Workflow annotation 
A box with colored borders and optional text that can be placed in a workflow to highlight an area and provide additional information. Right-click on an empty spot in a workflow and select “New workflow annotation” to create a new workflow annotation at that position.
### Node label 
The text underneath a node. Select the node and click on the text. Then you can edit it and add a description of what the node does.
### Workflow description 
The description of a workflow that can be edited by the user. Click on the ![16](https://docs.knime.com/latest/analytics_platform_best_practices_guide/img/icons/pencil.svg) at the top-right to edit it. The workflow description is also shown in the KNIME Hub when a user opens the workflow’s page.
### Hub spaces 
The place on the KNIME Hub where workflows, components, and data files are stored. Find out more in the Community Hub User Guide.
### Job 
Every time a workflow is executed ad hoc or a deployment is executed a job is created on KNIME Hub. More information about jobs can be found here.
### (Shared) Component 
A workflow snippet wrapped into a single node. A component can have its own configuration dialog and can also be assigned a view. Components enable you to bundle functionality for a particular part of the workflow and allow sharing and reusing. For more information refer to KNIME Components Guide.
### Data app deployment 
A workflow comprising one or more components that contain view and widget nodes a user can interact with in the browser. A Data App is deployed on KNIME Business Hub. A guide on building Data Apps can be found here.
### Schedule deployment 
A schedule allows you to set up a workflow to run automatically at selected times. More information is available:
  * Here for KNIME Business Hub, or
  * here for Team plan on KNIME Community Hub.


### Service deployment 
A service deployment allows you to use the workflow as an API endpoint. For more information refer to KNIME Business Hub User Guide.
### Workflow Service 
A workflow with Workflow Service Input and Workflow Service Output nodes that can be called from other workflows using the Call Workflow Service node.
  * Introduction
  * What is KNIME?
    * KNIME Analytics Platform
    * KNIME Hub
  * Workflow Design Process: From KNIME Analytics Platform to KNIME Hub
  * Project prerequisites: Before building a KNIME workflow
  * Aims of best practices when working with KNIME
  * Make your workflow reusable
    * Naming conventions
    * Use relative file paths
    * Document your workflow with node labels and annotations
    * Structure your workflow with components and metanodes
    * Share components on the KNIME Hub
    * Components for Data apps
    * Capture and invoke workflows
    * Testing and data validation
  * Make your workflow efficient
    * Efficient file reading
    * Efficient workflow design
    * Measure performance with KNIME nodes
    * Process in memory option
    * Streaming execution
    * Columnar table backend
    * Parallelize branch execution with loops
    * Efficient database usage
  * Make your workflow secure
    * Handle credentials accordingly
    * Handling sensitive data
    * Data anonymization
    * Secrets: Secure information on the KNIME Hub
  * Glossary
    * Workflow annotation
    * Node label
    * Workflow description
    * Hub spaces
    * Job
    * (Shared) Component
    * Data app deployment
    * Schedule deployment
    * Service deployment
    * Workflow Service


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. KNIME Components Guide 


KNIME Analytics Platform 5.4
# KNIME Components Guide
IntroductionComponents vs metanodesCreating components and metanodesSetup components and metanodesSetup components and metanodesReconfigure components and metanodesExecution state of components and metanodesChange the flow variables scope in componentsCustom components configuration dialogsConfiguration nodesComponent configuration dialogScripting ComponentsComponents composite viewsWidget nodesView nodesRefresh Button Widget nodeRe-execution of Widget nodesInteractive Widget nodesLayout of composite viewsVisual layout editorEnable the reporting function of a componentLegacy flagAdvanced layoutingNode Usage tabLayout of configuration dialogsStreaming execution of componentsDefault executionStreaming executionError HandlingEdit components descriptionSharing componentsShare components in local workspaceShare components on the KNIME HubShare components on the KNIME Business HubLink typeUse a shared componentEdit the instance of a shared componentEdit a shared componentUpdate linked componentsVersion a component shared to KNIME HubChange component version to useComponents for Data AppsWidget nodesInteractive Widget nodes and View nodesRe-execution and Refresh Button Widget nodeIntroduction
  * Introduction
    * Components vs metanodes
  * Creating components and metanodes
  * Setup components and metanodes
    * Setup components and metanodes
    * Reconfigure components and metanodes
    * Execution state of components and metanodes
  * Change the flow variables scope in components
  * Custom components configuration dialogs
    * Configuration nodes
    * Component configuration dialog
  * Scripting Components
  * Components composite views
    * Widget nodes
    * View nodes
    * Refresh Button Widget node
    * Re-execution of Widget nodes
    * Interactive Widget nodes
  * Layout of composite views
    * Visual layout editor
    * Enable the reporting function of a component
    * Legacy flag
    * Advanced layouting
    * Node Usage tab
    * Layout of configuration dialogs
  * Streaming execution of components
    * Default execution
    * Streaming execution
  * Error Handling
  * Edit components description
  * Sharing components
    * Share components in local workspace
    * Share components on the KNIME Hub
    * Share components on the KNIME Business Hub
    * Link type
    * Use a shared component
    * Edit the instance of a shared component
    * Edit a shared component
    * Update linked components
    * Version a component shared to KNIME Hub
    * Change component version to use
  * Components for Data Apps
    * Widget nodes
    * Interactive Widget nodes and View nodes
    * Re-execution and Refresh Button Widget node


Download PDF
## Introduction 
In this guide we introduce components and metanodes.
Components and metanodes are both built from wrap workflows.
Components really are KNIME nodes that you create which bundle functionality, have their own configuration dialog and their own composite views. Metanodes on the other hand are containers of a part of your workflow, that help to build cleaner and structured nested workflows.
In this guide, we explain how to create components and metanodes, how to create composite views and configuration dialogs for components, and how to use them.
### Components vs metanodes 
Components are nodes that contain a sub-workflow, which lets you bundle functionality for sharing and reusing. Components encapsulate and abstract functionality, can have their own configuration dialog, and custom interactive composite views. You can use them to hide some complexity in a workflow and you can also reuse them in other workflows or in different parts of the same workflow, or you can share them with others via KNIME Server or on the KNIME Hub. Additionally, components and their composite views are also used to define pages in web application workflows, which once uploaded to KNIME Hub can be deployed as Data Apps.
In contrast to components, metanodes are purely used to organize your workflows better: you can take parts of a larger workflow and collapse it into a metanode, hiding that part of the workflow’s functionality.
The main differences are:
  * Custom configuration dialogs: components can have custom configuration dialogs, which are managed through the Configuration nodes
  * Custom composite views: components can also have composite views, which are acquired from the interactive views of Widget nodes and View nodes inside the component
  * Sharing: components can be shared via KNIME Hub while metanodes can not
  * Flow variable scope: the flow variable scope of a component is local which makes them self-containing and less polluting to the parent workflow. A flow variable defined inside a component is by default not available outside it, and a flow variable defined outside the component is by default not available inside it.


## Creating components and metanodes 
To encapsulate nodes into a component or collapsing a set of nodes into a metanode follow these steps:
  1. Select the nodes by either:
    1. Dragging a rectangle with the mouse over the nodes in the workflow editor
    2. Press and hold the "Ctrl" button and select the nodes clicking them
  2. Create a component by:
    1. Clicking the _Create component_ button at the top of the Workflow Editor shown in Figure 1
![02 1 new create component button](https://docs.knime.com/latest/analytics_platform_components_guide/img/02-1_new_create_component_button.png)
Figure 1. _Create component_ button
    2. Alternatively, right-click the selection and select either _Create component_ or _Create metanode_ in the context menu shown in Figure 2
![02 2 new create component right click](https://docs.knime.com/latest/analytics_platform_components_guide/img/02-2_new_create_component_right_click.png)
Figure 2. Creating a component or a metanode via right click
  3. Give the component or metanode a name as shown in Figure 3
![02 3 new name component](https://docs.knime.com/latest/analytics_platform_components_guide/img/02-3_new_name_component.png)
Figure 3. Giving a component or metanode a name
  4. Press Enter or click the yellow tick on top to confirm the name. You will see the new component or the new metanode in the workflow editor in place of the single nodes, as shown in Figure 4
![02 new component metanode](https://docs.knime.com/latest/analytics_platform_components_guide/img/02_new_component_metanode.png)
Figure 4. Newly created component and metanode in the workflow editor


Appropriate input and output ports will appear for the component and the metanode based on the connections coming into and out of it.
Notice that collapsing nodes into a component or a metanode resets the nodes. Confirm with _OK_ in the dialog.
## Setup components and metanodes 
You can change different settings for components and metanodes, like name, number of input and output ports and their types. Moreover, you can change metadata, description and customize the icon of a component. In this section we will explain you how to setup and reconfigure components and metanodes.
### Setup components and metanodes 
You can change the settings of a component directly in the workflow editor.
Here you can:
  * Change the component name by double-clicking on it.
  * Add input and output ports


Click the _Add Input Port_ or the _Add Output Port_ button to right of the component or metanode as shown in Figure 5
![03 add port](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_add_port.png)
Figure 5. Adding input and output ports to a component or metanode
### Reconfigure components and metanodes 
  * Remove input and output ports Remove existing input and output ports by clicking on the port you want to remove. A _Remove port_ button appears. Notice that you have to remove from inside the component or metanode, all connections coming to and from the port before you can remove it as shown in Figure 6.
![03 new remove ports](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_new_remove_ports.png)
Figure 6. Left: the lower output port still has a connection inside the component so the trash bin button is inactive. Right: the lower output port without connection inside the component
  * Expand components and metanodes


To return the nodes within a component or a metanode into their original, uncollapsed state, right-click the component or metanode and select _Component → Expand component _or_ Metanode → Expand metanode_ in the context menu.
### Execution state of components and metanodes 
Similar to regular KNIME nodes, components can be configured and executed. How to create a component configuration dialog will be explained in Custom components configuration dialogs section. Components use a traffic light to indicate their execution state, same as for the nodes as shown in the Workflow Editor & nodes section of the KNIME Workbench Guide. In order to access data at the output port(s) every node of the sub-workflow enclosed in the component needs to be successfully executed, hence have a "green" traffic light.
Metanodes can also be executed, meaning that the nodes building up the sub-workflow contained by them, will be executed. However, since metanodes are only containers of parts of the workflow they themselves can not be configured. Only when all of the nodes inside the metanode are executed successfully, the metanode is executed successfully. A metanode has two execution states: A tick indicates that the metanode is executed. A double arrow indicates a running execution (shown in Figure 7, and the dot at the respective output ports).
![03 new metanode running](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_new_metanode_running.png)
Figure 7. Left: A successfully executed metanode; right: A metanode during a running execution
Metanodes have three output states:
  * Accessible ("green" dot), meaning that all the nodes building the sub-workflow branch connected to that output port are successfully executed
  * Connected but empty ("yellow" dot), meaning that the sub-workflow branch connected to that output port did not produce an output
  * Disconnected ("red" dot), meaning that the port is not connected to any sub-workflow’s node.


This is shown in Figure 8
![03 new three output states metanode](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_new_three_output_states_metanode.png)
Figure 8. Three different states of the output ports of a metanode
Table 1 and Table 2 show the execution states in detail.
Table 1. The different execution and output states of components **Components**  
---  
Icon | Output  
![02 new component green](https://docs.knime.com/latest/analytics_platform_components_guide/img/02_new_component_green.png) | Successfully executed  
![03 new component yellow](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_new_component_yellow.png) | ConfiguredAll outputs connected  
![03 new component red x](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_new_component_red_x.png) | Execution failedContains unconnected output  
![03 new component yellow x](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_new_component_yellow_x.png) | Execution failedOne branch failed  
Table 2. The different execution and output states of metanodes **Metanodes**  
---  
Icon | Output  
![03 new metanode green](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_new_metanode_green.png) | Successfully executed All outputs available  
![03 new metanode yellow](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_new_metanode_yellow.png) | All nodes of the sub-workflow are configured All outputs connected  
![03 new metanode red green](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_new_metanode_red-green.png) | All nodes of the sub-workflow are executed Contains unconnected output ("red" dot)  
![03 new metanode yellow green](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_new_metanode_yellow-green.png) | All nodes of the sub-workflow are executed One branch failed and the corresponding output is empty ("yellow" dot)  
## Change the flow variables scope in components 
Flow variables that are created inside the component have a local scope and are only available inside the component. Flow variables that are not created within the component are only available outside the component.
To change this you need to specifically allow the flow variable to exit or enter the component:
  1. Right-click the component in the executed state and choose _Component_ → _Open component_ from the context menu
  2. From inside the component right-click the Component Output node and select _Configure_ if you want a locally created flow variable to exit the component, or the Component Input node to allow an externally created flow variable to be available within the component
  3. Add the desired flow variable in the _Include_ column on the right pane in the window that opens, shown in Figure 9.
![03 exit flow variable](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_exit_flow_variable.png)
Figure 9. The Component Output node configuration dialog


## Custom components configuration dialogs 
Components can be designed in a way that the user can configure it from the outside, without having to change the configuration of the nodes inside it, once the component is saved and shared. This is done by using the Configuration Nodes which help to expose the necessary settings to the outside, through the component configuration dialog.
In this section, we explain how to create a custom configuration dialog for a component using Configuration nodes.
You can also change the order of the panes in the layout of the configuration dialogs. For more details about how to do this please refer to the Layout of configuration dialogs section.
### Configuration nodes 
A Configuration node can provide input parameters for other nodes in the workflow. If you use one or more Configuration nodes inside a component, the configuration dialog of the component will show all these configuration options you created inside it in its custom configuration dialog.
Configuration nodes enable different types of user inputs such as string input, integer input, selecting one value in a list and more. To access configuration nodes, navigate to the node repository within the KNIME Analytics Platform. Type the key word "configuration" into the search bar, as shown in Figure 10. You can narrow down the results by selecting the tag "Configuration" below the search bar.
![04 configuration nodes](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_configuration_nodes.png)
Figure 10. The Configuration nodes (Input and Selection) in the node repository
KNIME nodes are divided into two categories, which are explained in the Table 3.
Table 3. Configuration nodes Icon | Configuration node | User input | Output  
---|---|---|---  
**Input Nodes**  
![04 boolean](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_boolean.png) | Boolean | Boolean values | Checked = true Unchecked = false  
![04 string](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_string.png) | String | Any user input is accepted | String  
![04 integer](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_integer.png) | Integer | Integer values | Integer  
![04 integer slider](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_integer_slider.png) | Integer Slider | Value on a slider | Integer  
![04 double](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_double.png) | Double | Floating point numbers | Double  
![04 date time](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_date_time.png) | Date&Time | A date and time (as string) (or a selected date and time from the calendar form). | String  
![04 credentials](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_credentials.png) | Credentials | User credentials (user name and password) for later use in authenticated nodes. | Credentials Flow Variable  
![04 local file browser](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_local_file_browser.png) | Local File Browser | Select one or multiple local files. | Table with paths to selected items (as knime:// protocol). First path is also output as flow variable.  
![04 repository file chooser](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_repository_file_chooser.png) | Repository File Chooser | Select one or multiple local files, workflows or folders. | Table with paths to selected items (as knime:// protocol). First path is also output as flow variable.  
![04 list box](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_list_box.png) | List Box | Separate string inputs | Data table with a column of string values  
**Selection Nodes**  
![04 single selection](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_single_selection.png) | Single Selection | Choice of the available values. The available selection depends on the node’s configuration. | String  
![04 multiple selection](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_multiple_selection.png) | Multiple Selection | Multiple selections | Data table with a column of selections  
![04 column selection](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_column_selection.png) | Column Selection | Column name | String  
![04 value selection](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_value_selection.png) | Value Selection | Value in a column | String  
![04 column filter](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_column_filter.png) | Column Filter | Select columns from a data table | Data table with selected columns  
![04 nominal row filter](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_nominal_row_filter.png) | Nominal Row Filter | Value in a column | Data table with row filtered according to the selected value  
Figure 11 shows, for example, the configuration dialog of the Value Selection Configuration node, where you can define the input label, description, default selection option, and some visual properties. Here, you can also define the name and default value of the output flow variable, along with other settings to control the appearance of the custom dialog, if the Configuration node is used inside a component, as explained in the next section.
![04 value selection configuration dialog](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_value_selection_configuration_dialog.png)
Figure 11. Configuration dialog of the Value Selection Configuration node
Another node can access the flow variable output of a Configuration node, if the flow variable output of the Configuration node is connected to it, as shown in Figure 12. The flow variable created in the Configuration node as output, will then be used to overwrite the settings of the connected node. To know how to do this, please refer to the Overwriting settings with flow variables section of the KNIME Flow Control Guide. The value of the output of the Configuration node is either its default value defined in the Configuration node configuration dialog, or corresponds to the value provided by the user in the custom component configuration dialog.
![04 component workflow](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_component_workflow.svg)
Figure 12. Configuring a node with a value defined by a user
|  This workflow is also available on the KNIME Hub.  
---|---  
### Component configuration dialog 
Configuration nodes that are contained in a component, represent a customized configuration dialog.
Once you create a component, like the one shown in Figure 12, right-click the component and select _Configure_ from the context menu to open the configuration dialog, shown in Figure 13, and configure the component’s parameters.
![04 component configuration dialog](https://docs.knime.com/latest/analytics_platform_components_guide/img/04_component_configuration_dialog.png)
Figure 13. The component configuration dialog
You can also combine different Configuration nodes in one component and have a more complex component configuration dialog, where different parameters can be configured.
## Scripting Components 
KNIME provides a possibility to implement desired component functionality through scripting by supporting a number of scripting frameworks. You will also have the possibility to integrate dependencies with the scripted component. Given below are the nodes that can be used to script a desired component.
Table 4. KNIME Scripting Nodes Node | Description  
---|---  
![04b python script node](https://docs.knime.com/latest/analytics_platform_components_guide/img/04b_python_script_node.png) |  It offers a code editor for Python to process any number and type of inputs into outputs. KNIME executes the Python installation configuration either from the node settings and/or from KNIME Preferences.  
![04b R snippet node](https://docs.knime.com/latest/analytics_platform_components_guide/img/04b_R_snippet_node.png) |  It offers a code editor for R to process a KNIME table. KNIME executes the R installation configuration either in the node settings and/or in KNIME Preferences.  
![04b javascript view](https://docs.knime.com/latest/analytics_platform_components_guide/img/04b_javascript_view.png) |  It offers a code editor for JavaScript to implement a customized view. Optionally, you may feed in data to visualize it based on your implementation. The node offers checkboxes for a few dependencies (d3.js, ..) as well as a CSS editor.  
![04b conda environment propogation](https://docs.knime.com/latest/analytics_platform_components_guide/img/04b_conda_environment_propogation.png) |  It automatically installs the Conda environment necessary for your component to execute the downstream R/Python nodes. The environment usually includes the R/Python installation plus precise versions of the libraries.  
## Components composite views 
Besides custom configuration dialogs, components can have their own custom composite views. Composite views contain the interactive views of Widget nodes, and Interactive Widget nodes and View nodes, that are part of a component.
|  All composite views on root level also define a web application, accessible through KNIME Hub.  
---|---  
To inspect the composite view in KNIME Analytics Platform, as for any KNIME node that outputs a view, right-click the component and select _Open view_ after execution.
You can also customize the layout of the composite views. For more details about how to do this please refer to the Layout of composite views section.
In the next sections we will explain how to use Widget nodes, Interactive Widget nodes and View nodes to build a customized composite view.
### Widget nodes 
Widget nodes, similarly to Configuration nodes can provide input parameters for other nodes in the workflow. However, unlike Configuration nodes, Widget nodes are shown as widgets in the composite views. When inspecting the composite view in KNIME Analytics Platform in the window that opens you can adjust the parameters, and on the right bottom corner of the window:
  * Click _Apply_ to set these parameters for the current execution of the workflow
  * Choose _Apply as new default_ from the drop-down menu next to _Apply_ button to set these parameters as the new default parameters for the Widget nodes
  * Click _Close_ and, choose to either discard changes, apply settings temporarily or apply settings as new default
  * Choose to _Close & Discard_, _Close & Apply temporarily_ or _Close & Apply as new default_ from the drop-down menu next to _Close_ button.


Additionally, when the workflow is deployed to KNIME Hub, Widget nodes allow you to set parameters for the workflow execution.
You can find all available Widget nodes in the node repository. Type the key word "widget" into the search bar, as shown in Figure 14. You can narrow down the results by selecting the tag "Widgets" below the search bar.
![05 widget nodes](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_widget_nodes.png)
Figure 14. The Widget nodes in the node repository
You can also access the Widget nodes on the KNIME Quick Forms Extension Hub page.
The Widget nodes are divided into the following five categories:
  * _Input_ : you can use widgets in this category to input parameters of different type into the workflow. For example you can input integers, strings, booleans, doubles, lists, but also other formats like date&time or credentials. They are shown in Table 5.


Table 5. Widget input nodes Icon | Widget node | User input | Output  
---|---|---|---  
**Input Nodes**  
![05 boolean](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_boolean.png) | Boolean | Boolean values | Checked = true Unchecked = false  
![05 string](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_string.png) | String | Any user input is accepted | String  
![05 integer](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_integer.png) | Integer | Integer values | Integer  
![05 double](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_double.png) | Double | Floating point numbers | Double  
![05 date time](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_date-time.png) | Date&Time | A date and time (as string) (or a selected date and time from the calendar form). | String  
![05 credentials](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_credentials.png) | Credentials | User credentials (user name and password) for later use in authenticated nodes. | Credentials Flow Variable  
![05 file upload](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_file-upload.png) | File Upload | Upload a file to the server using a temporary folder. | Path to the uploaded file  
![05 file chooser](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_file-chooser.png) | File Chooser | Select one or multiple remote files, workflows or folders. | Table with paths to selected items (as knime:// protocol). First path is also output as flow variable.  
![05 list box](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_list-box.png) | List Box | Separate string inputs | Data table with a column of string values  
![05 slider](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_slider.png) | Slider | Value on a slider | Double  
![05 molecule widget](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_molecule_widget.png) | Molecule * | Molecule string in specified format, e.g., SMILES notation | Molecule string in specified format (can be edited). Molecule can be sketched.  
* Requires extension(s)
  * _Selection_ : you can use the widgets in this category to select input values from an available list of values. For example, you can choose a specific column from a data table, multiple columns to include or exclude from a dataset or select a value of a chosen column to filter a data table. You can also enable the choice of single or multiple values from a list, or a single value from a single column of a data set. They are shown in Table 6.


Table 6. Widget selection nodes Icon | Widget node | User input | Output  
---|---|---|---  
**Selection Nodes**  
![05 single selection](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_single_selection.png) | Single Selection | Choice of the available values. The available selection depends on the node’s configuration. | String  
![05 multiple selection](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_multiple_selection.png) | Multiple Selection | Multiple selections | Data table with a column of selections  
![05 column selection](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_column_selection.png) | Column Selection | Column name | String  
![05 value selection](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_value_selection.png) | Value Selection | Value in a column | String  
![05 column filter](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_column_filter.png) | Column Filter | Select columns from a data table | Data table with selected columns  
![05 nominal row filter](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_nominal_row_filter.png) | Nominal Row Filter | Value in a column | Data table with row filtered according to the selected value  
  * _Output_ : you can use these widgets to either produce a link to download files or to display images or dynamic text. They are shown in Table 7.


Table 7. Widget output nodes Icon | Widget node | User input | Output  
---|---|---|---  
**Output Nodes**  
![05 file download](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_file_download.png) | File Download | A flow variable storing an absolute file path | String  
![05 image output](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_image_output.png) | Image Output | KNIME Image | SVG or PNG image  
![05 text output](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_text_output.png) | Text Output | Any text | String or HTML content  
  * _Filter_ : you can use these widgets to trigger interactive filter events in a layout of views. In the next section these interactive widget nodes are explained in more details.
  * _Re-execution_ : you can use the Refresh Button Widget node to add a button widget with configurable text to the composite view of the component. When the user clicks the button it will emit reactivity events that trigger re-execution of the component downstream nodes. In the Refresh Button Widget node section you will find more detailed information on the functionality and outcome of the reactivity functionality of the Refresh Button Widget.


You can arrange different Widget nodes in a composite view, enclosing them into a component, where ideally you could adjust different parameters to be injected into the workflow.
### View nodes 
You can use View nodes to visualize your data as charts, plots, tables. You can choose between two KNIME extensions to leverage their potential, as described in the following.
#### KNIME Views Extension 
The KNIME Views Extension for the KNIME Analytics Platform provides nodes for creating interactive visualizations within workflows. Interactivity between multiple views is currently only possible for views coming from the KNIME Views Extension.
Type the key word "view" in the search bar. You can narrow down the results by selecting the tag "Views" below the search bar, shown in Figure 15.
![05 view nodes repository](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_nodes_repository.png)
Figure 15. The View nodes in the node repository
You can also access the KNIME Views Extension on the KNIME Hub.
One example of a component made of four different View nodes is shown in Figure 16.
![05 component views](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_component_views.png)
Figure 16. A sub-workflow of a component combining different View nodes
Once this component has been executed, right-click, select _Component_ → _Open view_ in the KNIME Analytics Platform. An interactive dashboard like the one shown in Figure 17 will appear. The layout can be adjusted as explained in the Layout of composite views section and different elements can be added like text or images, with the use of Widget nodes.
![05 composite view](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_composite_view.png)
Figure 17. The composite view of a component combining different View nodes
Each node’s configuration dialog provides:
  * A preview pane to adjust settings based on how they affect the visualization
  * An input field where you can set a title


|  Missing, infinite or not a number (NaN) values are excluded in all View nodes except for the Histogram node. You can choose different ways of handling them in the configuration dialog.   
---|---  
The extension includes the nodes shown in Table 8. All nodes have the option to output a rendered PNG image of the generated visualization.
Table 8. KNIME Views Extension nodes Icon | View node | Input | Output  
---|---|---|---  
![05 bar chart](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_bar_chart.png) |  Bar Chart |  Data table containing the categories and values to be plotted in a bar chart |  ![05 view output 01 Example for Bar Chart](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_01_Example_for_Bar_Chart.png)  
![05 box plot](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_box_plot.png) |  Box Plot |  Data table containing the dimensions and conditions to be plotted in a box plot |  ![05 view output 02 Example for Box Plot](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_02_Example_for_Box_Plot.png)  
![05 density plot](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_density_plot.png) |  Density Plot |  Data table containing the dimension and condition column to be plotted in a density plot |  ![05 view output 07 Example for Density Plot](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_07_Example_for_Density_Plot.png)  
![05 heatmap](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_heatmap.png) |  Heatmap |  Data table containing the categories and values to be plotted in a heatmap |  ![05 view output heatmap](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_heatmap.png)  
![05 histogram](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_histogram.png) |  Histogram |  Data table containing the values to be plotted in a histogram |  ![05 view output histogram](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_histogram.png)  
![05 image view](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_image_view.png) |  Image View |  The image data to display |  ![05 view output image view](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_image_view.png)  
![05 line plot](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_line_plot.png) |  Line Plot |  Data table with data to display |  ![05 view output 03 Example for Line Plot](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_03_Example_for_Line_Plot.png)  
![05 parallel coordinates plot](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_parallel_coordinates_plot.png) |  Parallel Coordinates Plot |  Data table with data to display |  ![05 view output parallel coordinates plot](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_parallel_coordinates_plot.png)  
![05 pie chart](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_pie_chart.png) |  Pie Chart |  Data table containing the categories and values to be plotted in a pie chart |  ![05 view output 04 Example for Pie Chart](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_04_Example_for_Pie_Chart.png)  
![05 roc curve](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_roc_curve.png) |  ROC Curve (Receiver Operating Characteristic Curve) |  Data table with data to display |  ![05 view output roc](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_roc.png)  
![05 scatter plot](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_scatter_plot.png) |  Scatter Plot |  Data table with data to display |  ![05 view output 05 Example for Scatter Plot](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_05_Example_for_Scatter_Plot.png)  
![05 scatter plot matrix](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_scatter_plot_matrix.png) |  Scatter Plot Matrix |  Data table with data to display |  ![05 view output scatterplot matrix](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_scatterplot_matrix.png)  
![05 stacked area chart](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_stacked_area_chart.png) |  Stacked Area Chart |  Data table containing the categories and values to be plotted in a stacked area chart |  ![05 view output 06 Example for Stacked Area Chart](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_06_Example_for_Stacked_Area_Chart.png)  
![05 statistics](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_statistics.png) |  Statistics View |  Data table with data to display |  ![05 view output statistics](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_statistics.png)  
![05 sunburst chart](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_sunburst_chart.png) |  Sunburst Chart |  Data table with data to display hierarchical data in a radial layout |  ![05 view output sunburst chart](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_sunburst_chart.png)  
![05 table view](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_table_view.png) |  Table View |  Data table with data to display |  ![05 view output table view](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_table_view.png)  
![05 generic echart](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_generic_echart.png) |  Generic ECharts View |  Data table with data to display |  ![05 view generic echart templates](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_generic_echart_templates.png)  
![05 text view](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_text_view.png) |  Text View |  No input port. Rich text can be directly added in the node configuration dialog |  ![05 view output text view](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_view_output_text_view.png)  
#### KNIME Generic ECharts View Node 
The Generic ECharts View node is a special View node that combines the functionality of multiple views, e.g. Bar Chart, Line Plot, Scatter Plot and many more. It contains a script editor where you can write your own JavaScript code to generate a view with the Apache ECharts library. You can also use the templates provided in the node configuration dialog (see Figure 18). In addition to the templates, more examples can be found on the Apache ECharts website.
![05 generic echart node view](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_generic_echart_node_view.png)
Figure 18. The Generic ECharts View node configuration dialog
Furthermore, when logged into KNIME Hub, you can utilize the KNIME AI Assistant to generate JavaScript. To do so there is a button called 'Ask K-AI' in the node configuration dialog. Clicking on it will open a dialog where you can ask the AI Assistant to generate a chart for you, for instance, by typing in a question like "Generate a scatter plot for the two universes in the input data", see Figure 19. You can then press 'Insert in editor' to insert the generated code into the script editor. The view on the right side will automatically update. You can then adjust the code to your needs or try a follow-up prompt.
For more information on the KNIME AI Assistant please refer to K-AI.
|  Upon utilizing the KNIME AI Assistant, be aware that the current code from the editor, the input data’s schema, and the prompt are sent over the internet to the configured KNIME Hub and OpenAI, which is a consideration for data privacy. This transmission is necessary for the AI to tailor code suggestions accurately to your script’s context and the data you are working with.   
---|---  
![05 echart prompt](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_echart_prompt.png)
Figure 19. The Generic ECharts View node with a K-AI prompt
#### KNIME JavaScript Views Extension 
You can use nodes from the JavaScript Views Extension to visualize your data as charts, plots, tables, or visualize your own views generated from JavaScript code. The nodes are available in the node repository. Type the key word "javascript" in the search bar. Narrow down the results by selecting the tags "JavaScript" and "Views" below the search bar until the node repository looks like Figure 20.
![05 js view nodes repository](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_js_view_nodes_repository.png)
Figure 20. The nodes from the JavaScript Views Extension in the node repository
You can also access the KNIME JavaScript Views Extension on the KNIME Hub.
These nodes can also be combined together in a component in order to build composite views and dashboards.
### Refresh Button Widget node 
You can use the Refresh Button Widget node within a component to add a button widget with configurable text to its composite view. When user clicks the resulting button in the composite view a reactivity event will trigger re-execution of the component downstream nodes. This will result in the update of the visualizations of the composite view corresponding to the Widget and the View nodes that are in the branch downstream to the Refresh Button Widget node. To use the node, connect the flow variable output port to the nodes which should be re-executed. The downstream nodes of those connected nodes will also be re-executed when the widget is clicked.
Please note that the Refresh Button Widget works in KNIME Analytics Platform only when using the Chromium Embedded Framework as browser for displaying JavaScript views. In case Chromium Embedded Framework is not set as the default you need to configure it. To do so go to _File_ > _Preferences_ and find _JavaScript Views_ under _KNIME_. Set the first pane to _Chromium Embedded Framework (CEF) Browser_ as shown in Figure 21.
![img cef](https://docs.knime.com/latest/analytics_platform_components_guide/img/img-cef.png)
Figure 21. Setting KNIME Analytics Platform preferences to use CEF Browser for displaying _Views_
|  The re-execution functionality is available only if the legacy flag in the _Composite View Layout_ tab of the _Node Usage and Layout_ window is deactivated.   
---|---  
The example in Figure 22 shows the workflow bundled in a component. The first branch (yellow rectangle) has a Refresh Button Widget node, connected via flow variable port to a Column Selection Widget node and a Box Plot node. The second branch instead does not have the Refresh Button Widget node connected. In the resulting composite view is possible to choose the columns to be plotted in the Box Plot views. When clicking the Refresh button only the first Box Plot view is showing the selected column.
![05 refresh widget](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_refresh_widget.png)
Figure 22. An example of a re-executable branch
### Re-execution of Widget nodes 
Complementary to the functionality of the Refresh Button Widget node it is also possible to configure some of the Widget nodes (Selection Widget nodes and Boolean Widget node) so that the change in their value in the composite view upon user interaction will directly trigger re-execution of the component downstream nodes. This will result in the update of the visualizations of the composite view corresponding to the Widget and the View nodes that are in the branch downstream to the re-executable Widget node. In order to activate this option open the Widget node configuration dialog, go to _Re-execution_ tab, and check the option _Re-execution on widget value change_ , as shown in Figure 23.
![05 re execution config](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_re-execution-config.png)
Figure 23. The re-execution tab of a re-executable Widget node
When the re-execution option is selected the Widget node will have the icon on the right upper corner of the node, as shown in Figure 24.
![05 re executable icon](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_re-executable-icon.svg)
Figure 24. The re-executable Widget node
|  Please note that the re-execution of Widget nodes works in KNIME Analytics Platform only when using the Chromium Embedded Framework as browser for displaying JavaScript views. In case Chromium Embedded Framework is not set as the default you need to configure it. To do so go to _File_ > _Preferences_ and find _JavaScript Views_ under _KNIME_. Set the first pane to _Chromium Embedded Framework (CEF) Browser_ as shown in Figure 21.   
---|---  
|  The re-execution functionality is available only if the legacy flag in the _Composite View Layout_ tab of the _Node Usage and Layout_ window is deactivated.   
---|---  
### Interactive Widget nodes 
Interactive Widget nodes are special Widget nodes that can be combined together with View nodes in order to build composite views where you are allowed to interactively filter the data visualized in the View node(s) connected to them, acting on the different elements which are integrated in the composite view.
#### Interactive Range Slider Filter Widget 
This Interactive Widget node shows a slider in a composite view. You can define the column to be filtered according to the slider and the range, together with different configurations and settings, in the node configuration dialog.
An example about how to use this Interactive Widget node is available on the KNIME Hub and is shown in Figure 25.
![05 wf interactive range slider filter widget](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_wf_interactive_range_slider_filter_widget.svg)
Figure 25. An example component where the Interactive Range Slider Filter node is used
Two snapshots of the interactive composite view are shown in Figure 26. The range of values that are filtered and plotted is adjusted with the slider.
![05 interactive range slider filter widget outputs](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_interactive_range_slider_filter_widget_outputs.png)
Figure 26. Two possible snapshots of the interactive composite view of a component using Interactive Range Slider Filter node
#### Interactive Value Filter Widget 
This Interactive Widget node shows a filter in a composite view. You can define the column to which the values are filtered. You can also have different configurations for this widget like choose a single or multiple values, and other settings, that you can configure in the node configuration dialog.
An example about how to use this Interactive Widget node is available on the KNIME Hub and is shown in Figure 27.
![05 wf interactive value filter widget](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_wf_interactive_value_filter_widget.svg)
Figure 27. An example component where the Interactive Range Slider Filter node is used
Two snapshots of the interactive composite view are shown in Figure 28. The values of the chosen column that are included or excluded and plotted is adjusted with the include/exclude element on the top.
![05 interactive value filter widget outputs](https://docs.knime.com/latest/analytics_platform_components_guide/img/05_interactive_value_filter_widget_outputs.png)
Figure 28. Two possible snapshots of the interactive composite view of a component using Interactive Value Filter node
## Layout of composite views 
Any component that contains at least one Widget or JavaScript view node can have a layout defined. The layout can be edited within the _Composite View Layout_ tab of the _Node Usage and Layout_ window.
To access the layout editor you can either:
  * Open the component and click the _Open layout editor button_ , as shown in Figure 29.


![06 component layouting toolbar](https://docs.knime.com/latest/analytics_platform_components_guide/img/06_component_layouting_toolbar.png)
Figure 29. The layout editor button in the toolbar
### Visual layout editor 
The visual layout editor allows you to create and adjust layouts using a drag & drop grid.
  * A layout consists of one or more rows. Each row can have up to twelve columns.
  * A column can be resized when there is more than one column in a row
  * One or more views can be added to a column
  * By default the position of widgets and views in the custom view follows the position of the corresponding nodes in the component sub-workflow, from top to bottom.


The visual layout editor, shown in Figure 30, consists of a left panel which shows a list of all Widget and View nodes in your component that have not yet been added to the layout and an interactive preview of the layout on the right.
![06 component layouting editor](https://docs.knime.com/latest/analytics_platform_components_guide/img/06_component_layouting_editor.svg)
Figure 30. The visual layout editor of a component
To add a view, drag it from the left panel (1) to the desired position in the layout preview.
To add a column, click the '+' button (2) in the layout preview.
To resize a column, click and move the resize handle between columns (3).
To add a row, drag a row template (4) from the left panel to the desired position in the layout preview. You can choose between different templates, e.g. 1-column, 2-column, 3-column or you can add and remove columns later on.
To delete a view, column or row use the trash bin button (5). This is only available for columns and rows when they are empty, i.e. do not contain widgets or views.
To move a view into another column drag it to the layout preview. Complete rows can also be moved by dragging.
Note that nesting is possible. Columns can contain rows as well as views, those nested rows can contain columns, rows, and views, and so on.
You can also adjust the height of the views. Each view has default sizing settings which can be changed via the cog icon (6) in the layout preview. You can choose between automatic height based on the content of the view or aspect ratio sizing (`16:9`, `4:3` or `square`). When using automatic height it is possible to define minimal and maximal pixel sizes.
If you have switchable views or widgets within your component, for example if you are using a Refresh Button Widget node combined with IF and CASE Switches, you need to insert **all** the switchable views and widgets in your layout by positioning them within the same cell of the Composite View Layout, as shown in Figure 31.
![img switchable views](https://docs.knime.com/latest/analytics_platform_components_guide/img/img-switchable-views.png)
Figure 31. The visual layout editor of a component with switchable views
#### Layouting composite views with switchable views 
When building a component to be deployed as a page of a Data App you might want to give the final Data App user the possibility to choose the type of visualization they want to see in a specific position of the page. To do this you can use for example IF and CASE Switches in order to enable the user to alternatively select a visualization. When building such an application you need to insert all the View or Widget nodes that you might want to show in a specific position on the page.
### Enable the reporting function of a component 
The KNIME Reporting Extension KNIME Reporting Extension allows you to create and share static reports based on the results of the component’s composite view of your workflows.
To use this functionality, navigate to the left side of the _Composite View Layout_ tab of the _Node Usage and Layout_ window and check the _Enable Reporting_ button, number (7) in Figure 30.
See the KNIME Reporting Guide to learn more about the reporting function in the KNIME Analytics Platform.
### Legacy flag 
On the left side of the _Composite View Layout_ tab of the _Node Usage and Layout_ window a _Use legacy mode_ button, number (8) in Figure 30, is available.
The Widget nodes user interface has been improved starting from KNIME Analytics Platform version 4.2.
When creating components with KNIME Analytics Platform version 4.2 the legacy mode is deactivated by default. This means that the composite views are visualized with the new improved user interfaces for the Widget nodes.
Instead, for components that have been created using Widget nodes in KNIME Analytics Platform version 4.1 and earlier, the legacy mode is activated by default. The composite views will have the previous user interface as in KNIME Analytics Platform version 4.1 and earlier.
It is always possible to check/uncheck the _Use legacy mode_ checkbox to visualize the composite views with old/new Widget node style. This is found in the _Composite View Layout_ tab of the layout editor for the component view, number (8) in Figure 30.
Alternatively, in the _Advanced Composite View Layout_ tab of the layout editor, this property can be enabled/disabled on a node-by-node basis (see (8) and (9) in the next section).
### Advanced layouting 
The layout structure is saved in a JSON format which advanced users can edit directly in the _Advanced Composite View Layout_ tab.
An example of JSON format generated by the visual layout editor is shown in Figure 32.
![06 component layouting editor advanced](https://docs.knime.com/latest/analytics_platform_components_guide/img/06_component_layouting_editor_advanced.svg)
Figure 32. Component advanced layouting in JSON Format
#### Row (1) 
A row is the outer most element that can be defined and is the first element inside the layout container. The JSON structure’s outer layer is an array of rows. A row contains a number of layout-columns.
To further customize a row you can add optional fields. With `additionalClasses` you can provide an array of class names to append to the created HTML row element, `additionalStyles` (2) is an option to directly insert CSS style commands on the element. For example, to create a visual separator between one row and the next, you can add a bottom border:
```
"additionalStyles" : [ "border-bottom: thin solid grey;" ]
```

The grey line that appears in the custom view output of the component is shown in Figure 33.
![06 component layouting additional view](https://docs.knime.com/latest/analytics_platform_components_guide/img/06_component_layouting_additional_view.png)
Figure 33. Custom view output of a component with additional styling
#### Column (3) 
A column is a layout element inside a row which determines the width of its components. To define a width, use a number between 1 and 12. 12 means taking up 100% of the width, whereas 6 would be 50% of the width. In this way it is possible to define a layout with components side by side by providing their relative widths. For example, if three components are to be laid out horizontally with equal column widths use a row with three columns, each of width 4. If the sum of widths for a particular row is larger than 12, the extra columns are wrapped onto a new line.
#### Responsive layouts (4) 
It is also possible to define multiple widths of the columns so that they can adapt to the screen size. With this option responsive layouts can be achieved.
To define the responsive width of a column, use at least `widthXS` and one or more of the following fields: `widthSM`, `widthMD`, `widthLG`.
The content of a column can be an array of one of any of the following: . Another set of rows, providing the possibility to create nested layouts . Regular HTML content, to insert plain HTML elements into the layout . A node reference to embed the contents of a JavaScript-enabled KNIME node.
As for rows, it is also possible to further customize the column using the optional fields `additionalClasses` and `additionalStyles`.
#### HTML content 
It is possible to include plain HTML into the layout by placing a content element of type `html` inside a column. To insert the content a single field `value` is used.
For example:
```
[...]
"content":[{
  "type":"html",
  "value":"<h2 >Title defined in layout</h2>"
 }]
[...]
```

#### View content (5) 
To embed the contents of a KNIME node inside the layout, you can use a content element with type `view`. The element has quite a few ways to customize the sizing and behavior of the content, which are explained in Table 9.
Referencing the node is done by the field `nodeID` (6), which takes the ID-suffix of the node as a string argument. If nodes exist inside the component which are not referenced by the layout, a warning message appears underneath the editor. Errors will also be issued for referencing nodes twice or referencing non-existing nodes.
The content of each is wrapped in its own `iframe` element, allowing to encapsulate the implementation and avoid reference and cross-scripting issues. As `iframe` elements do not adapt to the size of their content automatically, you need to resize them to achieve the desired behavior. To achieve this result you have the following options:
  1. Size-based methods: This method uses an iframe-resizer library to resize the `iframe` according to the size of its contents. You will need to explicitly or implicitly set a concrete size for the content. You can determine the size using different approaches, as explained on the iframe-resizer GitHub page. Size-based resize methods all start with the prefix `View` in the JSON structure.
  2. Aspect-ratio based methods: If a node view is set to adapt to its parent size, rather then implicitly providing a size, the size-based methods will either not work properly. To allow these views to take up an appropriate amount of space in the layout an aspect ratio setting can be used. Here the width is taken as 100% of the horizontal space available at that position in the layout and the height is calculated according to the given ratio. Aspect-ratio based resize methods start with the prefix `aspectRatio` in the JSON structure.
  3. Manual method: You can also trigger manually resize events at appropriate times. This requires the implementation of the node to make the appropriate resize calls itself.


In the table below a list of available fields to personalize the `view` content (7) is shown.
Table 9. Available fields to personalize the `view` content Field name | Explanation / Possible Values  
---|---  
`nodeID` | ID-suffix of referenced node  
`minWidth` | Constrain the size of the `iframe` by setting a minimum width in pixels.  
`minHeight` | Constrain the size of the `iframe` by setting a minimum height in pixels.  
`maxWidth` | Constrain the size of the `iframe` by setting a maximum width in pixels.  
`maxHeight` | Constrain the size of the `iframe` by setting a maximum height in pixels.  
`resizeMethod` | The resize method used to correctly determine the size of the `iframe` at runtime. Can be any of the following values: `viewBodyOffset`, `viewBodyScroll`, `viewDocumentElementOffset`, `viewDocumentelementScroll`, `viewMax`, `viewMin`, `viewGrow`, `viewLowestElement`, `viewTaggedElement`, `viewLowestElementIEMax`, `aspectRatio4by3`, `aspectRatio16by9`, `aspectRatio1by1`, `manual`  
`autoResize` | Boolean only working with size based resize methods. Use this to enable or disable automatic resizing upon window size or DOM changes. Note that the initial resize is always done.  
`resizeInterval` | Number only working with size based resize methods. Sets the interval to check if resizing needs to occur. The default is `32` (ms).  
`scrolling` | Boolean only working with size based resize methods. Enables or disables scroll bars inside `iframe`. The default is `false`.  
`sizeHeight` | Boolean only working with size based resize methods. Enables or disables size adaption according to content height. The default is `true`.  
`sizeWidth` | Boolean only working with size based resize methods. Enables or disables size adaption according to content width. The default is `false`.  
`resizeTolerance` | Number only working with size based resize methods. Sets the number of pixels that the content size needs to change, before a resize of the `iframe` is triggered. The default is `0`.  
`additionalClasses` | Array of additional classes added to the HTML container element.  
`additionalStyles` | Array of additional CSS style declaration added to the HTML container element.  
#### Parent legacy mode (8) (9) 
The `parentLayoutLegacyMode` (8) is activated (`true`) to allow all Widget nodes contained in the component to use the legacy mode. Each individual Widget node can also be unset from the legacy mode setting the option (9) `useLegacyMode` to `false`.
### Node Usage tab 
The first tab of the layout editor is the Node Usage tab. Here you can choose which Widget nodes to show in the composite view checking/unchecking them in the WebPortal/Component View column.
It is best practice to avoid the usage of Quickform nodes and use instead Configuration nodes and Widget nodes. However, in case you are using Quickform nodes in your components and you want to hide them you can do it in the Node Usage tab.
Check or uncheck the node in the _WebPortal/Component View_ column to show it or hide it from the composite view. Check or uncheck the node in the _Component Dialog_ column to show it or hide it from the configuration dialog.
### Layout of configuration dialogs 
The last tab of the layout editor is the _Configuration Dialog Layout_ and an example is shown in Figure 34.
![06 config dialog layout](https://docs.knime.com/latest/analytics_platform_components_guide/img/06_config_dialog_layout.png)
Figure 34. The _Configuration Dialog Layout_ tab of the layout editor
Here, you will find all the Configuration nodes that are part of the component. You can easily drag and drop them to resort their position which then they will have in the component configuration dialog.
## Streaming execution of components 
You can define the mode of execution of components, e.g. in which order and how the data are passed from one node to another in the sub-workflow. After installing the KNIME Streaming Execution (Beta) extension, in each component configuration dialog you can find a _Job Manager Selection_ tab. Not all nodes support streaming execution. However, streaming execution can be applied to entire sub-workflows inside a component. Notice that the non-streamable nodes can still be part of a sub-workflow inside a component, which is executed in the streaming mode. They will simply be executed in the default execution mode.
|  The KNIME Streaming Execution (Beta) is an extension available under KNIME Labs Extensions. Install it by navigating to _Menu_ - > _Install extensions_.  
---|---  
The execution can be performed in default or streaming mode which are described in the next sections.
### Default execution 
In the default execution mode, the operations in a workflow are executed node by node. Data are passed from one node to another after the entire input data of a node has been processed. The dataset passed to the next node is the intermediate output table, which you can inspect by opening the output table of a node in the middle of a workflow. If you open the _Job Manager Selection_ tab in any configuration dialog, and see the job manager selection `<<default>>`, then the node operation is executed according to the default execution mode.
### Streaming execution 
In the streaming execution mode, data is passed from one node to another as soon as it is available. That is, all data do not have to be processed at once, but they can be divided into a number of batches which are streamed one by one. Therefore, the streaming execution mode leads to a faster in-memory execution because only the rows in transit are concerned and intermediate tables are not stored.
To switch from default to streaming execution select _Simple Streaming_ under the _Job Manager Selection_ tab in the component or node configuration dialog. If the streaming execution option is not available for the node you will only see the `<<default>>` option in the menu.
Here, for streaming execution mode, you can also choose the size of the batch to be streamed, as shown in Figure 35.
|  Larger values will reduce synchronization, with better runtime, while small values will assure that less data is in transit.  
---|---  
![07 chunk size](https://docs.knime.com/latest/analytics_platform_components_guide/img/07_chunk_size.png)
Figure 35. Streaming execution mode and chunk size selection
#### Streaming execution of a component 
If you use the streaming mode to execute a component, the sub-workflow inside it is always executed entirely. The intermediate output tables of the nodes inside the component are not available, because they are not stored. Each batch of data is streamed one by one through the streamable nodes of the sub-workflow. If they reach a non-streamable node they will be stored there until all the batches have been streamed. Then they are again divided into batches and passed to the next streamable node(s).
The component and the streamable nodes in the component sub-workflow show a dashed black arrow on the right bottom corner, while the nodes show an X, if they are not, as shown in Figure 36. The numbers that appear above the connection between nodes refer to the number of records that have passed that particular connection.
![07 streaming execution](https://docs.knime.com/latest/analytics_platform_components_guide/img/07_streaming_execution.png)
Figure 36. Streaming execution inside a component
## Error Handling 
Error handling is a significant issue that needs to be addressed while building a workflow. While executing a workflow, you might encounter various errors. For instance, a failing connection to a remote service, the invocation of a non-accessible database etc. It is necessary to provide an error handling method that alerts if a node execution fails. While building a component, Breakpoint node as shown in Figure 37 can be used for error handling. The node can be used to detect whether the input or configurations of the component satisfy the minimum requirements. It can also be configured to provide a customized error message to the user about what should be fixed if the component execution fails. Thus, when specified conditions are not met the error message appears on the node and on the outside of the component.
![07b breakpoint node](https://docs.knime.com/latest/analytics_platform_components_guide/img/07b_breakpoint_node.png)
Figure 37. Breakpoint Node
## Edit components description 
As each component is a real KNIME node you can also change its description, provide a name and a description for output and input ports, and customize the component icon.
To do this open the component by either right-clicking it and going to _Component_ → _Open component_ in the context menu or "ctrl" + double-clicking it. Open the Description tab, shown in Figure 38, from the side panel navigation.
![03 component description](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_component_description.png)
Figure 38. The description panel of a component
Click on the pen icon on the right upper corner to change:
  * Description: you can insert here a description of the component
  * Type and icon: you can select a a square image file, png format of a minimum size of 16x16, and select the type from the drop-down menu, shown in Figure 39, to setup background color.
  * External resources: you can add links to useful resources.
  * Tags: add meaningful tags to the component
  * Name and Description of input and output ports: the name of input and output ports will also be visible on the input and output ports hover of the component in the workflow editor.


![03 change component description](https://docs.knime.com/latest/analytics_platform_components_guide/img/03_change_component_description.png)
Figure 39. Changing the description panel of a component
## Sharing components 
Components encapsulate functionalities that can be reused as your personal customized KNIME nodes, to perform tasks that you often repeat, or you can simply store them for further reuse for yourself. They can also be shared with others via KNIME Hub and KNIME Server.
After you create a component in a workflow, if you would like to reuse it in some other workflow you could copy paste it in to the new workflow. However, in this way, changes to the component in one workflow are not triggered to the others. You can obtain changes to the component to be applied to the others by sharing and linking components.
To share a component, right-click it, select _Component_ → _Share_ from the context menu, and choose the destination for the shared component in the window that opens, shown in Figure 40.
Here you can choose:
  * The mountpoint where to share the component
  * To include or exclude the input data eventually present with the component.


|  Please be aware that if input data are included they will be accessible to everyone who has access to the component itself.  
---|---  
![08 share component](https://docs.knime.com/latest/analytics_platform_components_guide/img/08_share_component.png)
Figure 40. The Save As Shared Component window
### Share components in local workspace 
If you choose to save a component in your local workspace you can have access to that component from your local KNIME Analytics Platform installation.
### Share components on the KNIME Hub 
You can also save and share components in the KNIME Hub. Here you can share into a private space, having therefore access to them logging in to your KNIME Hub profile, or into a public space, sharing your components with the KNIME Community.
### Share components on the KNIME Business Hub 
Similar to the KNIME Community Hub, you can also save and share components on the KNIME Business Hub. Here you can save the component to a team space and share it with the members of your team.
To save a linked component to the KNIME Business Hub, right-click it, select _Component_ → _Share_ from the context menu. In the Save As Shared Component window, select your Business Hub instance and the space where the component will be saved, as shown in Figure 41.
![08 save as shared component hub space](https://docs.knime.com/latest/analytics_platform_components_guide/img/08_save_as_shared_component_hub_space.png)
Figure 41. The KNIME Business Hub instance in the Save As Shared Component window
### Link type 
The link type defines how a workflow looks for the shared component when checking for updates.
After choosing the destination of your component, a dialog opens asking you to choose for the link type you want to use.
Upon saving a linked component locally, you have the following four possibilities:
  * Create absolute link: the workflow connects to the absolute location of the shared component
  * Create mountpoint-relative link: the workflow connects to the shared component based on the folder structure under which mountpoint the workflow lives. If you deploy a workflow to a KNIME Server you have to deploy the shared component as well, and keep the path from the mountpoint to the component the same.
  * Create workflow-relative link: the connection between the shared component and a workflow where an instance of it is used is based on the folder structure between the workflow and the shared component
  * Don’t create link with shared instance: creates a shared component but does not link the current instance to it.


If you share a component on the KNIME Server, the KNIME Community Hub or the KNIME Business Hub, you can only choose between creating an absolute link or not creating a link at all, as indicated by the Link Shared Component dialog shown in Figure 42.
![08 link shared component dialog hub server](https://docs.knime.com/latest/analytics_platform_components_guide/img/08_link_shared_component_dialog_hub_server.png)
Figure 42. The Link Shared Component dialog of a component about to be shared to a KNIME Server or Hub instance
After selecting the proper link type click _OK_ and the shared component appears in KNIME Explorer within the folder it was saved to, as shown in Figure 43.
![08 shared component local](https://docs.knime.com/latest/analytics_platform_components_guide/img/08_shared_component_local.png)
Figure 43. A shared component saved in the local workspace
### Use a shared component 
To use a shared component in a workflow, you can drag and drop it into the workflow editor. An arrow on the bottom left corner, shown in Figure 44 indicates that the component is an instance of a shared component. You can use the component as a KNIME node, using the functionalities that have been enveloped into it, like configure it or visualize its output(s) or interactive views.
![08 instance component](https://docs.knime.com/latest/analytics_platform_components_guide/img/08_instance_component.png)
Figure 44. The instance of a shared component in a workflow
### Edit the instance of a shared component 
To check the link type of a certain instance or to customize it for a specific instance, right-click the instance and choose _Component_ → _Change link type_ from the context menu. The dialog shown in Figure 45 will open. There, you can choose the new type of link.
![08 change link type](https://docs.knime.com/latest/analytics_platform_components_guide/img/08_change_link_type.png)
Figure 45. Change the link type of an instance of a shared component
When you open an instance of a shared component, a blue bar indicates that you cannot make any changes to the current instance of the component.
To be able to edit the instance, you must first unlink it from the shared component. Right-click the instance and select _Component_ → _Disconnect link_.
After disconnecting the component, you can resume making changes to it by clicking _Component_ → _Open_. To save these changes to the previously created shared component, right-click the edited component instance and select _Component_ → _Share_. Then select the folder in which the shared component is located and overwrite it. You also have the option of saving the changes to a new component with a different name.
To verify that the current instance of the component in use is the latest of the linked shared component, right-click the current instance and select _Component_ → _Update component_. If updates are available, you will be notified when opening a tab with a workflow where an instance of the component is used. Click _Update_ to update the current instance of the shared component.
### Edit a shared component 
You can edit a shared component by opening it directly from the space explorer. You can now modify the sub-workflow contained in the component by adding or deleting nodes, changing the parameters of individual nodes, or changing the layout of composite visualizations.
### Update linked components 
You can choose to automatically update the linked components that are eventually reused in a workflow. The first time you open a workflow that uses an instance of a shared component, you will be prompted to choose whether to check for updates to these components.
### Version a component shared to KNIME Hub 
You can create versions of your shared components so that you can return to a specific saved version at any point in the future to download the item in that specific version. Versioning a component works like versioning a workflow, as detailed in the KNIME Business Hub User Guide.
|  To version a component, it must first be shared on the KNIME Hub.   
---|---  
Once you have shared your component, access its location on the KNIME Hub. Alternatively, you can access the component through the KNIME Analytics Platform. Navigate to the component in the space explorer, right-click it, and select _Open in Hub_.
On the component Hub page, click _History_. A panel opens on the right, where you can see the unversioned changes to the component, as shown in Figure 46.
![08 version shared components](https://docs.knime.com/latest/analytics_platform_components_guide/img/08_version_shared_components.png)
Figure 46. Component history panel
To create a new version, make sure you are signed in, then click _Create version_. You can then name the version and add a description. After you click _Create_ , the new version of your component appears in the panel on the right. Any future edits to the component will appear there as unversioned changes, which can then be versioned again, as described above.
### Change component version to use 
|  This feature is currently available only in the classic user interface. Switch to classic user interface by clicking _Menu_ > _Switch to classic user interface_.   
---|---  
When working with a linked component in a specific workflow, you may want to specify which version to use in the current workflow. This is possible if the component is uploaded to a KNIME Hub instance and has different versions. To do so, right-click the component and select _Component → Change KNIME Hub Item Version_. In the _Select KNIME Hub Item Version_ dialog, you can choose between three options, as shown in Figure 47.
![08 update item version shared components](https://docs.knime.com/latest/analytics_platform_components_guide/img/08_update_item_version_shared_components.png)
Figure 47. Select KNIME Hub Item Version dialog
Specific version
If you select this option, the component instance is set to one specific version created on the Hub. In the current workflow, it will not be affected by any future changes.
Latest version
The component instance will be updated to the latest version created on the KNIME Hub. Unversioned changes are not included.
Working area
The component instance is updated whenever a change is made to the linked component. These changes do not need to be versioned yet.
## Components for Data Apps 
When you upload a workflow containing components to KNIME Hub, create a Data app deployment and execute it you are guided through the process in one or more pages. Each page corresponds to a component in the root level of the workflow, containing Widget nodes and View nodes.
### Widget nodes 
You can use Widget nodes inside a component to build a composite view that will be visualized as a web page on KNIME Hub. The use of Widget nodes is meant to set specific configurations, e.g. select a value from a specific column of a data table to filter by. In the web page you will be then able to enter values for specific parameters before proceeding with the workflow execution. These values are injected into the workflow, and used to parameterize its execution.
### Interactive Widget nodes and View nodes 
You can use Interactive Widget nodes and View nodes inside a component to build a composite view that will be visualized as a web page on KNIME Hub. The use of View nodes is meant to build specific visualizations, like tables, charts and plots, which are shown as a web page. Interactive Widget nodes can be also arranged together with the View nodes in order to interact with the visualizations directly on the web page.
### Re-execution and Refresh Button Widget node 
You can use Refresh Button Widget node inside a component to built a composite view that will be visualized as a interactively re-executable web page on KNIME Hub. The Refresh Button Widget node is used to add a button visual element to the data application that is able to re-execute specific nodes thus refreshing the desired visualizations.
You can find additional explanation on how to use KNIME Hub and how to build specific workflows in the Data Apps section of the KNIME Business Hub User Guide.
  * Introduction
    * Components vs metanodes
  * Creating components and metanodes
  * Setup components and metanodes
    * Setup components and metanodes
    * Reconfigure components and metanodes
    * Execution state of components and metanodes
  * Change the flow variables scope in components
  * Custom components configuration dialogs
    * Configuration nodes
    * Component configuration dialog
  * Scripting Components
  * Components composite views
    * Widget nodes
    * View nodes
    * Refresh Button Widget node
    * Re-execution of Widget nodes
    * Interactive Widget nodes
  * Layout of composite views
    * Visual layout editor
    * Enable the reporting function of a component
    * Legacy flag
    * Advanced layouting
    * Node Usage tab
    * Layout of configuration dialogs
  * Streaming execution of components
    * Default execution
    * Streaming execution
  * Error Handling
  * Edit components description
  * Sharing components
    * Share components in local workspace
    * Share components on the KNIME Hub
    * Share components on the KNIME Business Hub
    * Link type
    * Use a shared component
    * Edit the instance of a shared component
    * Edit a shared component
    * Update linked components
    * Version a component shared to KNIME Hub
    * Change component version to use
  * Components for Data Apps
    * Widget nodes
    * Interactive Widget nodes and View nodes
    * Re-execution and Refresh Button Widget node


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. KNIME Extensions Guide 


KNIME Analytics Platform 5.4
# KNIME Extensions Guide
KNIME ExtensionsCommunity ExtensionsUpdate Sites for Community ExtensionsPartner ExtensionsKNIME Extensions
  * KNIME Extensions
  * Community Extensions
    * Update Sites for Community Extensions
  * Partner Extensions


Download PDF
Extensions provide additional functionality such as **access to and processing of complex data types** , as well as the use of **advanced algorithms**.
Extensions are provided by several groups.
  * KNIME provides KNIME Extensions and also KNIME Labs Extensions. KNIME Labs Extensions come straight out of our labs, offering cutting-edge functionalities, but might not be finalized yet.
  * Partners provide extensions which might require an additional (commercial) license or which are licensed under a non-open source license.
  * Our community provides Trusted and Experimental Extensions. Trusted Extensions are fulfilling stricter requirements, thus the update site for Experimental Extensions is disabled by default.


|  The Extension Installation Guide explains how to install extensions.   
---|---  
|  Check which Update Sites are currently enabled in your installation. By default every update site is enabled except for `Community Extensions (Experimental) update site`.   
---|---  
## KNIME Extensions 
Open source KNIME Extensions are developed and maintained by KNIME. On our KNIME Extensions page you can see what functionalities to expect, whereas on KNIME Community Hub: KNIME Extensions you can see the list of all extensions developed by KNIME.
|  KNIME Labs Extensions are new extensions coming straight out of our lab whose functionality may not yet be finalized.   
---|---  
##  Community Extensions 
KNIME Community Extensions include functionality specific to various industries and domains such as chemo- and bioinformatics, and image processing.
Some of the Community Extensions are **Trusted Community Extensions**. These Extensions have been tested for backward compatibility and compliance with the KNIME usage model and quality standards. These Trusted Community Extensions are on the KNIME Community Extensions (Trusted) update site, which is enabled by default in _Available Software Sites_ (click _Preferences_ to open the _Preferences_ dialog → _Install/Update_ → _Available Software Sites_).
The stringent requirements for Trusted Community Extensions are not necessarily fulfilled by the **Experimental Community Extensions** , which come directly from the labs of our community developers. Find them on the Community Extensions (Experimental) update site, which is included (but not enabled!) in the Available Software Sites by default.
|  The Community Extensions (Experimental) update site is disabled by default. To install these Extensions, you have to enable the update site in the _Available Software Sites_ dialog.  
---|---  
###  Update Sites for Community Extensions 
As update sites you can either use the ones already stored or you use zipped update sites (e.g. because you are sitting behind a proxy). Here is an overview over the two latest update sites and the most current zipped update site for Trusted and Experimental Extensions as well as the oldest ones available. You can also construct the links of versions between them by replacing them with the corresponding KNIME Analytics Platform version.
**KNIME Community Extensions (Trusted)**
KNIME Analytics Platform Version | Update site link (not browsable, only to be used within KNIME Analytics Platform) | Zipped update site  
---|---|---  
5.4 | https://update.knime.com/community-contributions/trusted/5.4 | Zip File for 5.4  
5.3 | https://update.knime.com/community-contributions/trusted/5.3 | Zip File for 5.3  
5.2 | https://update.knime.com/community-contributions/trusted/5.2 | Zip File for 5.2  
5.1 | https://update.knime.com/community-contributions/trusted/5.1 | Zip File for 5.1  
4.7 | https://update.knime.com/community-contributions/trusted/4.7 | Zip File for 4.7  
**KNIME Community Extensions (Experimental)**
KNIME Analytics Platform Version | Update site link (not browsable, only to be used within KNIME Analytics Platform) | Zipped update site  
---|---|---  
5.4 | https://update.knime.com/community-contributions/5.4 | Zip File for 5.4  
5.3 | https://update.knime.com/community-contributions/5.3 | Zip File for 5.3  
5.2 | https://update.knime.com/community-contributions/5.2 | Zip File for 5.2  
5.1 | https://update.knime.com/community-contributions/5.1 | Zip File for 5.1  
4.7 | https://update.knime.com/community-contributions/4.7 | Zip File for 4.7  
**Nightly Builds - only relevant for Community Developers**
The nightly builds update site needs to be added manually to the list of Available Update Sites.
```
https://update.knime.com/community-contributions/trunk
```

|  KNIME does not take any responsibility for extensions provided by the community.  
---|---  
## Partner Extensions 
Partner extensions provide **additional capabilities offered and maintained by our partners**. They range from industry specific functionality to scientific software integrations. Some of the extensions are free, others require a license. These extensions are on the KNIME Partner Extensions Update Site, which is enabled by default.
  * KNIME Extensions
  * Community Extensions
    * Update Sites for Community Extensions
  * Partner Extensions


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. KNIME File Handling Guide 


KNIME Analytics Platform 5.4
# KNIME File Handling Guide
IntroductionBasic concepts about file systemsWorking directoryHidden filesPath syntaxKNIME Analytics Platform and file systemsStandard file systemsConnected file systemsRead and write from or to a connected file systemReader nodesWriter nodesPath data cell and flow variableCreating path data cellsManipulating path data cellsCreating path flow variablesString and path type conversionFile Folder Utility nodesTable based input Utility nodesCompatibility and migrationHow to work with workflows that contain both old and new file handling nodesHow to migrate your workflows from old to new file handling nodesIntroduction
  * Introduction
  * Basic concepts about file systems
    * Working directory
    * Hidden files
    * Path syntax
  * KNIME Analytics Platform and file systems
    * Standard file systems
    * Connected file systems
  * Read and write from or to a connected file system
    * Reader nodes
    * Writer nodes
  * Path data cell and flow variable
    * Creating path data cells
    * Manipulating path data cells
    * Creating path flow variables
    * String and path type conversion
  * File Folder Utility nodes
    * Table based input Utility nodes
  * Compatibility and migration
    * How to work with workflows that contain both old and new file handling nodes
    * How to migrate your workflows from old to new file handling nodes


Download PDF
## Introduction 
With the move towards cloud and hybrid environments we had to rethink and rewrite the existing file handling infrastructure in KNIME Analytics Platform to provide our users with a better user experience.
With KNIME Analytics Platform release 4.3 we introduced a new file handling framework thanks to which you can migrate workflows between file systems or manage various file systems within the same workflow in a more convenient way.
In this guide the following topics are covered:
  * Basic concepts concerning file systems
  * How to access different file systems from within KNIME Analytics Platform
  * How to read and write from and to different file systems and conveniently transform and adjust your data tables when importing them into your workflow
  * The new Path type and how to use it within the nodes that are build on the basis of the file handling framework
  * Finally, in the Compatibility and migration section you will find more detailed information about:
    * How to distinguish between the old and new file handling nodes
    * How to work with workflows that contain both old and new file handling nodes
    * How to migrate your workflows from old to new file handling nodes.


## Basic concepts about file systems 
In general, a file system is a process that manages how and where data is stored, accessed and managed.
In KNIME Analytics Platform a file system can be seen as a forest of trees where a folder constitutes an inner tree node, while a file or an empty folder are the leaves.
### Working directory 
A working directory is a folder used by KNIME nodes to disambiguate relative paths. Every file system will have a working directory whether it is explicitly configured or implicitly.
### Hidden files 
Within KNIME Analytics Platform hidden files and folders are not displayed when browsing a file system. However they can be referenced knowing the path. Hidden files currently only exist for the local file systems in KNIME Analytics Platform:
  * On Linux and macOS their filename starts with a dot `"."`
  * On Windows instead they are treated as regular files and folders


### Path syntax 
A path is a string that identifies a file or folder position within a file system. The path syntax depends on the file system, e.g. a Windows local file system might look like `C:\Users\username\file.txt`, while on Linux and most other file systems in KNIME Analytics Platform might look like `/folder1/folder2/file.txt`.
Paths can be distinguished in:
  * Absolute: An absolute path uniquely identifies a file or folder. It starts always with a file system root.
  * Relative: A relative path does not identify one particular file or folder. It is used to identify a file or folder relative to an absolute path.


## KNIME Analytics Platform and file systems 
Different file systems are available to use with KNIME Analytics Platform. Reader and writer nodes are able to work with all supported file systems.
File systems within KNIME Analytics Platform can be divided into two main categories:
  * Standard file systems
  * Connected file systems


### Standard file systems 
Standard file systems are available at any time meaning they do not need a connector node to connect to it.
Their working directory is pre-configured and does not need to be explicitly specified.
To use a reader node to read a file from a standard file system drag and drop the reader node for the file type you want to read, e.g. CSV Reader for a `.csv` file, into the workflow editor from the node repository.
Right click the node and choose _Configure…​_ from the context menu. In the _Input location_ pane under the tab _Settings_ you can choose the file system you want to read from in a drop-down menu.
![03 csv reader config standard fs](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_csv_reader_config_standard_fs.png)
The following standard file systems are available in KNIME Analytics Platform:
  * Local file system
  * Mountpoint
  * Relative to
    * Current workflow
    * Current mountpoint
    * Current workflow data area
  * Custom/KNIME URL


#### Local file system 
When reading from _Local File System_ the path syntax to use will be dependent on the system on which the workflow is executing, i.e. if Windows or UNIX operative system.
The working directory will be implicit and will correspond to the system root directory.
![03 csv reader config local fs](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_csv_reader_config_local_fs.svg)
|  You can also have access to network shares supported by your operating system, via the local file system within KNIME Analytics Platform.   
---|---  
Please be aware that when executing a workflow on KNIME Server version 4.11 or higher the local file system access is disabled for security reasons. The KNIME Server administrators can activate it, but this is not recommended. For more information about this please refer to the KNIME Server Administration Guide.
#### Mountpoint 
With the _Mountpoint_ option you will have access to KNIME mount points, such as LOCAL, your KNIME Server mount points, if any, and the KNIME Hub. You have to be logged in to the specific mountpoint to have access to it.
The path syntax will be UNIX-like, i.e. `/folder1/folder2/file.txt` and relative to the implicit working directory, which corresponds to the root of the mountpoint.
![03 csv reader config mountpoint fs](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_csv_reader_config_mountpoint_fs.svg)
Please note that workflows inside the mountpoints are treated as files, so it is not possible to read or write files inside a workflow.
#### Relative to 
With the _Relative to_ option you will have access to three different file systems:
  * _Current mountpoint_ and _Current workflow_ : The file system corresponds to the mountpoint where the currently executing workflow is located. The working directory is implicit and it is:
    * _Current mountpoint_ : The working directory corresponds to the root of the mountpoint
    * _Current workflow_ : The working directory corresponds to the path of the workflow in the mountpoint, e.g. `/workflow_group/my_workflow`.
  * _Current workflow data area_ : This file system is dedicated to and accessible by the currently executing workflow. Data are physically stored inside the workflow and are copied, moved or deleted together with the workflow.


All the paths used with the option _Relative to_ are of the type `folder/file` and they must be relative paths.
![03 csv reader config relativetodata fs](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_csv_reader_config_relativetodata_fs.svg)
In the example above you will read a `.csv` file from a folder `data` which is in:
```
<knime-workspace>/workflow_group/my_workflow/data/
```

Please note that workflows are treated as files, so it is not possible to read or write files inside a workflow.
When the workflow is executing on KNIME Server the options _Relative to_ → _Current mountpoint_ or _Current workflow_ will access the workflow repository on the server. The option _Relative to_ → _Current workflow data area_ , instead, will access the data area of the **job copy** of the workflow. Please be aware that files written to the data area will be lost if the job is deleted.
#### Custom/KNIME URL 
This option works with a pseudo-file system that allows to access single files via URL. It supports the following URLs:
  * `knime://`
  * `http(s)://` if authentication is not needed
  * `ssh://` if authentication is not needed
  * `ftp://` if authentication is not needed


For this option, you can also set up manually a timeout parameter (in milliseconds) for reading and writing.
The URL syntax should be as follows:
  * `scheme:[//authority]path[?query][#fragment]​`
  * The URL must be encoded, e.g. spaces and some special characters that are reserved, as `?`. To encode the URL you can use any available online URL encoder tool.


Using this option you can read and write single files, but you would not be able to move and copy files or folders. However, listing files in a folder, i.e. browsing, is not supported.
#### Standard file systems connector nodes 
With KNIME Analytics Platform version 4.5 new connectors nodes for standard file systems have been introduced. These nodes allow you to prototype workflows using a specific file system, by providing access to the standard file systems. The resulting output port allows downstream nodes to access files, e.g. to read or write, or to perform other file system operations, e.g. browse/list files, copy, move.
|  Please note that in many cases it is not necessary to use these connector nodes to access the standard file system. Nodes that require file system access (e.g. the File Reader node) typically provide standard file system access. The purpose of these connector nodes is that the working directory can be chosen, which makes file access with relative paths more convenient.   
---|---  
**Local File System Connector node**
This node provides access to the file system of the local machine.
In the Local File System node configuration dialog you can choose to use a custom working directory. If this option is not set, the default working directory is the home directory of the current operating system.
**Mountpoint Connector node**
This node provides a file system connection with access to a mountpoint, for example "LOCAL", or "My-KNIME-Hub". It can also provide a file system connection with access to the mountpoint which contains the current workflow, similar to the option _Relative to > Current mountpoint_.
In the Mountpoint Connector node configuration dialog you can specify the mountpoint to access.
  * _Current mountpoint_ : If chosen, the file system connection will give access to the mountpoint which contains the current workflow. If you are not using this connector node, this option is equivalent to choosing _Read from > Relative to > Current mountpoint_. Selecting the option _Set working directory to the current workflow_ , will additionally set the working directory of the file system connection to the location of the current workflow. This is then equivalent to choosing _Read from > Relative to > Current workflow_.
  * _Other mountpoint_ : If chosen, the file system connection will give access to the selected mountpoint. Unconnected mountpoints are greyed out and can still be selected, but you need to go to the KNIME Explorer and connect to the mountpoint before executing this node. A mountpoint is displayed in red if it was previously selected but is no longer available. You will not be able to save the dialog in this case.


Finally, you can specify a _Working directory_ using a Path syntax.
**Workflow Data Area Connector**
This node provides a file system connection with access to the data area of the current workflow.
If you are not using this connector node, this option is equivalent to choosing _Read from > Relative to > Current workflow data area_.
In the Workflow Data Area Connector node configuration dialog you can specify a _Working directory_ using a Path syntax.
### Connected file systems 
Connected file systems instead require a connector node to connect to the specific file system. In the connector nodes configuration dialog it is possible to configure the most convenient working directory.
The file system Connector nodes that are available in KNIME Analytics Platform can be divided into two main categories:
Connector nodes that need an Authenticator node:
  * Amazon S3 Connector node with Amazon Authenticator node
  * Google Cloud Storage Connector node with Google Authenticator node
  * Google Drive Connector node with Google Authenticator node
  * SharePoint Online Connector node with Microsoft Authenticator node
  * Azure Blob Storage Connector node with Microsoft Authenticator node
  * Azure Data Lake Storage Gen2 node with Microsoft Authenticator node


Connector nodes that do not need an Authenticator node:
  * Databricks File System Connector node
  * HDFS Connector node
  * HDFS Connector (KNOX) node
  * Create Local Bigdata Environment node
  * SSH Connector node
  * HTTP(S) Connector node
  * FTP Connector node
  * Space Connector node
  * KNIME Server Connector node


#### File systems with external Authenticator 
The path syntax varies according to the connected file system, but in most cases it will be UNIX-like. Information on this are indicated in the respective Connector node descriptions.
Typically in the configuration dialog of the Connector node you will be able to:
  * Set up the working directory: In the _Settings_ tab type the path of the working directory or browse through the file system to set up one.
  * Set up the timeouts: In the _Advanced_ tab set up the connection timeout (in seconds) and the Read timeout (in seconds).


![03 general connector config](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_general_connector_config.svg)
Most connectors will require a network connection to the respective remote service. The connection is then opened when the Connector node executes and closed when the Connector node is reset or the workflow is closed.
It is important to note that the connections are not automatically re-established when loading an already executed workflow. To connect to the remote service you will then need to execute again the Connector node.
![03 credentials notavail](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_credentials_notavail.png)
##### Amazon file system 
To connect to Amazon S3 file system you will need to use:
  * Amazon Authenticator node
  * Amazon S3 Connector node


![03 amazon](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_amazon.png)
Amazon S3 file system normalizes paths. Amazon S3 allows paths such as `/mybucket/.././file`, where `".."` and `"."` must not be removed during path normalization because they are part of the name of the Amazon S3 object. When such a case is present you will need to uncheck _Normalize paths_ option from the Amazon S3 Connector node configuration dialog.
Please be aware that each bucket in Amazon S3 belongs to an AWS region, e.g. eu-west-1. To access the bucket the client needs to be connected to the same region. You can select the region to connect to in the Amazon Authenticator node configuration dialog.
![03 amazon auth config](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_amazon_auth_config.png)
##### Google file systems 
We support two file systems related to Google. However, even though they both belong to Google services the corresponding Connector nodes use a different authentication type, and hence Authenticator node.
To connect to Google Cloud Storage you will need to use:
  * Google Cloud Storage Connector node
  * Google Authenticator node


![03 google cloud](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_google_cloud.png)
Also the Google Cloud Storage Connector node, as the Amazon S3 Connector node, normalizes the paths.
To use the Google Authenticator node​ you will need to create a project at console.developers.google.com.
|  The specific Google API you want to use has to be enabled under `APIs`.   
---|---  
After you create your Service Account you will receive a p12 key file to which you will need to point to in the Google Authenticator node configuration dialog.
To connect to Google Drive, instead, you will need to use:
  * Google Drive Connector node
  * Google Authenticator node


![03 google drive](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_google_drive.png)
The root folder of the Google Drive file system, contains your `Shared drivers`, in case any is available, and the folder `My Drive`. The path of your `Shared drivers` will then be `/shared_driver1/`, while the path of your folder `My Drive` will be `/My Drive/`.
##### Microsoft file systems 
We support three file systems related to Microsoft.
To connect to SharePoint Online, Azure Blob Storage, or to Azure Data Lake Storage Gen2, you will need to use:
  * SharePoint Online Connector node, or Azure Blob Storage Connector node, or Azure ADLS Gen2 Connector node
  * Microsoft Authenticator node


The SharePoint Online Connector node connects to a SharePoint Online site​. Here document libraries are represented as top-level folders​.
![03 sharepoint](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_sharepoint.png)
In the node configuration dialog you can choose to connect to the following sites:
  * _Root Site_ : Root site of the your organization​
  * _Web URL_ : https URL of the SharePoint site (same as in the browser)​
  * _Group site_ : Group site of a Office365 user group​
  * _Subsite_ : Connects to subsite or sub-sub-site of the above​


The Azure Blob Storage Connector node connects to an Azure Blob Storage file system.
The path syntax will be UNIX-like, i.e. `/mycontainer/myfolder/myfile` and relative to the root of the storage. Also Azure Blob Storage Connector node performs paths normalization.
![03 azure blob](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_azure_blob.png)
The Azure ADLS Gen2 Connector node connects to an Azure Blob Storage file system.
The path syntax will be UNIX-like, i.e. `/mycontainer/myfolder/myfile` and relative to the root of the storage.
![03 azure adls](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_azure_adls.png)
The Microsoft Authenticator​ node offers OAuth authentication for Azure and Office365 clouds.
It supports the following authentication modes:
  * _Interactive authentication_ : Performs an interactive, web browser based login by clicking on _Login_ in the node dialog. In the browser window that pops up, you may be asked to consent to the requested level of access. The login results in a token being stored in a configurable location. The token will be valid for a certain amount of time that is defined by your Microsoft Entra ID settings.
  * _Username/password authentication_ : Performs a non-interactive login to obtain a fresh token every time the node executes. Since this login is non-interactive and you get a fresh token every time, this mode is well-suited for workflows on KNIME Server. However, it also has some limitations. First, you cannot consent to the requested level of access, hence consent must begiven beforehand, e.g. during a previous interactive login, or by an Microsoft Entra ID directory admin. Second, accounts that require multi-factor authentication (MFA) will not work.
  * _Shared key authentication (Azure Storage only)_ : Specific to Azure Blob Storage and Azure Data Lake Storage Gen2. Performs authentication using an Azure storage account and its secret key.
  * _Shared access signature (SAS) authentication (Azure Storage only)_ : Specific to Azure Blob Storage and Azure Data Lake Storage Gen2. Performs authentication using shared access signature (SAS). For more details on shared access signatures see the Azure storage documentation. ​


![03 microsoft auth config](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_microsoft_auth_config.png)
#### File systems without external Authenticator 
All the Connector nodes that do not need an external Authenticator node will connect upon execution to a specific file system. This allows the downstream nodes to access the files of the remote server or file system.
##### Space Connector node 
The Space Connector node connects to a KNIME Hub instance. It provides a file system connection with access to a KNIME Hub space.
You can either use it without a preceeding KNIME Hub Authenticator node or you can use the Authenticator node.
If you are not using a KNIME Hub Authenticator node:
  * If your workflow is located in a local space you need to first connect to the KNIME Hub from the Analytics Platform.
  * If your workflow is located in a space on a KNIME Hub instance it is also possible to always connect to the space that contains the current workflow.


Alternatively, you can use a KNIME Hub Authenticator node, by adding the node and connecting it to a KNIME Hub instance. Then you can connect the Authenticator output port to the Space Connector node. To do so you can either add a connection port by clicking the + icon that appears when hovering over the Space Connector node or by dragging and dropping the output port of the Authenticator node into the Space Connector node. The port will be added and the two nodes connected. To authenticate against a KNIME Hub instance by using the KNIME Hub Authenticator node you will need to provide the URL and an application id/password. To create an application password please follow the instructions on the KNIME Hub User Guide.
In any case, the resulting output port allows downstream nodes to access files, e.g. to read or write, or to perform other file system operations (browse/list files, copy, move, …​) in the selected Space. Files outside the configured space cannot be accessed.
##### KNIME Server Connector node 
With the KNIME Server Connector node you are able to connect to a KNIME Server instance.
When opening the KNIME Server Connector node configuration dialog you will be able to either type in the URL of the KNIME Server you want to connect to, or to _Select…​_ it from those available among your mountpoints in KNIME Explorer. You do not need to have the KNIME Server mountpoint set up or already connected to use this node.
![03 server connector config](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/03_server_connector_config.png)
You can authenticate either by typing in your username and password or by using the selected credentials provided by flow variable, if any is available. Please be aware that when authenticating typing in your username and password the password will be persistently stored in an encrypted form in the settings of the node and will be then saved with the workflow.
##### SMB Connector node 
With SMB Connector node you can connect to a remote SMB server (e.g. Samba, or Windows Server). The resulting output port allows downstream nodes to access the files in the connected file system.
This node generally supports versions 2 and 3 of the SMB protocol. It also supports connecting to a Windows DFS namespace.
When opening the SMB Connector node configuration dialog you can choose to connect to a _File server_ host or a Windows _Domain_. Choosing _File server_ specifies that a direct connection shall be made to access a file share on a specific file server. A file server is any machine that runs an SMB service, such as those provided by Windows Server and Samba.
Choosing _Domain_ specifies that a connection shall be made to access a file share in a Windows Active Directory domain.
## Read and write from or to a connected file system 
When you successfully connect to a connected file system you will be able to connect the output port of the Connector node to any node that is developed under the File Handling framework.
To do this you will need to activate the corresponding dynamic port of your node.
|  You can also enable dynamic ports to connect to a connected file system in Utility nodes.   
---|---  
To add a port to one of the nodes where this option is available, drag and drop the output port of the Connector node onto the workflow canvas and select the node you want to add from the panel that opens. This will automatically add the node with the port and connect it.
![04 add dyn port](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/04_add_dyn_port.png)
### Reader nodes 
A number of Reader nodes in KNIME Analytics Platform are updated to work within the File Handling framework.
|  See the How to distinguish between the old and new file handling nodes section to learn how to identify the reader nodes that are compatible with the new File Handling framework.   
---|---  
You can use these Reader nodes with both Standard file systems and Connected file systems. Moreover, using the File System Connection port you can easily switch the connection between different connected file systems.
In the following example a CSV Reader node with a File System Connection port is connected to a SharePoint Online Connector node and it is able to read a `.csv` file from a SharePoint connected file system. Exchange the connection with any other Connector nodes, i.e. Google Drive, to read a `.csv` file from the other file system.
![04 multi fs reader example](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/04_multi_fs_reader_example.png)
#### Transformation tab 
The new table based reader nodes e.g. Excel Reader or CSV Reader allow you to transform the data while reading it via the _Transformation_ tab. To do so the node analyzes the structure of the file(s) to read in the node dialog and stores the file structure and the transformation that should be applied. When the node is executed this information is used to read the data and apply the transformations before creating the KNIME data table. This allows the nodes to return a table specification during configuration of the node and not only once the node is executed, assuming that the file structure does not change. The benefits of this is that you can configure downstream nodes without executing the reader node first and improves execution speed of the node.
|  To speedup the file analysis in the node dialog by default the reader node only reads a limited amount of rows which might lead to mismatching type exceptions during execution. To increase the limit or disable it completely open the _Advanced Settings_ tab and go to the _Table specification_ section.   
---|---  
![04 transformation tab](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/04_transformation_tab.png)
To change the transformation in the updated reader nodes configuration dialog, go to the _Transformation_ tab after selecting the desired file. This tab displays every column as a row in a table that allows modifying the structure of the output table. It supports reordering, filtering and renaming columns. It is also possible to change the type of the columns. Reordering is done via drag and drop. Just drag a column to the position it should have in the output table. Note that the positions of columns are reset in the dialog if a new file or folder is selected.
If you are reading multiple files from a folder, i.e. with the option _Files in folder_ in the _Settings_ tab, you can also choose to take the resulting columns from _Union_ or _Intersection_ of the files you are reading in.
The _Transformation_ tab provides two options which are important when dealing with changing file structures if you control the reader via flow variables.
![04 enforce type unknown column](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/04_enforce_type_unknown_column.png)
The _Enforce types_ option controls how columns whose type changes are dealt with. If selected, the node attempts to map to the KNIME type you configured in the transformation tab and fails if that is not possible. If unselected, the KNIME type corresponding to the new type is used whether it is compatible to the configured type or not. This might cause downstream nodes to fail if the type of a configured column has changed.
The `<any unknown new column>` option is a place holder for all previously unknown columns and specifies whether and where these columns should be added during execution.
#### Controlling readers via flow variables 
As described in the _Transformation_ tab section, the reader assumes that the structure of the file will not change after closing the node dialog. However when a reader is controlled via a flow variable the resulting table specification can change. This happens if settings that affect which data should be read are changed via a flow variable. One example is the reading of several CSV files with different columns in a loop where the path to the file is controlled by a flow variable.
![07 read files in loop](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/07_read_files_in_loop.png)
During configuration the node checks if the settings have changed. If that is the case the node will not return any table specification during configuration and will analyze the new file structure with each execution. In doing so it will apply the transformations to the new discovered file structure using the `<any unknown new column>` and _Enforce types_ option.
|  Currently the new reader nodes only detect changes of the file path. Other settings via flow variables that might influence the structure such as a different sheet name in the _Excel Reader_ or a different column separator in the _CSV Reader_ are not monitored with the current version. We are aware of this limitation and plan to change this with KNIME Analytics Platform 4.4. Until then you need to select the _Use new schema_ option which is described in the next section if you encounter this problem.   
---|---  
#### Reading the same data files with changing structure 
As described in the _Transformation_ tab section, the reader assumes that the structure of the file will not change after closing the node dialog. However the structure of the file can change (e.g. by adding additional columns) if it gets overwritten. In this case you need to select the _Use new schema_ option from the _When schema in file has changed_ setting on the _Advanced_ tab. Enabling this option forces the reader to compute the table specification during each execution thus adapting to the changes of the file structure.
|  The _Use new schema_ option will disable the _Transformation_ tab and the node will not return any table specification during configuration.   
---|---  
|  The option _Ignore (deprecated)_ is enabled only for the nodes created with KNIME Analytics Platform versions < 5.2.2, if the _Support changing schemas_ option was not selected. In this case the node did ignore a changed schema which might have resulted in wrong data.   
---|---  
#### Append a path column 
In the _Advanced Settings_ tab you can also check the option _Append path column_ under _Path column_ pane. If this option is checked, the node will append a path column with the provided name to the output table. This column will contain for each row which file it was read from. The node will fail if adding the column with the provided name causes a name collision with any of the columns in the read table. This will allow you to then distinguish the file from which a specific row is read from in case you are reading multiple files and are concatenating them into a single data table.
### Writer nodes 
Also Writer nodes can be used in KNIME Analytics Platform to work within the File Handling Framework.
You can use these Writer nodes with both Standard file systems and Connected file systems. Moreover, using the File System Connection port you can easily switch the connection between different connected file systems.
An output File System Connection port can be added to Writer nodes and this will allow them to be easily connected to different file systems and will be able to write persistently files to them.
In the following example a CSV Writer node with a File System Connection port is connected to a Google Drive Connector node and it is able to write a `.csv` file to a Google Drive connected file system. A CSV Reader node reads a `.csv` file from a SharePoint Online File System, data is transformed, and the resulting data is written to the Google Drive file system.
![04 multi fs writer example](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/04_multi_fs_writer_example.png)
#### Image Writer (Table) node 
The Image Writer (Table) node is also available with a table based input.
You can give to this node an input data table where a column contains, in each data cell, images. Image Writer (Table) is able to process the images contained in a column of the input data table and write them as files in a specified output location.
## Path data cell and flow variable 
Files and folders can be uniquely identified via their path within a file system. Within KNIME Analytics Platform such a path is represented via a path type. A path type consists of three parts:
  1. **Type:** Specifies the file system type e.g. local, relative, mountpoint, custome_url or connected.
  2. **Specifier:** Optional string that contains additional file system specific information such as the location the relative to file system is working with such as workflow or mountpoint.
  3. **Path:** Specifies the location within the file system with the file system specific notation e.g. `C:\file.csv` on Windows operating systems or `/user/home/file.csv` on Linux operating systems.


Path examples are:
  * Local
    * `(LOCAL, , C:\Users\username\Desktop)`
    * `(LOCAL, , \\fileserver\file1.csv)`
    * `(LOCAL, , /home/user)`
  * RELATIVE
    * `(RELATIVE, knime.workflow, file1.csv)`
    * `(RELATIVE, knime.mountpoint, file1.csv)`
  * MOUNTPOINT
    * `(MOUNTPOINT, MOUNTPOINT_NAME, /path/to/file1.csv)`
  * CUSTOM_URL
    * `(CUSTOM_URL, , https://server:443/my%20example?query=value#frag)`
    * `(CUSTOM_URL, , knime://knime.workflow/file%201.csv)`
  * CONNECTED
    * `(CONNECTED, amazon-s3:eu-west-1, /mybucket/file1.csv)`
    * `(CONNECTED, microsoft-sharepoint, /myfolder/file1.csv)`
    * `(CONNECTED, ftp:server:port, /home/user/file1.csv)`
    * `(CONNECTED, ssh:server:port, /home/user/mybucket/file1.csv)`
    * `(CONNECTED, http:server:port, /file.asp?key=value)`


A path type can be packaged into either a Path Data Cell or a Path Flow Variable. By default the Path Data Cell within a KNIME data table only displays the path part. If you want to display the full path you can change the cell renderer via the context menu of the table header to the _Extended path_ renderer.
### Creating path data cells 
In order to work with files and folders in KNIME Analytics Platform you can either select them manually via the node configuration dialog or you might want to list the paths of specific files and/or folder. To do this you can use the _List Files/Folders_ node. Simply open the dialog and point it to the folder whose content you want to list. The node provides the following options:
  * _Files in folder_ : Will return a list of all files within the selected folder that match the _Filter options_.
  * _Folders_ : Will return all folders that have the selected folder as parent. To include all sub folders you have to select the _Include subfolders_ option.
  * _Files and folders_ : Is a combination of the previous two options and will return all files and folders within the selected folder.


![05 list files folders config](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/05_list_files_folders_config.png)
### Manipulating path data cells 
Since KNIME Analytics Platform version 4.4 you can also use the Column Expressions node which supports now the Path type data cells. This node provides the possibility to append an arbitrary number of columns or modify existing columns using expressions.
### Creating path flow variables 
There are two ways to create a path flow variable. The first way is to export it via the dialog of the node where you specify the path. This could be a _CSV Writer_ node for example where you want to export the path to the written file in order to consume it in a subsequent node. The second way is to convert a path data cell into a flow variable by using one of the available variable nodes such as the _Table Row to Variable_ or _Table Row to Variable Loop Start_ node.
### String and path type conversion 
Until now not all nodes that work with files have been converted to the new file handling framework and thus do not support the path type. These nodes require either a String or URI data cell or a string flow variable.
#### From path to string 
The _Path to String_ node converts a path data cell to a string data cell. By default the node will create a String representation of the path that can be used in a subsequent node that still requires the old string or URI type, e.g. _JSON Reader_.
|  You can download this example workflow from KNIME Hub.   
---|---  
![05 path to string wf](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/05_path_to_string_wf.png)
If you only want to extract the plain path you can disable the __Create KNIME URL for 'Relative to' and 'Mountpoint' file system_ option in the Path to String node configuration dialog.
![05 path to string config](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/05_path_to_string_config.png)
Similar to the _Path to String_ node, the _Path to String (Variable)_ node converts the selected path flow variables to a string variable.
If you want to use a node that requires a URI cell you can use the _String to URI_ node after the _Path to String_ node.
#### From string to path 
In order to convert a string path to the path type you can use the _String to Path_ node. The node has a dynamic File System port that you need to connect to the corresponding file system if you want to create a path for a connected file system such as Amazon S3.
Similar to the _String to Path_ node the _String to Path (Variable)_ node converts a string flow variable into a path flow variable.
## File Folder Utility nodes 
With the introduction of the new file handling framework with KNIME Analytics Platform release 4.3, we also updated the functionality of the utility nodes.
![06 file folder utility repo](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_file_folder_utility_repo.png)
You can add a dynamic port to connect to a connected file system directly with the File Folder Utility nodes. In this way you can easily work with files and folders in any file system that is available.
In the example below the Transfer Files node is used to connect to two file systems a source file system, Google Drive in this example, and a destination file system, SharePoint Online in this example, to easily transfer files from Google Drive to SharePoint Online.
![06 transfer files example](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_transfer_files_example.png)
### Table based input Utility nodes 
Some of the File Folder Utility nodes are also available with a table based input. Those are the ones that have _(Table)_ in their name, e.g. Compress Files/Folder (Table), Delete Files/Folders (Table), and Transfer Files (Table) nodes. You can give to these nodes an input data table where a column contains, in each data cell, Paths to files and these are processed by the File Folder Utility (Table) nodes.
For example, with Compress Files/Folder (Table) and Delete Files/Folder (Table) nodes you can respectively compress or delete the files whose Paths are listed in a column of the input data table. With Transfer Files (Table) you can transfer all the files whose Paths are listed in a column of the input data table.
|  Since the new utility nodes (e.g. _Compress Files/Folder_ , _Delete Files/Folders_ and _Transfer Files_) do not provide the full functionality of the old nodes yet, as of now the nodes, including the required connector nodes for the different file systems, are still available but marked as _(legacy)_. You can find them in the node repository under the _File Handling (legacy)_ category. They will be deprecated once the new nodes have the full functionality. If you are missing some functionality in the new nodes please let us know via the KNIME Forum.   
---|---  
## Compatibility and migration 
With the 4.3 release of the KNIME Analytics Platform we introduced the new file handling framework. The framework requires a rewrite of the existing file reader and writer nodes as well as the utility nodes. However not all nodes provided by KNIME or the Community have been migrated yet which is why we provide this section to help you to work with old and new file handling nodes side by side.
### How to work with workflows that contain both old and new file handling nodes 
#### How to distinguish between the old and new file handling nodes 
You can identify the new file handling nodes by the three dots for the dynamic port on the node icon that allow you to connect to a connected file system.
![07 dynamic ports](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/07_dynamic_ports.png)
Another way to identify a new file handling node is by the _Input location_ section in the node configuration dialog. Here, for nodes that have been migrated to the new file handling framework you will typically find a drop-down menu that allows you to specify the file system that should be used. The following image shows the _Input location_ section of the configuration dialog of a typical reader node in the new and old file handling framework.
![07 new old node config](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/07_new-old_node-config.svg)
#### Working with old and new flow variables and data types 
Using flow variables you can specify the location of a file to read or write automatically. Because not all nodes provided by KNIME or the Community have been migrated yet you might face the problem that the newer file nodes support the new Path type whereas older nodes still support a string flow variable which can be either a file path or a URI.
The List Files/Folders node returns a table with a list of files/folders in the new Path type. To use the Path type in a node that has not been migrated yet (e.g. in the Table Reader), you need to first convert the new Path type using the Path to String node into a string.
![07 path table reader](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/07_path_table_reader.svg)
Some older nodes require a URI as input. To convert the Path type to an URI you first need to convert it to a string using the Path to String node and then convert the string to an URI using the String to URI node.
If you already have a Path type flow variable you can use the Path to String (Variable) node to convert the Path type flow variable into a string flow variable.
If you want to convert an existing file path or string to the new Path type you can either us the String to Path node to convert data columns or the String to Path (Variable) node to convert string variable.
### How to migrate your workflows from old to new file handling nodes 
This section should help you to migrate your workflows from the old file handling framework to the new file handling framework. The new framework provides lots of advantages over the old framework which come at the cost of some changes in the usage which we will address here.
In order to standardize the node names and better capture the functionality of the actual node we have renamed some of the nodes. In addition we have also removed some nodes whose functionality has been integrated into another node such as the _Excel Writer_ which replaces the original _Excel Writer (XLS)_ as well as the _Excel Sheet Appender_ node.
Table 1. Old (<4.2) and new (4.3+) file handling framework nodes conversion table **Node Icon** | **4.2 Old** |  | **4.3 New**  
---|---|---|---  
![06 compress files folders](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_compress_files_folders.png) | Zip Files | → | 
  * Compress Files/Folder
  * Compress Files/Folder (Table)

  
![06 decompress files](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_decompress_files.png) | Unzip Files | → | Decompress Files  
![06 create folder](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_create_folder.png) | Create Directory | → | Create Folder  
![06 delete files folders](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_delete_files_folders.png) | Delete Files | → | 
  * Delete Files/Folders
  * Delete Files/Folders (Table)

  
![06 transfer files](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_transfer_files.png) | 
  * Copy/Move Files
  * Download
  * Upload

| → | 
  * Transfer Files
  * Transfer Files (Table)

  
![06 list files folders](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_list_files_folders.png) | 
  * List Remote Files
  * List Files

| → | List Files/Folders  
![06 create temp folder](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_create_temp_folder.png) | Create Temp Dir | → | Create Temp Folder  
![06 connector](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_connector.png) | Connection (e.g. SSH Connection) | → | Connector (e.g. SSH Connector)  
![06 excel reader](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_excel_reader.png) | Excel Reader (XLS) | → | Excel Reader  
![06 read excel sheet names](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_read_excel_sheet_names.png) | Read Excel Sheet Names (XLS) | → | Read Excel Sheet Names  
![06 excel writer](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_excel_writer.png) | 
  * Excel Writer (XLS)
  * Excel Sheet Appender

| → | Excel Writer  
![06 file reader](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/06_file_reader.png) | File Reader | → | 
  * File Reader
  * File Reader (Complex Format)

  
#### Reading multiple files into a single KNIME data table 
Using the old file handling framework you had to use loops in order to read multiple files into a single KNIME data table. So your workflows looked like the following image.
![07 legacy read multiple files](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/07_legacy_read_multiple_files.svg)
With the new file handling framework you no longer need to use loops since the reading of multiple files into a single KNIME data table is now built into the reader nodes them self. All you have to do is to change the reading mode to _Files in Folder_. You can than open the Filter options dialog by clicking on the _Filter options_ button. Here you can filter the files that should be considered e.g. based on their file extension. In addition you can specify if files in subfolders should be included or not.
![07 files in folders](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/07_files_in_folders.png)
|  If you continue to use a loop to read multiple files and encounter problems during the execution have a look at the Controlling readers via flow variables section.   
---|---  
#### Reading from or writing to remote file systems 
With the old file handling framework, when working with remote file systems such as Amazon S3 or HDFS you had two options when reading files:
  1. Download the file to a local folder and then point the reader node to the local copy
  2. If available, use one of the File Picker nodes (e.g. Amazon S3 File Picker, Azure Blob Store File Picker or Google Cloud Storage File Picker) to create a signed URL to pass into the reader node.


In order to write a file to a remote file system you had to first write the file to your local hard drive and then upload it to the remote file system.
![07 legacy read write remote files](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/07_legacy_read_write_remote_files.svg)
With the new file handling framework you no longer need to use any additional nodes to work with files in remote file systems but simply connect the reader and writer nodes to the corresponding connector node via the dynamic ports.
![07 read write remote files](https://docs.knime.com/latest/analytics_platform_file_handling_guide/img/07_read_write_remote_files.png)
#### Working with KNIME URL 
When working with the nodes of the new file handling framework you no longer need to create KNIME URLs but can use the built-in convenience file systems Relative to and Mountpoint. The following table lists the relation between the KNIME URL and the two new file systems.
KNIME URL | Convenient file system  
---|---  
_knime://knime.node/_ | No direct replacement due to security reasons but check out Relative to → Current workflow data area  
_knime://knime.workflow/_ | Relative to → Current workflow*  
_knime://knime.mountpoint/_ | Relative to → Current mountpoint  
_knime:// <mountpoint_name>/_ | Mountpoint → <mountpoint_name>  
|  Due to security reasons KNIME workflows are no longer folders but considered as files. So you can no longer access data within a workflow directory except for the newly created workflow data area.   
---|---  
If you need the KNIME URL for a _Relative to_ or _Mountpoint_ path you can use the Path to String and Path to String (Variable) nodes with the _Create KNIME URL for 'Relative to' and 'Mountpoint' file systems_ option enabled, as it is by default.
|  We encourage you to use the new convenience file systems Relative to and Mountpoint in favor of the KNIME URLs. For more details on how to convert from the new file systems to the old KNIME URLs see the Working with old and new flow variables section.   
---|---  
#### Excel Appender 
The functionality of the _Excel Appender_ node has been integrated into the new Excel Writer node. The node allows you to create new Excel files with one or many spreadsheets but also to append any number of spreadsheets to an existing file. The number of sheets to write can be changed via the dynamic ports option of the node.
#### File Reader and File Reader (Complex Format) nodes 
With KNIME Analytics Platform release 4.4.0 we:
  * Improved the **File Reader node**
  * Introduced a new **File Reader (Complex Format) node**


The **File Reader node** now supports the new File Handling framework and can use path flow variables and connect to file systems. The File Reader node is able to read the most common text files.
The newly introduced **File Reader (Complex Format) node** also supports the new File Handling framework, and it is able to read complex format files. We recommend to use the File Reader (Complex Format) node only in case the File Reader node is not able to read your file.
With the File Reader node you can also select the _Use new schema_ option in the _Advanced Settings_ tab of its configuration dialog. This allows the node to support eventual input file structure changes between different invocations. This is instead not possible with the File Reader (Complex Format) node that does not support files changing schemas. Thus, if the File Reader (Complex Format) node is used in a loop, you should make sure that all files have the same format (e. g. separators, column headers, column types). The node saves the configuration only during the first execution. Alternatively, the File Reader node can be used.
  * Introduction
  * Basic concepts about file systems
    * Working directory
    * Hidden files
    * Path syntax
  * KNIME Analytics Platform and file systems
    * Standard file systems
    * Connected file systems
  * Read and write from or to a connected file system
    * Reader nodes
    * Writer nodes
  * Path data cell and flow variable
    * Creating path data cells
    * Manipulating path data cells
    * Creating path flow variables
    * String and path type conversion
  * File Folder Utility nodes
    * Table based input Utility nodes
  * Compatibility and migration
    * How to work with workflows that contain both old and new file handling nodes
    * How to migrate your workflows from old to new file handling nodes


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. KNIME Flow Control Guide 


KNIME Analytics Platform 5.4
# KNIME Flow Control Guide
IntroductionFlow variablesCreating flow variablesUsing flow variablesLoopsLoop commandsUsing flow variables in loopsBreakpoint nodeIF and CASE SwitchesIF Switch nodeDefining the active port via a flow variableCASE Switch Data (Start) nodeError handlingIntroduction
  * Introduction
  * Flow variables
    * Creating flow variables
    * Using flow variables
  * Loops
    * Loop commands
    * Using flow variables in loops
    * Breakpoint node
  * IF and CASE Switches
    * IF Switch node
    * Defining the active port via a flow variable
    * CASE Switch Data (Start) node
  * Error handling


Download PDF
## Introduction 
Not all workflows have a static input and only one branch. Often, data are updated regularly, and some settings can be different from time to time. In other cases a workflow might have branches and a rule that determines which branch to follow.
In this guide, the tools available in KNIME Analytics Platform to control the flow in the needed direction are introduced.
In particular this guide explains how to:
  * Parametrize settings using flow variables
  * Repeat a part of the workflow for different inputs
  * Define a rule to activate a branch
  * Provide an error handling branch if node execution fails


The nodes that come in hand here are shipped automatically with the KNIME Analytics Platform and do not require the installation of any extension. You can find these nodes in the node repositoy under the _Workflow Control_ category as shown in Figure 1.
![01 workflow control repository](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/01_workflow_control_repository.png)
Figure 1. The Workflow Control nodes in the node repository
## Flow variables 
Flow variables are parameters with string, integer, double, arrays, or Path values. These parameters can be used to avoid manually changing settings within the nodes of a workflow when a new execution with different settings is required. Flow variables are available only for the downstream nodes in the workflow.
### Creating flow variables 
To create flow variables you have the following possibilities:
  * Convert a table row into flow variables
  * Export a node configuration as flow variable
  * Use Configuration and Widget nodes
  * Combine or modify existing flow variables


The first two options are introduced in this section. The Widgets and Configuration nodes are explained in more details in the Components Guide. However, an example that makes use of this type of nodes in the context of creating a flow variable is available on KNIME Hub. An example of the last option is the Rule Engine Variable node, which is introduced in the IF and CASE Switches section.
#### Converting a table row into flow variables 
The Table Row to Variable node, converts each column of the first row of a data table into a flow variable. In the example in Figure 2, which is available on KNIME Hub, the Table Row to Variable node is connected to Group By and Sorter node which respectively group the original data by country, counting how many times an entry in the data corresponds to a specific country and sorts the data accordingly, as shown in Figure 3.
![02 table row to variable](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/02_table_row_to_variable.png)
Figure 2. Converting first row of a data table to flow variables
![02 sorted sales data](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/02_sorted_sales_data.png)
Figure 3. Data grouped and sorted according to sales by country
The output of the Table Row to Variable node is shown in Figure 4. The column names in the data table are now the names of the flow variables, while the values of the first row are the corresponding values of the flow variables.
![02 table row to variable output](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/02_table_row_to_variable_output.png)
Figure 4. First row converted to flow variables
|  In the video From Data to Variables this is explained in more detail.   
---|---  
#### Exporting a node configuration as flow variable 
Another option to create a new flow variable is to export a node configuration. In this case the flow variable acquires the same value used for the node configuration. The name for the flow variable is defined in the node configuration dialog.
Some configuration options have a flow variable icon next to them, for example the string pattern in the Row Filter node configuration dialog shown in Figure 5.
![02 row filter dialog](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/02_row_filter_dialog.png)
Figure 5. Flow variable button in a Row Filter node dialog
To export a node configuration that has a flow variable icon next to it, open the node configuration dialog and follow these steps:
  1. Define the node configuration value in the corresponding field
  2. Click the icon and select _Create Variable_ in the dialog that opens, shown in Figure 6
  3. Write the flow variable name in the field that activates.
![02 flow variables var settings](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/02_flow_variables_var_settings.png)
Figure 6. Variable Settings dialog


If there is no flow variable icon next to the setting to be exported, follow these steps:
  1. Define the node configuration value in the corresponding field
  2. Open the _Flow Variables_ tab in the node configuration dialog
  3. Write the flow variable name in the text field close to the drop-down menu in the row corresponding to the node configuration to export, as shown in Figure 7.
![02 flow variables tab dialog](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/02_flow_variables_tab_dialog.png)
Figure 7. Flow Variables tab in a node configuration dialog


Now, the _Flow Variables_ tab in the output table view of the node, shows the exported flow variable, as in Figure 8.
![02 flow variables tab table](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/02_flow_variables_tab_table.png)
Figure 8. Flow Variables tab in a node output
### Using flow variables 
#### Flow variable ports 
The flow variable ports are red circles above each node. Each node has flow variable ports, but for most nodes they are hidden by default. You can make them visible by right clicking a node and selecting _Show Flow Variable Ports_ in the context menu as shown in Figure 9.
![02 show flow variable ports](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/02_show_flow_variable_ports.png)
Figure 9. Showing flow variable ports
#### Overwriting node configurations with flow variables 
In the previous section, we explained how to create flow variables. Here, we show how to use a previously created flow variable to configure a node.
In case you want to use a flow variable you need to first connect the node where the flow variable is created to the following one. If they are not connected already via any other port, use the flow variable ports. You need to create the connection only once, then the transferred flow variables are available for all subsequent nodes in the workflow.
To use a flow variable follow these steps:
  1. Open the configuration dialog of the node whose setting you want to overwrite via the flow variable. The flow variable icon is not always present in the node configuration dialog, but you can find it next to node configurations that are often overwritten by flow variables.
    1. If the chosen configuration setting has a flow variable icon next to it, click it and in the _Variable Settings_ dialog that opens select _Use Variable_. Then select the flow variable from the drop-down menu, as shown in Figure 10.
![02 use flow variable dialog](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/02_use_flow_variable_dialog.png)
Figure 10. Define a specific node configuration by a flow variable
    2. If the flow variable icon is not present, go to the _Flow Variables_ tab, navigate to the chosen node configuration and select the flow variable from the drop-down menu as shown in Figure 11.
![02 use flow variables in tab](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/02_use_flow_variables_in_tab.png)
Figure 11. Overwriting node configurations in the flow variables tab


When you overwrite a node configuration with a flow variable, a warning message appears in the lower part of the node configuration dialog pointing out which node configuration is overwritten, as shown in Figure 12.
![02 flow variable dialog message](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/02_flow_variable_dialog_message.png)
Figure 12. Warning message in lower part of configuration dialog
## Loops 
Loops iterate over a certain part of the workflow. Each iteration, a repeated procedure, has different input. What changes for each iteration can be a parameter value, dataset, subgroup of the same dataset, single column, or single row as flow variables.
|  In the videos What is a Loop? and Counting Loop we explain the loop concept and build a simple example loop.  
---|---  
A loop in KNIME begins with a Loop Start node and ends with a Loop End node. The operations that are performed for each iteration are executed in the loop body. Generally, the Loop Start node is responsible for increasing the iteration counter and for sending the data to the loop body, which is then responsible for executing sub-workflow steps. After those are performed the Loop End node checks if the end condition is fulfilled, and if this is not the case the Loop Start node increases the counter and performs the loop body operations again. When the end condition is fulfilled, the Loop End node collects the data from the different iterations and the next step in the workflow is performed.
The loop in Figure 13, which is available on KNIME Hub, is an example where the Chunk Loop Start node is used to iterate over a table, which contains ten rows filled with the letter `A`, created with the Table Creator node.
![03 chunk loop](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_chunk_loop.svg)
Figure 13. An example of loop with Chunk Loop Start node
The Chunk Loop Start node takes three rows of the input table and sends a chunk of the data to the loop body, which is made of a Value Counter node. Finally, a Loop End node collects the results of each loop iteration and ends the loop when the condition of reaching the last row of the input data is fulfilled.
KNIME Analytics Platform provides different loop start and loop end nodes for different types of loops. The loop start and loop end nodes are collected into Table 1 and Table 2. You will find these nodes in the node repository by navigating to _Workflow Control_ → _Loop Support_.
Table 1. Loop start nodes **Node Icon** | **Loop Start Node** | **Explanation**  
---|---|---  
![03 counting loop start](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_counting_loop_start.png) | Counting Loop Start | Triggers loop for a predefined number of iterations  
![03 chunk loop start](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_chunk_loop_start.png) | Chunk Loop Start | Splits data into consecutive chunks for each iteration. Either the number of chunks or the number of rows per chunk is defined.  
![03 column list loop start](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_column_list_loop_start.png) | Column List Loop Start | Iterates over a list of columns  
![03 generic loop start](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_generic_loop_start.png) | Generic Loop Start | Together with the Variable Condition Loop End node it iterates until a certain condition is met  
![03 table row to variable loop start](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_table_row_to_variable_loop_start.png) | Table Row To Variable Loop Start | Converts every row in a table into row variables and iterates over them  
![03 group loop start](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_group_loop_start.png) | Group Loop Start | Iterates over groups of data that are defined based on a condition  
![03 interval loop start](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_interval_loop_start.png) | Interval Loop Start | Increases a variable value for each iteration within a given interval  
![03 recursive loop start](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_recursive_loop_start.png) | Recursive Loop Start | Iterates over the output data table from the Recursive Loop End node  
![03 recursive loop start 2 ports](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_recursive_loop_start_2_ports.png) | Recursive Loop Start (2 Ports) | Iterates over the two output data tables from the Recursive Loop End (2 ports) node  
Table 2. Loop end nodes **Node Icon** | **Loop End Node** | **Explanation**  
---|---|---  
![03 loop end](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_loop_end.png) | Loop End | Concatenates the output tables from the different iterations  
![03 variable condition loop end](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_variable_condition_loop_end.png) | Variable Condition Loop End | Together with the Generic Loop Start node it executes a loop until a certain condition is met  
![03 loop end 2 ports](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_loop_end_2_ports.png) | Loop End (2 ports) | Concatenates the output tables from each iteration into two separate tables when each iteration produces two output tables  
![03 loop end column append](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_loop_end_column_append.png) | Loop End (Column Append) | After each iteration the output table is joined with the output table from previous iteration  
![03 recursive loop end](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_recursive_loop_end.png) | Recursive Loop End | Passes the output table from an iteration to a Recursive Loop Start node until either the maximum number of iterations, minimum number of rows, or a certain condition is met  
![03 recursive loop end 2 ports](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_recursive_loop_end_2_ports.png) | Recursive Loop End (2 ports) | Passes the output table from an iteration to a Recursive Loop Start (2 ports) node until the maximum number of iterations, minimum number of rows, or a certain condition is met  
![03 variable loop end](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_variable_loop_end.png) | Variable Loop End | Collects flow variables from each iteration. Can be used when the calculations are finished inside the loop and the output is not needed.  
### Loop commands 
While executing a loop, you can follow the execution monitoring a selected node output using the node monitor.
You have two ways of executing a loop.
  * Complete execution: Right click the Loop End node and choose _Execute_ from the context menu. Now a yellow loop sign is shown at the Loop End node while the loop steps are executed. As soon as it turns to green the loop is completely and successfully executed. To reset the loop you can reset any of the nodes that belong to the loop sub-workflow, by right clicking a node and choosing _Reset_ from the context menu.
  * Step-wise execution: Right click the Loop End node and choose _Step Loop Execution_ from the context menu, to execute one iteration of the loop. At any time you can execute the remaining steps by choosing _Resume Loop Execution_ from the context menu of the Loop End node. You can also pause the step-wise execution or cancel it, choosing _Pause Execution_ or _Cancel_ from the context menu of the Loop End node. In both cases this affects only the Loop End node, while the antecedent nodes will still be in the executed state.


|  The videos Loop End Nodes and Loop Commands explain loop end nodes in more detail and show guided loop execution options.  
---|---  
### Using flow variables in loops 
In the Flow variables section we introduced flow variables and their function. They are often used in loops and Table Row to Variable Loop Start and Variable Loop End nodes are two specific loop nodes with input and output flow variable type ports.
An example for Table Row to Variable Loop Start is shown in Figure 14, and is also available on KNIME Hub.
![03 loop flow variable start](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_loop_flow_variable_start.svg)
Figure 14. An example of loop with Table Row to Variable Loop Start node
Similarly to the Table Row to Variable node, the Table Row to Variable Loop Start node transforms the row of a table to a set of variables that have as name the column name and as value the corresponding value in the current row. It loops over each row of the input table and exposes the flow variables obtained to the loop body. In the example in Figure 14, one of the flow variables obtained is used to overwrite the column value parameter setting of the Constant Value Column node, which appends a cell containing the current value to the current table.
### Breakpoint node 
In the node repository, under _Workflow Control_ → _Loop Support_ , Breakpoint node is also available. You can use it to halt execution when certain conditions are met. Figure 15 and Figure 16 show the use of this node to impair the execution of the loop when the input flow variable corresponding to the value of the customer segment is equal to three. You can also configure the Breakpoint node to halt execution when it receives as input an empty table, and active or inactive branch. You can also set a _Custom message_ to be shown if the Breakpoint node condition is met. This workflow is also available on KNIME Hub.
![03 breakpoint workflow](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_breakpoint_workflow.svg)
Figure 15. A workflow with a Breakpoint node
![03 breakpoint loop dialog](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/03_breakpoint_loop_dialog.png)
Figure 16. The Breakpoint node configuration dialog
## IF and CASE Switches 
In case you need to perform different operations on different groups of the data, you can use a logic which is able to split the workflow into branches. The IF and CASE nodes available in KNIME Analytics Platform have this function. The nodes for IF and CASE switches are located in the node repository under _Workflow Control_ → _Switches_.
![04 switch type nodes](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/04_switch_type_nodes.png)
Figure 17. Switch type nodes
### IF Switch node 
The IF Switch node creates two branches in the workflow which can be alternatively activated or disactivated. This means, the nodes in the active branch or branches are executed and the nodes in the inactive one are not. In the node configuration dialog you can define the active branch either manually, or it can be dynamically controlled by a condition, through a flow variable.
The example workflow in Figure 18, also available on KNIME Hub, loops over an IF Switch to read in data row by row, to categorize the customers based on their call activity. When the loop is finished, the rows are concatenated back into the same table.
![04 if switch](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/04_if_switch.svg)
Figure 18. An example using an IF Switch node
In the workflow in Figure 18, the IF Switch node has both data and flow variable input. The flow variable, created with the String Input node, defines the active branch. The data are then handled in the active branch.
### Defining the active port via a flow variable 
As shown in Figure 19, the _Select active port_ option is located in the _Options_ tab of the IF Switch node configuration dialog. Here, you can manually select either `both`, `bottom` or `top`, to define the active branch. However, in the same way as in the Overwriting settings with flow variables section, you can also use a flow variable to overwrite the active port option.
![04 if switch dialog](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/04_if_switch_dialog.png)
Figure 19. Defining active branch by a flow variable
First, you need to create a suitable flow variable. Since you need to use it to overwrite the _Select active port_ option of the IF Switch node, you have to assign to it a string value equal to either `both`, `bottom` or `top`.
In the example shown in Figure 18 this flow variable is created using a String Input node which allows to manually set the output flow variable to one of the suitable string values.
Another possibility is to automatically activate a branch according to some set condition, within a loop. In the example shown in Figure 20 and available on KNIME Hub, a loop is performed row by row over some data. Each row is then transformed in flow variables and, depending on the value of a specific column the IF Switch ports are activated.
![04 if switch loop](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/04_if_switch_loop.svg)
Figure 20. IF Switch combined with loop
The Rule Engine Variable node configuration dialog is shown in Figure 21. Here the antecedents are defined using the flow variables and functions available while the consequent of a true condition is assigned with the `=>` sign. The default outcome, i.e. the value assigned to all cases for which none of the rules is true, is defined using the syntax `TRUE => "default outcome"`.
![04 define flow variable rule engine](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/04_define_flow_variable_rule_engine.png)
Figure 21. Defining flow variable values by condition
### CASE Switch Data (Start) node 
With the CASE Switch Data (Start) node you can activate one of three branches in a workflow. Similar to the IF Switch node, this can be either done manually or by using a condition.
Once the execution has finished, the tables resulting from the branches can be concatenated by using an End IF node or CASE Switch Data (End) node.
An example, similar to the one of IF Switch Data node, is shown in Figure 22 and available on KNIME Hub.
![04 case switch](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/04_case_switch.svg)
Figure 22. An example using a Case Switch node
|  Note that when using the CASE Switch, the possible values for the active output port are `0` for the top, `1` for the middle, and `2` for the bottom output port. That said, the functionality is similar, but the flow variable values are different.  
---|---  
## Error handling 
Different type of errors, such as a failing connection to a remote service, the invocation of a not accessible database, and so on, can occur when executing a workflow. To handle them you can use the Try-Catch enclosures. The Try part executes some node(s). If the execution fails, the Catch branch with error handling is activated. Otherwise the Default branch is executed. At the end of the Try-Catch enclosure, the results from either the successful or failed execution are collected. An example of a Try-Catch enclosure is shown in Figure 23, which is available on KNIME Hub.
![05 try catch example](https://docs.knime.com/latest/analytics_platform_flow_control_guide/img/05_try_catch_example.svg)
Figure 23. Error handling with a Try-Catch enclosure
In the workflow in Figure 23, the Try (Data Ports) node begins the enclosure, followed by the Breakpoint node. The Breakpoint node is set to fail at the fifteenth iteration. Here, if execution of the Try part fails the Catch branch is executed.
The Active Branch Inverter node begins the Catch branch for error handling. This node makes an inactive branch active and active branch inactive:
  * It activates a branch in the event that execution fails
  * It deactivates a branch if execution is successful


The Catch Errors (Data Ports) node closes the Try-Catch enclosure:
  * If execution is successful, the output of the node is the output from the Default branch. Therefore, the Default branch must be connected to the top input port of the Catch Errors (Data Ports) node.
  * If execution fails, the output of the enclosure comes from the Catch branch, which must be connected to the bottom input port of the Catch Errors (Data Ports) node. The reasons for the failure are then reported in the flow variable output of the Catch Errors (Data Ports) node.


Besides data tables, the Try part can also be started with a flow variable, using the Try (Variable Ports) node instead.
One of the following four alternatives is available to end the Try-Catch loop:
  * Catch Errors (Data Ports) shown in Figure 23
  * Catch Errors (Var Ports) if the outputs of the Catch and Default branches are flow variables
  * Catch Errors (Generic Ports) for models
  * Catch Errors (DB Ports) for database queries.


You will find the nodes for a Try-Catch enclosure in the node repository by navigating to _Workflow Control_ → _Error Handling_.
  * Introduction
  * Flow variables
    * Creating flow variables
    * Using flow variables
  * Loops
    * Loop commands
    * Using flow variables in loops
    * Breakpoint node
  * IF and CASE Switches
    * IF Switch node
    * Defining the active port via a flow variable
    * CASE Switch Data (Start) node
  * Error handling


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. KNIME Analytics Platform Installation Guide 


KNIME Analytics Platform 5.4
# KNIME Analytics Platform Installation Guide
Installing KNIME Analytics PlatformConfiguration settings and knime.ini fileAllocating memory in knime.ini fileInstalling Extensions and IntegrationsUpdating KNIME Analytics Platform and ExtensionsUpdate SitesDefault Update SitesAdding External Update SitesAdding Local Update SitesWorking with the Nightly BuildsRelease notes and changelogs (KNIME Analytics Platform 5.4)KNIME Analytics Platform 5.4.3KNIME Analytics Platform 5.4.2KNIME Analytics Platform 5.4.1KNIME Analytics Platform 5.4.0Installing KNIME Analytics Platform
  * Installing KNIME Analytics Platform
    * Configuration settings and knime.ini file
    * Allocating memory in knime.ini file
  * Installing Extensions and Integrations
  * Updating KNIME Analytics Platform and Extensions
  * Update Sites
    * Default Update Sites
    * Adding External Update Sites
    * Adding Local Update Sites
  * Working with the Nightly Builds
  * Release notes and changelogs (KNIME Analytics Platform 5.4)
    * KNIME Analytics Platform 5.4.3
    * KNIME Analytics Platform 5.4.2
    * KNIME Analytics Platform 5.4.1
    * KNIME Analytics Platform 5.4.0


Download PDF
## Installing KNIME Analytics Platform 
  1. Go to the download page on the KNIME.com website to start installing KNIME Analytics Platform.
  2. The download page shows three tabs which can be opened individually:
     * _Register for Help and Updates_ : here you can optionally provide some personal information and sign up to our mailing list to receive the latest KNIME news
     * _Download KNIME_ : this is where you can download the software
     * _Getting Started_ : this tab gives you information and links about what you can do after you have installed KNIME Analytics Platform
  3. Now open the _Download KNIME_ tab and click the installation option that fits your operating system. KNIME Analytics Platform can be installed on Windows, Linux, or macOS.
Notes on the different options for Windows:
     * The Windows installer extracts the compressed installation folder, adds an icon to your desktop, and suggests suitable memory settings.
     * The self-extracting archive simply creates a folder containing the KNIME installation files. You don’t need any software to manage archiving.
     * The zip archive can be downloaded, saved, and extracted in your preferred location on a system to which you have full access rights.
![010 install files](https://docs.knime.com/latest/analytics_platform_installation_guide/img/010_install_files.png)
Figure 1. KNIME Analytics Platform available installers
  4. Read and accept the privacy policy and terms and conditions. Then click _Download_.
  5. Once downloaded, proceed with installing KNIME Analytics Platform:
     * _Windows:_ Run the downloaded installer or self-extracting archive. If you have chosen to download the zip archive instead, unpack it to a location of your choice. Run `knime.exe` to start KNIME Analytics Platform.
|  With KNIME Analytics Platform version 5.3 you have the possibility to start KNIME Analytics Platform in high-resolution mode. To do so, go to the installation folder and run `knime-hidpi.bat`. Please notice that the batch file to start the KNIME Analytics Platform in the high-resolution mode is an experimental feature designed exclusively for Modern UI. This feature aims to enhance visual fidelity but is not yet fully supported across the classic user interface and the Modern UI. So, users should be aware of potential limitations and compatibility issues when switching between Modern UI and Classic UI contexts.   
---|---  
     * _Linux:_ Extract the downloaded tarball to a location of your choice. Run the `knime` executable to start KNIME Analytics Platform.
     * _Mac:_ Double click the downloaded dmg file and wait for the verification to finish. Then move the KNIME icon to _Applications_. Double click the KNIME icon in the list of applications to launch KNIME Analytics Platform.


The following operating systems versions are supported:
  * Windows
    * Windows 10, 11
    * Windows Server - 2016, 2019, 2022,
  * Ubuntu 20.04 LTS and 22.04 LTS and derivatives
  * RHEL/CentOS/Rocky Linux 8, 9
  * macOS (12 and above - macOS x86_64 Intel) and M1 (macOS arm64 - Apple silicon) (only the last two major versions are supported)
    * macOS 12: Monterey
    * macOS 13: Ventura


|  Also check the KNIME Getting Started Guide and the KNIME Analytics Platform User Guide.   
---|---  
### Configuration settings and knime.ini file 
When installing KNIME Analytics Platform, configuration settings are set to their defaults, and they can later be changed in the _knime.ini_ file. The configuration settings, i.e. options used by the Java Virtual Machine when KNIME Analytics Platform is launched, range from memory settings to system properties required by some extensions.
You can find `knime.ini` in the installation folder of KNIME Analytics Platform.
|  **On macOS** : To locate `knime.ini` on macOS, open Finder and navigate to your installed Applications. Next, right click the KNIME application, select _Show Package Contents_ in the menu, and navigate to `Contents` → `Eclipse`.   
---|---  
The `knime.ini` file can be edited with any plaintext editor, such as Notepad (Windows), TextEdit (macOS) or gedit (Linux).
### Allocating memory in knime.ini file 
The entry `-Xmx1024m` in the `knime.ini` file specifies how much memory KNIME Analytics Platform is allowed to use. The setting for this value will depend on how much memory is available in your machine. KNIME recommends setting it to approximately one half of your available memory, but you can modify the value based on your needs. For example, if your computer has 16 GB of memory, you might set the entry to `-Xmx8192m`.
## Installing Extensions and Integrations 
If you want to add capabilities to KNIME Analytics Platform, you can install extensions and integrations. The available extensions range from free open source extensions and integrations provided by KNIME to free extensions contributed by the community and commercial extensions including novel technology nodes provided by our partners.
The KNIME extensions and integrations developed and maintained by KNIME contain deep learning algorithms provided by Keras, high performance machine learning provided by H2O, big data processing provided by Apache Spark, and scripting provided by Python and R, just to mention a few.
Install extensions from:
  * KNIME Hub:
    * Search for the Extension or Integration you want to install in the search bar
    * Click Extensions on the results page
    * Click the extension you want to install, and from the extension page and drag and drop the squared yellow icon, shown in Figure 2, to the KNIME Analytics Platform. A window will open asking if you want to search and install the extension or integration. Click _Yes_ and follow the instructions.
![02 hub extension page](https://docs.knime.com/latest/analytics_platform_installation_guide/img/02_hub_extension_page.png)
Figure 2. Install the KNIME Integrated Deployment Extension from KNIME Hub
    * Restart KNIME Analytics Platform.
  * KNIME Analytics Platform:
    * Go to the _Menu_ in the top right corner of the KNIME Analytics Platform.
    * Select _Install Extensions_. The dialog shown in Figure 3 opens.
![040 install extensions](https://docs.knime.com/latest/analytics_platform_installation_guide/img/040_install_extensions.PNG)
Figure 3. Installing Extensions and Integrations from KNIME Analytics Platform
    * Select the extensions you want to install
    * Click _Next_ and follow the instructions
    * Restart KNIME Analytics Platform.


The _Install Extensions_ menu provides the extensions that are available via the update sites you have enabled.
|  If you encounter issues during the installation of Python based extensions, these might be related to challenges specific to this type of extensions. Find here information about how to troubleshoot these issues.   
---|---  
To uninstall an extension, go to the _Help_ menu in the top right corner of the user interface and select the _About KNIME Analytics Platform_. In the window that opens click _Installation Details_. A dialog shown in Figure 4 opens. Now, select the extension that you want to uninstall, and click _Uninstall…​_.
![050 uninstall extensions](https://docs.knime.com/latest/analytics_platform_installation_guide/img/050_uninstall_extensions.png)
Figure 4. Uninstalling Extensions and Integrations
## Updating KNIME Analytics Platform and Extensions 
It is good to make sure that you always use the latest version of KNIME Analytics Platform and its extensions.
To do so:
  1. Go to the _Menu_ in the top right corner of the KNIME Analytics Platform.
  2. Select _Check for updates_. In the dialog that opens, select the available updates you want to install and then click _Next_.
  3. Proceed by following the instructions. KNIME Analytics Platform has to be restarted in order to apply the updates.


## Update Sites 
The Update Sites are where KNIME retrieves additional software in the form of extensions as well as updates. To see or edit the available update sites, click _Preferences_ in the top right corner of the user interface. This opens the _Preferences_ dialog. Select _Install/Update_ → _Available Software Sites_.
### Default Update Sites 
These four updates sites are provided by KNIME and are always available:
![030 update site](https://docs.knime.com/latest/analytics_platform_installation_guide/img/030_update_site.PNG) Figure 5. Available Update Sites |  |  **KNIME Analytics Platform 5.4 Update Site** : Provides all extensions and integrations maintained by KNIME: R, Python, H2O Machine Learning, Apache Spark for big data, and many more. Contains KNIME Labs Extensions, which are extensions that are not yet part of the set of stable KNIME extensions because their functionality may not yet be finalized.  
---  
**KNIME Community Extensions (Experimental)** : Provides additional extensions created by the KNIME community. **Note: this update site is not enabled by default.**  
**KNIME Community Extensions (Trusted)** : Provides trusted community extensions, i.e. extensions created by the KNIME community, which have been tested for backward compatibility and compliance with KNIME quality standards.  
**KNIME Partner Update Site 5.4** : Provides extensions created by KNIME partners.  
_KNIME Analytics Platform 5.4 Update Site_ , _KNIME Community Extensions (Trusted)_ , and _KNIME Partner Update Site 5.4_ are enabled by default.
### Adding External Update Sites 
To install extensions that are not part of the above update sites, click _Add_ to manually add the relevant update site, inserting the Name and Location as shown in Figure 6.
![030 add update site](https://docs.knime.com/latest/analytics_platform_installation_guide/img/030_add_update_site.png)
Figure 6. Add Update Sites
After adding a new update site you will see it listed in the _Available Software Sites_. You must now enable it by selecting it from the list.
### Adding Local Update Sites 
If your working environment has limited internet access or you receive an error message “Proxy Authentication Required” when connecting to a remote update site (provided by a URL), you can install extensions from a local zip file.
  1. Download KNIME update sites as zip files at the following links:
     * KNIME Analytics Platform Update Site
     * KNIME Community Extensions
     * KNIME Partner Update Site 5.4
  2. Save the zip file containing the extensions to your local system
  3. Click _Preferences_ in the top right corner of the user interface. This opens the _Preferences_ dialog. Select _Install/Update_ → _Available Software Sites_ and enter the path to the zip file by clicking _Add_ → _Archive…​_ as shown in Figure 7.
![040 add zip archive](https://docs.knime.com/latest/analytics_platform_installation_guide/img/040_add_zip_archive.PNG)
Figure 7. Adding Update Sites from Zip Archive
|  If the same extensions are provided by a URL, you will first have to disable the update site by disabling it in the list.   
---|---  
  4. Now click _Apply and Close_
|  If the same extensions are also provided by a remote update site, you will first have to disable that update site by deselecting its entry in the _Available Software Sites_ dialog and confirming via _Apply and Close_.   
---|---  


## Working with the Nightly Builds 
Once a night, a new version of KNIME Analytics Platform is created directly from our development branch. The Nightly Build versions available here provide insight into what’s coming up in the next regular release. However, for real work, always use a version of a standard KNIME release. Also read the following disclaimer before proceeding:
|  **Really, really,_really_ important disclaimer** This is most definitely not production quality code. These nightly builds are what we use internally to validate and test recent developments, so they are not tested as thoroughly as standard KNIME releases. Furthermore new nodes or functionality may change substantially (or disappear entirely) from one build to the next. It’s even possible that workflows you edit or create with nightly builds stop being readable by future (or past) versions…​ These nightlies are a great way to get a sneak peek at what may be coming in the next version of KNIME and provide feedback and suggestions. They are not a particularly safe way to do real work.  
---|---  
## Release notes and changelogs (KNIME Analytics Platform 5.4) 
Release notes and detailed changelog for v5.4.x releases
### KNIME Analytics Platform 5.4.3 
Release date: March 12, 2025
#### Changelog (KNIME Analytics Platform 5.4.3) 
##### Enhancements 
  * AP-23801: Allow to define model context size in GPT4All Connectors (kudos to @havarde for reporting 1)
  * AP-24042: Extract Context Properties node to emit only "old" properties if system property is set (`-Dorg.knime.base.extractcontextproperties.compatibilitymode=true`)
  * AP-24009: Add garbage collection to executor metrics
  * AP-23931: Make KNIME Explorer synchronization less resource intensive on remote end
  * AP-23873: Tool calling in Chat Model Prompter
  * AP-23836: Make Google Sheets nodes more robust by retrying failing requests


##### Bug Fixes 
  * AP-23905: Google Sheets Reader cannot read sheets with + in sheet name (kudos to @Lee_F for reporting 1)
  * AP-23909: Expression "like" Function Fails with Consecutive Wildcards (%) and Multiple Underscores (_) in Patterns (kudos to @cmora for reporting 1)
  * AP-23979: String Manipulation (Multi Column): Flow Variables present upon configuration but gone upon execution (kudos to @mwiegand for reporting 1)
  * NXT-3229: Some community nodes are not categorized in node repository’s tree view (kudos to @sw1336 for reporting 1)
  * AP-23976: KNIME URI resolution fails if no Workbench is running and URL is not accessible
  * NXT-3266: The output Email Session port view of the Email Connector fails to load and prompts to download an empty index.html file
  * UIEXT-2314: Nominal Row Filter Widget: No value selected upon first re-execution
  * UIEXT-2301: Nominal Row Filter Widget can cause validation errors in re-execution
  * AP-24055: E-Mail port spec view not (always) available
  * AP-24049: DB nodes get stuck in execution if database connection has been closed by database server during execution of parallel DB node that shares the same connection
  * AP-24038: Ordering of two INTEGERS that evaluate to MISSING is wrong
  * AP-24014: Loading executed workflow: Table file extraction happening in non-annotated thread, causing files to be place in temp root (instead of workflow temp)
  * AP-23995: SharePoint Online Connector does not allow to select group sites if using a Microsoft secret from the Secrets Retriever node
  * AP-23974: Google Sheets nodes: If a sheet name is a valid range (like A4:D20), the nodes use that range in the first sheet instead of a sheet with the name "A4:D20 "
  * AP-23973: Google Sheets Updater node ignores "Select First Sheet" option
  * AP-23972: Google Sheets nodes overwrite existing flow variables with name, sheet name and sheet id and instead of creating new variables
  * NXT-3416: Component drag’n’drop with a file extension adds node instead of component
  * NXT-3361: User profile (settings, onboarding hints) not reliably saved on shutdown


#### Known issue 
Updating KNIME installations older than 5.4.0 might trigger a _Trust Certificates_ confirmation due to an expired certificate from _Equo Tech, Inc._ This extension is safe to install, and a new certificate will be included in the 5.4.4 bugfix release.
### KNIME Analytics Platform 5.4.2 
Release date: February 14, 2025
#### Changelog (KNIME Analytics Platform 5.4.2) 
##### Enhancements 
  * AP-23677: Reduce error logging in Tableau Reader node


##### Bug Fixes 
  * AP-23957: Secrets Retriever node fails with unknown secret types on freshly started Hub Executors
  * AP-23950: Can’t log into KNIME Server if OAuth token is opaque (depends on identity provider in use)
  * AP-23715: Installed Python extensions break after moving an AP installation


### KNIME Analytics Platform 5.4.1 
Release date: February 6, 2025
#### Changelog (KNIME Analytics Platform 5.4.1) 
##### New nodes 
  * AP-23896: DeepSeek Chat Model Connector


##### Enhancements 
  * AP-22348: Support meta data filters in Vector Store Retriever (kudos to @TosinLitics for reporting 1)
  * AP-23605: Change node categories of Expression nodes (kudos to @mwiegand for reporting 1)
  * AP-23881: AWS Translate nodes: Update available languages
  * AP-23880: Snowflake Connector node with support for generic OAuth2 access token (e.g. Snowflake OAuth)
  * AP-23877: Improve node error message when Conda is cancelled due to the Watchdog
  * AP-23865: Enable Hub execution for Tableau Reader
  * AP-23843: Make watchdog consider current memory usage instead of theoretical max mem usage
  * AP-23790: Ability to update (legacy) shared metanodes to components
  * AP-23782: Give users a direct way to login to K-AI’s backend Hub after session timeout
  * AP-23764: MySQL Connector node with new default JDBC parameter to enable cursor-based streaming to avoid memory problems when reading large results
  * AP-23733: Handle failing redirects (e.g. firewall) during Hub downloads
  * AP-23009: Support JSON mode for OpenAI chat models in prompters
  * AP-22968: Check container memory usage for job acceptance thresholds as well
  * AP-22954: Prevent Conda Env Prop Nodes from executing in parallel
  * AP-22931: Add container metrics to executor metrics endpoint
  * AP-22777: Add /metrics endpoint to Executors
  * AP-18631: Integrated Deployment / Workflow Executor: Always execute entire workflow (not just output node)
  * NXT-3078: Hide single group and space in server providers
  * NXT-3047: New customization in AP to define custom "tile data" on Home screen (served from KNIME Business Hub)


##### Bug Fixes 
  * AP-23656: OpenAI nodes don’t work for the o-series reasoning models (kudos to @TosinLitics for reporting 1)
  * AP-23781: Python Views do not display non-ASCII unicode characters correctly on Windows (kudos to @guanlantang for reporting 1)
  * AP-23850: Query flattener creates invalid statement for SAP Hana if no aggregation column is used in the GroupBy node (kudos to @tescnovonesis for reporting 1)
  * NXT-3202: Port View not shown for Data Table Output Ports (kudos to @rfeigel, @VAGR_ISK for reporting 1, 2)
  * NXT-3229: Some community nodes are not categorized in node repository’s tree view (kudos to @sw1336 for reporting 1)
  * NXT-3327: Memory leak in Modern UI when rendering workflows (kudos to @mwiegand for reporting 1)
  * AP-23901: Remote shared component cannot be dropped to workflow immediately after renaming or moving it
  * AP-23810: Selection events do not work after the Concatenate node
  * AP-23773: Hub authentication token can expire if not interacting with Hub for some time (only Modern UI)
  * AP-23743: Support disabling K-AI via a flag in knime.ini
  * NXT-3289: Cannot delete local workflow from Sidebar Space Explorer
  * UIEXT-2403: Lengthy strings (>10k characters) are cut too short in table view (to 1k, should remain at 10k)
  * AP-23888: Character ![16](https://docs.knime.com/latest/analytics_platform_installation_guide/img/icons/bar-chart-emoji.svg) breaks the creation of workflow.svg
  * AP-23823: Google Ads Extension cannot be loaded in January
  * AP-23814: Watchdog cannot be disabled on hub executors
  * AP-23766: Nested parameter groups are not getting rendered in configuration dialogs
  * AP-23714: Conda package caches are not cleared after installation - bundling/root folder is huge
  * AP-23692: SSH Command Executor should be cancelable
  * AP-23627: Python Legacy integration does not work with numpy 2
  * AP-23601: GET Request node: ArrayIndexOutOfBoundsException in component with simple streaming job manager
  * AP-23482: KNIME Hub Authenticator should ignore credential settings if other KNIME Hub option is not used
  * AP-22088: Unconnected Google Analytics Connector throws NPE when opening dialog
  * NXT-3231: UI health checker continues to run while 'UI not responding' dialog is open
  * NXT-2982: Local storage state (onboarding hints) not retained across AP restarts


### KNIME Analytics Platform 5.4.0 
Release date: December 6, 2024
#### Release notes 
**Updates and compatibility**
  * Node API: Plug-in org.knime.core and org.knime.core.util no longer re-export the java package org.apache.commons.io*, 3rd party extensions might need to declare an explicit plug-in dependency in their MANIFEST.MF
  * Node API: The (deprecated) method NodeFactory#addLoadedFactories has been filled in empty. 3rd party must register nodes via extension points. (Method was deprecated since version 4.2.)
  * Authentication popups provided by the Eclipse platform are suppressed globally. If an extension needs the popup to provide username and password authentication, set the Java system property knime.auth.popups.allowed to true.
  * Perspective in Modern UI: All nodes are shown in the node repository (vs “Starter Nodes” only, as previously done in 5.3.x and before). This can be changed via preference setting.
  * Twitter: The extension is now declared “legacy”, due to various API changes. It can still be installed, but it is no longer listed by default (i.e. uncategorized)


#### Changelog (KNIME Analytics Platform 5.4.0) 
##### New nodes 
  * AP-17293: HEAD Request (kudos to @tbtt, @mwiegand, @tiaandp for reporting 1, 2, 3)
  * AP-23439: Databricks Embedding Connector
  * AP-23396: Tableau Reader (Labs)
  * AP-23251: Databricks Chat Model Connector
  * AP-23066: Variable Expression
  * AP-22958: Expression Row Filter
  * UIEXT-2349: Multiple File Upload Widget (kudos to @r_jainm, @Anjo, @Karlygash, @dnaki, @tiaandp, @nbrooijmans, @lsandinop for reporting 1, 2, https://forum.knime.com/t/ multiple-file-upload-file-upload-widget-and-file-chooser-widget/47653[3], 4, https://forum.knime.com/t/ request-select-multiple-files-or-directories-from-file-upload-widget/33601[5], 6, https://forum.knime.com/t/ uploading-multiple-files-from-same-directory-for-processing/12974[7], 8)
  * BD-1258: (Big Data Extensions): Databricks SQL Warehouse Connector


##### Enhancements 
  * AP-17683: Timer Info node: Add node annotation column to output (kudos to @Artem for reporting 1)
  * AP-19772: SSH Command Executor (replaces External SSH Tool node) (kudos to @RNovak for reporting 1, 2)
  * AP-20257: Add Bearer token authentication in REST Client Nodes (kudos to @JanDuo for reporting 1)
  * AP-20920: Provide API to create parameter arrays in Python node dialogs (kudos to @Vits for reporting 1)
  * AP-21273: Disable "Eclipse-based" auto-update check within KNIME AP (kudos to @ManiAthreyaS for reporting 1)
  * AP-23110: Snowflake Connector with configurable account domain (kudos to @Rock for reporting 1)
  * AP-23350: K-AI QA: display code nicer (kudos to @kowisoft for reporting 1)
  * AP-23623: Do not show initial syntax error for empty expressions (kudos to @MartinDDDD for reporting 1)
  * NXT-2193: Add "Copy to…​" function for Space explorer items (kudos to @HaveF for reporting 1)
  * NXT-2550: Reveal active hub space project in space explorer (kudos to @rrousselot for reporting 1)
  * NXT-2818: Open Quick-node-adding via double-click on canvas (kudos to @mwiegand for reporting 1)
  * NXT-2842: Modern Destination Picker for Upload, Download, Save, Move, Copy (kudos to @bobpeers for reporting 1)
  * NXT-2889: Export workflow via Save-As dropdown (kudos to @iCFO for reporting 1)
  * UIEXT-1163: SVG support in Views (kudos to @mwiegand for reporting 1)
  * UIEXT-1207: Add UI to create links in text view (kudos to @michazeidan for reporting 1)
  * UIEXT-1855: Enable flow variable handling for complex settings (kudos to @ArjenEX for reporting 1)
  * AP-23603: Twitter (X) extension removed from update site
  * AP-23546: Improve documentation for durations/periods in Column Expressions (legacy)
  * AP-23516: Deprecate Snowflake database driver version 3.13.24
  * AP-23511: Support OpenSSL V3 private keys in Snowflake Connector node by adding the Bouncy Castle JVM argument to knime.ini file
  * AP-23540: Upgrade avro in TP from 1.11.3 to 1.11.4 (CVE-2024-47561)
  * AP-23506: Use current Azure libraries in Azure Blob Storage Connection (Legacy)
  * AP-23497: Fetch disclaimer of K-AI scripting assistant from the ai-service
  * AP-23489: Update OkHttp, okio and Kotlin (CVE-2023-3635)
  * AP-23407: Mention missing-coalescing operator in error message if a non-missing type was expected but an optional was given
  * AP-23406: let if function treat MISSING as FALSE
  * AP-23374: LLM Prompter: Provide system prompt column for chat models
  * AP-23373: LLM Prompter: Allow to overwrite system prompt of chat models
  * AP-23363: Make sure Conda Environment Propagation Node doesn’t block in configure
  * AP-23359: Include K-AI in the default installation of AP
  * AP-23343: Include error details in node monitor and Catch-node
  * AP-23222: Include packages in Python-extension JARs but delete after use
  * AP-23191: Enable drag and drop of KNIME Hub short-links
  * AP-23173: Remove console from expression node (but leave it in for Python and ECharts)
  * AP-23163: Numeric Outlier Node: Change default quartile calculation value from R_4 to R_6
  * AP-23114: Update Snowflake driver to version 3.20
  * AP-23098: Support DB tables with no columns
  * AP-23083: New "Use latest driver version available" option in all DB connector node dialogs
  * AP-23045: Deprecate legacy Google Analytics Connector and Query nodes due to Google API shutdown
  * AP-22968: Check container memory usage for job acceptance thresholds as well
  * AP-22955: Use latest miniforge instead of micromamba+conda in executors
  * AP-22891: DB framework: Change default of retrieve metadata in configure of all DB connector nodes to disable to prevent UI freezes in very large workflows
  * AP-22816: Tableau Extension: Native support for Apple Silicon and update Hyper API to 0.0.20027
  * AP-22715: Allow and implement warnings in aggregations
  * AP-22655: Internal API: Remove commons.io package re-exports from `org.knime.core.util`
  * AP-22590: Support PKCE in OAuth2 Authenticator
  * AP-22371: Improve Expression Syntax Error messages
  * AP-22259: Allow to work with multiple expressions with support for reordering and focussing
  * AP-22184: Provide descriptive error message for not installed deprecated JDBC drivers
  * AP-22174: Update Azure storage client libraries
  * AP-21668: Preference for hiding K-AI
  * AP-21631: Timer Info node: Report nodes in (nested) components
  * AP-21071: Add shortcut to run Python script
  * AP-20919: Allow to lay out Python node dialog elements horizontally
  * AP-20333: "Generate new RowIDs" option as new default in Concatenate node
  * AP-19947: Cache nodes from extension on first load if it is an installed extension
  * AP-19942: Extension point for Python-compatible PortObjects
  * AP-19595: Allow to check if column is contained in schema
  * BD-1328: (Big Data Extensions): Upgrade local Spark from 3.5.1 to 3.5.3
  * BD-1324: (Big Data Extensions): Upgrade avro in local Spark from 1.11.3 to 1.11.4 (CVE-2024-47561)
  * BD-1322: (Big Data Extensions): Support Databricks Workspace Connector port as input to Create Databricks Environment and Databricks File System Connector node
  * BD-1302: (Big Data Extensions): Remove concept of a default Spark Context
  * NXT-3049: Add Hub link to help menu
  * NXT-2985: Context-based home page tiles
  * NXT-2951: Add preference to enable/disable embedded dialogs
  * NXT-2863: Always ask for confirmation to close all workflows when switching perspectives
  * NXT-2682: Apply space renaming on clickaway
  * NXT-2672: CTRL + Shift + f to search in space view
  * NXT-2610: Quick-node insertion for backwards connections
  * NXT-1580: Delay workflow loading until AP in a ready state
  * UIEXT-2287: Flow variable errors throw very technical stacktrace
  * UIEXT-2165: Introduce new 'cell renderer' for port view
  * UIEXT-2091: Improve missing value handling in view nodes
  * UIEXT-2050: Double click on legend in view nodes filter for this item
  * UIEXT-2034: Small task polishing of views
  * UIEXT-1868: Include Breadcrumbs in file chooser
  * UIEXT-1867: Enable Embedded data tab for file chooser in new dialogs
  * UIEXT-1806: Web UI for Table Reader
  * UIEXT-49: WebUI dialogs to work in remote workflow editor


##### Bug Fixes 
  * AP-17326: JSON Path node slow/freezes (kudos to @HaveF, @Page0727, @Gonzo for reporting 1, 2, 3)
  * AP-23573: Excel Reader cannot read XLSX file from remote mountpoint if filename has less than 3 characters (kudos to @mychoi for reporting 1)
  * AP-23587: Add (legacy) suffix to Column/Variable Expressions to resolve node name collisions with new Expression nodes (kudos to @mwiegand for reporting 1)
  * AP-23629: Export workflow summary/SVG is missing in classic UI (kudos to @rfeigel for reporting 1)
  * AP-7071: Streaming executor causing thread pool errors. (kudos to @izaychik63 for reporting 1)
  * UIEXT-2143: Error Message on Column Resorter when moving columns up and down (kudos to @richards99 for reporting 1)
  * UIEXT-2248: File download widget opens images inside the view instead of downloading them (kudos to @TakuShikanaiJIS for reporting 1)
  * UIEXT-941: Date&Time widget always displays browser timezone (kudos to @roherm for reporting 1)
  * AP-23513: Base DB nodes are missing in the macOS standard build
  * AP-19503: Copy and Paste of Component with Missing Node Breaks Workflow Editor
  * AP-23696: Windows: Workflow download from Hub sometimes fails due to `AccessDeniedException` during import operation
  * AP-23625: Memory leak in XPath dialog XML cell preview
  * AP-23609: Copying and pasting a nested component inside itself leads to recursion
  * AP-23591: Confusing error in dialog if incompatible port is connected
  * AP-23584: Unpivot: Preformatted text (pre) in node description may exceed its containing box
  * AP-23534: Generic ECharts box plot template shows wrong min/max
  * AP-23528: Shared component cannot be used in Linux (name conflict in folder name)
  * AP-23517: knime-python-base doesn’t depend on PILLOW even though we need it when working with images
  * AP-23480: Container Input (Raw HTTP) shows only first couple of header/parameter entries
  * AP-23467: ClassicUI: AP shutdown doesn’t wait for workflow uploads to Hub to finish (e.g. on save of a hub workflow)
  * AP-23419: Download workflow from Hub to overwrite local workflow instance may not clear workflow directory properly
  * AP-23404: Azure OpenAI Connector: Handling for incorrect URLs
  * AP-23369: Credentials Cache (e.g. used in Salesforce Connector) might cause memory leak for long running KNIME instances
  * AP-23324: K-AI does not suggest latest nodes (e.g. Row Filter) in Q&A mode
  * AP-23288: Expression node cannot be configured if images in input table
  * AP-23205: K-AI uses deprecated Row Filter in build mode
  * AP-23113: Webpage Retriever returns broken XML for websites with invalid XML characters
  * AP-23047: NumberFormatException for "_" in column access offset
  * AP-22983: CXF-based non-redirecting requests do not find proxy authorization
  * AP-22972: Joiner node resets progress during execute
  * AP-22899: Giskard Scanner: Error in inference workflow obscured
  * AP-22884: Improve error message of Decompress Files node when decompressing a workflow export
  * AP-22860: Parallel Chunk Loop node: NullPointerException due to pcc is null
  * AP-22841: Workflow Reader: Can’t read Workflow with only Annotation
  * AP-22691: Error on empty choices in Python based nodes
  * AP-22378: Schedule dialog complains about duplicate email addresses for post-execution actions
  * AP-22086: Copy-Pasting component containing missing node does not paste any connections (and component without offset)
  * AP-17655: Table Manipulator loses sorting when configured
  * NXT-3111: "Create workflow" button is overlapped by long folder name
  * NXT-2899: Large data in flow variable freezes application
  * NXT-2815: Component tags lost on copy/paste
  * NXT-2371: Undoing an annotation deletion does not set dirty flag
  * UIEXT-2300: Scatter Plot Matrix with nominal color scale may plot incorrect data


#### Nodes changing in KNIME Analytics Platform 5.4.0 
**New nodes:**
  * Variable Expression
  * Expression Row Filter
  * SSH Command Executor
  * Databricks SQL Warehouse Connector
  * Tableau Reader (Labs)
  * Multiple File Upload Widget (Labs)
  * HEAD Request
  * Databricks Chat Model Connector
  * Databricks Embedding Connector
  * String Comparison
  * GDELT Global Knowledge Graph
  * Open Sky Network Data
  * TomTom Isochrone Map
  * Directed Bezier Curve
  * Create H3 Grid
  * Point to H3
  * FIFO / LIFO Resolver
  * SiriusExport
  * IonMobilityBinning
  * SageAdapter
  * AssayGeneratorMetaboSirius
  * ChemDraw Widget


**Nodes replaced, old nodes deprecated:**
  * External SSH Tool (deprecated)
  * Google Analytics Connector (deprecated)
  * Google Analytics Query (deprecated)
  * ChemSpider (deprecated)


**Nodes declared legacy:**
  * Column Expressions (legacy)
  * Variable Expressions (legacy)
  * Twitter Retweet Search (legacy)
  * Twitter Timeline (legacy)
  * Twitter API Connector (legacy)
  * Twitter Streaming (deprecated) (legacy)
  * Twitter Followers Search (legacy)
  * Twitter Users (legacy)
  * Twitter Post Tweet (legacy)
  * Twitter Search (legacy)


**Nodes renamed:**
  * Geometry to Lat/Long → Geometry to Lat/Lon


#### Extensions changing in KNIME Analytics Platform 5.4.0 
**New extensions:**
  * KNIME Chemdraw feature
  * FMI KNIME Plugins


**Extensions renamed:**
  * KNIME Column Expressions (Labs) → KNIME Column Expressions (legacy)
  * KNIME Twitter Connectors → KNIME Twitter Connectors (legacy)
  * KNIME Extension for Apache Spark → KNIME Extension for Apache Spark (legacy)


**Extensions moved:**
  * KNIME Hub Additional Connectivity (Labs) → KNIME Hub Additional Connectivity
  * KNIME-CDK - Moved from trusted to experimental


#### Community Extensions 
**Partner Extensions**
Extension | Contributor | Changes  
---|---|---  
ChemAxon/Infocom JChem Extensions Feature | INFOCOM CORPORATION | **Available**  
ChemAxon/Infocom Marvin Extensions Feature | INFOCOM CORPORATION | **Available**  
Schrödinger Extensions for KNIME | Schrödinger LLC | Available soon  
LigandScout Extensions for the KNIME Workbench | Inte:Ligand GmbH | Available soon  
MOE Extensions for KNIME | Chemical Computing Group ULC | **Available**  
Pharmacelera extensions | Pharmacelera S.L. | **Available**  
Spotfire File Nodes | TIBCO Spotfire | **Available**  
Symanto Brain | Symanto | **Available**  
Market Simulation nodes | Decision Ready, LLC | **Available**  
Metadata-Hub Extension | GRAU DATA GmbH | **Available**  
KNIME Connector for SAP(KCS) Nodes | De Villiers Walton Ltd. | Available soon  
exorbyte extension | exorbyte GmbH | Available soon  
KNIME H2O Driverless AI Integration | H2O.ai | Not available anymore since 5.x  
**Trusted Community Extensions**
Extension | Status  
---|---  
RDKit Nodes Feature | **Available**  
Vernalis KNIME Nodes | **Available**  
Generic Workflow Nodes for KNIME | **Available**  
Lhasa | **Available**  
Slack Integration | **Available**  
AF Utility Nodes | **Available**  
KNIME Groovy Scripting extension | **Available**  
KNIME HCS Tools | **Available**  
KNIME Python Scripting extension | **Available**  
KNIME R Scripting extension | **Available**  
KNIME Matlab Scripting extension | **Available**  
Continental Nodes for KNIME | **Available**  
Genentech | **Available**  
Geospatial Analytics Extension | **Available**  
Neo4J | **Available**  
KNIME Image Processing | **Available**  
OpenMS | **Available**  
Redfield NLP Nodes | **Available**  
Redfield Privacy Nodes | **Available**  
Redfield Conformal Prediction Nodes | **Available**  
**Experimental Community Extensions**
Extension | Changes  
---|---  
CIR KNIME Integration | **Available**  
Process Mining Extension | **Available**  
Apprise Nodes | **Available**  
Erlwood KNIME Open Source | **Available**  
KNIME Shapefile Support | **Available**  
Indigo KNIME integration | **Available** - Not working on macOS because of a bug in Indigo  
AI.Associates Signal Processing | **Available**  
Enalos Nodes for KNIME | **Available**  
3D-e-Chem KNIME nodes | Available soon  
FSK-Lab | **Available**  
Word2Vec | Available soon  
PIA | **Available**  
Redfield BERT Nodes | **Available**  
AIA Insights Bioactivity Predictor | Available soon - New since 2023  
Visualization for supply chains | **Available** - New since 2023  
BIM - Building Information Modelling Extension for KNIME (IFC) | **Available** - New since 2023  
Smartsheet extension | **Available** - New since 2024  
Redfield PST Nodes | **Available** - New since 2024  
Chem AI Extension | **Available** - New since 2024  
Salesforce Extension | **Available** - New since 2024  
SATELLiTES Extension | **Available** - New since 2024  
FMI KNIME Plugins | **Available** - Revived in 2024  
MMI Data Analytics Nodes | Not available anymore since 5.2  
OrientDB | Not available anymore since 5.1  
KNIME-CDK | **Available** - Moved to Experimental 2024 - ChemSpider node deprecated due to API change  
  * Installing KNIME Analytics Platform
    * Configuration settings and knime.ini file
    * Allocating memory in knime.ini file
  * Installing Extensions and Integrations
  * Updating KNIME Analytics Platform and Extensions
  * Update Sites
    * Default Update Sites
    * Adding External Update Sites
    * Adding Local Update Sites
  * Working with the Nightly Builds
  * Release notes and changelogs (KNIME Analytics Platform 5.4)
    * KNIME Analytics Platform 5.4.3
    * KNIME Analytics Platform 5.4.2
    * KNIME Analytics Platform 5.4.1
    * KNIME Analytics Platform 5.4.0


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. Kerberos User Guide 


KNIME Analytics Platform 5.4
# Kerberos User Guide
IntroductionOverviewKerberos ConfigurationUse system defaults (<strong>discouraged</strong>)Use Kerberos client configuration file (<code>krb5.conf</code>)Use realm and KDCLog into KerberosHow to log inStatus barStatusDebug LoggingGlossaryIntroduction
  * Introduction
  * Overview
  * Kerberos Configuration
    * Use system defaults (**discouraged**)
    * Use Kerberos client configuration file (`krb5.conf`)
    * Use realm and KDC
  * Log into Kerberos
    * How to log in
    * Status bar
    * Status
  * Debug Logging
  * Glossary


Download PDF
## Introduction 
This guide describes step-by-step how to configure Kerberos on KNIME Analytics Platform.
Kerberos, which dates back to 1993, is a network authentication protocol for distributed applications, and support for it is also integrated into KNIME Big Data Extensions and KNIME Extension for Apache Spark.
You can use Kerberos authentication to connect to a wide-array of Kerberos-secured services. Using KNIME Analytics Platform with the Big Data Extensions you can e.g connect to Kerberos-secured Hive clusters using Hive Connector node, or Impala using Impala Connector node.
## Overview 
To configure Kerberos in KNIME Analytics Platform, go to _File > Preferences > KNIME > Kerberos_ and open the Kerberos preferences page.
As shown in Figure 1, connecting to Kerberos consists of the following two high-level steps:
  1. Setup Kerberos configuration.
  2. Log into Kerberos (i.e. obtain a Kerberos ticket).


![02 preferences](https://docs.knime.com/latest/analytics_platform_kerberos_user_guide/img/02_preferences.png)
Figure 1. Kerberos preferences page
The following sections will explain these steps in more details.
|  If any of the Kerberos terminology is unclear, you can refer to the Glossary at the bottom of this guide.   
---|---  
## Kerberos Configuration 
The first step is to configure Kerberos in KNIME Analytics Platform (see Figure 1). One of the three options provided can be selected, depending on your needs and the environment setup of the system in use.
### Use system defaults (**discouraged**) 
|  This option is **discouraged** , because the correct setup is highly dependent on the system environment. In addition, if you move the preferences file or KNIME Analytics Platform to another machine, it might not work anymore. However, select this option if you want to keep existing setup running, or if you want to manage Kerberos configuration outside KNIME Analytics Platform.   
---|---  
When selecting this option KNIME Analytics Platform will look at a set of default locations for the `krb5.conf` (Kerberos client configuration file).
#### Possible locations for `krb5.conf`
KNIME Analytics Platform will try the following locations in the given order:
  1. First, it checks whether the `java.security.krb5.conf` system property is set. If so, it will try to read the file from the location specified in this system property.
  2. Otherwise, it will try to read the `krb5.conf` from the Java Runtime Environment of KNIME Analytics Platform:
```
<knime-analytics-platform-installation>/plugins/org.knime.binary.jre.<version>/jre/lib/security/krb5.conf
```

  3. If the above also fails, it will try the following operating system dependent locations:
     * Windows: `C:\Windows\krb5.ini`
     * Linux/macOS: `/etc/krb5.conf`


For more information, please refer to the Kerberos documentation.
### Use Kerberos client configuration file (`krb5.conf`) 
This option is the recommended way to configure KNIME Analytics Platform, since it allows for full configurability of the settings in the Kerberos client configuration file (`krb5.conf`).
First, you need to obtain a valid `krb5.conf` file. If you don’t know where the file is located or if you don’t have one already, please contact your local administrator. Alternatively, you can write a `krb5.conf` file yourself. A minimal configuration file could look like this:
```
[libdefaults]
default_realm = MYCOMPANY.COM
[realms]
MYCOMPANY.COM = {
 kdc = kdc.mycompany.com
 admin_server = kdc.mycompany.com
}
```

The above example declares that you are in a Kerberos realm called `MYCOMPANY.COM` and that the hostname of the Kerberos KDC is `kdc.mycompany.com`. Adjust these values as appropriate for your setup. Depending on your setup, more configuration settings may be necessary. The `krb5.conf` format is fully described as part of the MIT Kerberos documentation.
Now, move the `krb5.conf` file into a location of your choice, where it can be accessed by KNIME Analytics Platform, and enter the file path in the Kerberos preferences page. It is recommended to store the file outside of the KNIME Analytics Platform installation folder, to avoid accidentally deleting it during upgrades.
### Use realm and KDC 
The easiest way is to insert, directly in the Kerberos preferences page:
  * The name of the realm (the name needs to be in uppercase letters)
  * The IP or hostname of the KDC


Based on the input realm and KDC, the `krb5.conf` file will be generated.
## Log into Kerberos 
### How to log in 
After Kerberos is configured, the next step is to select one of the following authentication methods to log into Kerberos.
#### With system ticket cache (**discouraged**) 
|  This option is **discouraged**.   
---|---  
Select this option if you want the ticket-granting ticket (TGT) to be obtained from the system ticket cache. The ticket cache will be searched in the following locations:
  * On Solaris and Linux: `/tmp/krb5cc_uid` where `uid` is numeric user identifier.
  * On Windows:
    * `C:\Users\<username>\krb5cc_<username>`.
    * Otherwise, if the file does not exist or if it does not contain a valid TGT, the TGT will be obtained from the Local Security Authority (LSA) API.
|  On recent Windows versions, further changes to the Windows Registry are required for Java processes such as KNIME Analytics Platform to read the TGT from LSA. This type of setup is not recommended as it poses a security risk.   
---|---  


If a valid TGT is present on the system, no further action is required to log into Kerberos.
|  For more information on this, please check the Oracle documentation.   
---|---  
#### With username and password 
Using username and password is the recommended way to log into Kerberos. Username and password will be prompted at login time.
#### With keytab 
Select this option to use _Principal_ and _Keytab_. No further user interaction is then required to log into Kerberos.
### Status bar 
Further down in the preferences page, under the _Status Bar_ section, the option _Permanently show login status_ can be selected, as shown in Figure 2.
![04 status bar](https://docs.knime.com/latest/analytics_platform_kerberos_user_guide/img/04_status_bar.png)
Figure 2. Kerberos status bar
Enabling this option will show the Kerberos login status in the lower bar of KNIME Analytics Platform (see Figure 3). The advantage of this option is that you can do login/logout and check the Kerberos status anytime without having to open the Kerberos preferences page.
![04 login bar](https://docs.knime.com/latest/analytics_platform_kerberos_user_guide/img/04_login_bar.png)
Figure 3. Kerberos status bar in KNIME Analytics Platform
Right-clicking on the status bar will open a menu containing two options:
![04 login status menu](https://docs.knime.com/latest/analytics_platform_kerberos_user_guide/img/04_login_status_menu.png)
Figure 4. Kerberos status bar menu
  * _Login_ triggers the login process. Selecting username and password to login, a pop-up window will open. Here, you can enter your Kerberos credentials (see Figure 5 below).
|  Double-clicking on the status bar will also trigger the login process.   
---|---  


![04 login prompt](https://docs.knime.com/latest/analytics_platform_kerberos_user_guide/img/04_login_prompt.png)
Figure 5. Login prompt for username and password
  * _Preferences_ opens the Kerberos preferences page. This can also be achieved by going to _File > Preferences > KNIME > Kerberos_.


### Status 
After configuring and selecting the authentication method, you can check the validity of your settings in the lower part of the preferences page.
Here, several information are shown:
  * _Status_ shows the Kerberos status in general, e.g it will show red messages if any error occurs.
  * Under _Log_ , click _View debug log_ to view Kerberos debug log messages. A pop-up window will appear showing all log messages related to Kerberos. Please make sure to enable Kerberos debug logging beforehand (please check the Debug Logging section for more information).
  * Click the _Validate_ button to validate the Kerberos configuration.
  * Click the _Log in_ button to log into Kerberos.


![04 kerberos status](https://docs.knime.com/latest/analytics_platform_kerberos_user_guide/img/04_kerberos_status.png)
Figure 6. Kerberos status
## Debug Logging 
If you encounter problems with the Kerberos setup it is helpful to enable Kerberos logging to get more information about the problem. To enable Kerberos logging, simply check the option _Kerberos debug logging to KNIME log and Console_ in Kerberos preferences page (go to _File > Preferences > KNIME > Kerberos_).
![05 kerberos debug](https://docs.knime.com/latest/analytics_platform_kerberos_user_guide/img/05_kerberos_debug.png)
Figure 7. Enable debug log
After a restart of KNIME Analytics Platform, additional Kerberos information will be displayed in the KNIME Console and KNIME log file.
|  You should restart KNIME Analytics Platform for the changes to be effective.   
---|---  
![05 debug log](https://docs.knime.com/latest/analytics_platform_kerberos_user_guide/img/05_debug_log.png)
Figure 8. View debug log
To see only Kerberos-related log messages, click _View debug log_ in the Kerberos preferences page. A new pop-up window will open containing Kerberos debug log messages (see Figure 8). Note that you need to enable the option _Kerberos debug logging to KNIME log and Console_ beforehand to be able to see the log messages.
## Glossary 
  * **KDC** Key Distribution Center, a server that handles Kerberos authentication.
  * **Principal** The Kerberos-equivalent to a username. In Kerberos, principals identify users or services. Examples:
    * A user principal: `joe@MYCOMPANY.COM`
    * A service principal (in this case for Hive Server 2): `hive/server.mycompany.com@MYCOMPANY.COM`
  * **Realm** Indicates an administrative domain. Both users and services are registered as principals with their passwords in a realm. Example: `MYCOMPANY.COM`
  * **Ticket** A piece of data that serves as proof that you have authenticated yourself as a principal.
  * **Ticket cache** Holds your Kerberos tickets. In order to work with KNIME Analytics Platform, tickets need to be stored in a file-based ticket cache, as specified by the KRB5CCNAME environment variable.


  * Introduction
  * Overview
  * Kerberos Configuration
    * Use system defaults (**discouraged**)
    * Use Kerberos client configuration file (`krb5.conf`)
    * Use realm and KDC
  * Log into Kerberos
    * How to log in
    * Status bar
    * Status
  * Debug Logging
  * Glossary


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. Create a New Java based KNIME Extension 


KNIME Analytics Platform 5.4
# Create a New Java based KNIME Extension
IntroductionSet up a KNIME SDKCreate a New KNIME Extension ProjectThe KNIME Node WizardTest the Example ExtensionProject StructureNumber Formatter Node ImplementationDeploy your ExtensionOption 1: Local Update Site (recommended)Option 2: dropinFurther ReadingIntroduction
  * Introduction
  * Set up a KNIME SDK
  * Create a New KNIME Extension Project
    * The KNIME Node Wizard
  * Test the Example Extension
  * Project Structure
  * Number Formatter Node Implementation
  * Deploy your Extension
    * Option 1: Local Update Site (recommended)
    * Option 2: dropin
  * Further Reading


Download PDF
## Introduction 
This quickstart guide describes how to create a new KNIME Extension in Java, i.e. write a new node implementation to be used in KNIME Analytics Platform. You will learn how to set up a KNIME SDK, how to create a new KNIME Extension project, how to implement a simple manipulation node, how to test the node, and how to easily deploy the node in order to make it available for others. Publishing the extension is explained in detail in the following guide: Publish Your Extension on KNIME Community Hub
For this purpose, we created a reference extension you can use as orientation. This KNIME Extension project can be found in the `org.knime.examples.numberformatter` folder of the `knime-examples` GitHub repository. It contains all required project and configuration files and an implementation of a simple _Number Formatter_ example node, which performs number formatting of numeric values of the input table. We will use this example implementation to guide you through all necessary steps that are involved in the creation of a new KNIME Extension.
## Set up a KNIME SDK 
In order to start developing KNIME source code, you need to set up a KNIME SDK. A KNIME SDK is a configured Eclipse installation which contains KNIME Analytics Platform dependencies. This is necessary as Eclipse is the underlying base of KNIME Analytics Platform i.e. KNIME Analytics Platform is a set of plug-ins that are put on top of Eclipse and the Eclipse infrastructure. Furthermore, Eclipse is an IDE, which you will use to write the actual source code of your new node implementation.
To set up your KNIME SDK, we start with an "Eclipse IDE for RCP and RAP Developers" installation (this version of Eclipse provides tools for plug-in development) and add all KNIME Analytics Platform dependencies. In order to do that, please follow the SDK Setup instructions. Apart from giving instructions on how to set up a KNIME SDK, the SDK Setup will give some background about the Eclipse infrastructure, its plug-in mechanism, and further useful topics like how to explore KNIME source code.
## Create a New KNIME Extension Project 
After Eclipse is set up and configured, create a new KNIME Extension project. A KNIME Extension project is an Eclipse plug-in project and contains the implementation of one or more nodes and some KNIME Analytics Platform specific configuration. The easiest way to create a KNIME Extension project, is by using the KNIME Node Wizard, which will automatically generate the project structure, the plug in manifest and all required Java classes. Furthermore, the wizard will take care of embedding the generated files in the KNIME framework.
### The KNIME Node Wizard 
  1. **Install the KNIME Node Wizard**
Open the Eclipse installation wizard at `Help → Install New Software…` , enter the following update site location: `https://update.knime.com/analytics-platform/5.4/` in the location box labelled `Work with:`.
Hit the Enter key, and put `KNIME Node Wizard` in the search box. Tick the `KNIME Node Wizard` under the category `KNIME Node Development Tools`, click the **Next** button and follow the instructions. Finally, restart Eclipse.
![install node wizard](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/install_node_wizard.png)
Figure 1. The KNIME Node Wizard installation dialog.
  2. **Start the KNIME Node Wizard**
After Eclipse has restarted, start the KNIME Node Wizard at `File → New → Other…` , select `Create a new KNIME Node-Extension` (can be found in the category `Other`), and hit the **Next** button.
![start new node wizard](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/start_new_node_wizard.png)
Figure 2. The KNIME Node Wizard start dialogs.
  3. **Create a KNIME Extension Project**
In the `Create new KNIME Node-Extension` dialog window enter the following values:
     * New Project Name: `org.knime.examples.numberformatter`
     * Node class name: `NumberFormatter`
     * Package name: `org.knime.examples.numberformatter`
     * Node vendor: `<your_name>`
     * Node type: Select `Manipulator` in the drop down menu.
Replace `<your_name>` with the name that you like to be the author of the created extension. Leave all other options as is and click **Finish**.
![node wizard dialog](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/node_wizard_dialog.png)
Figure 3. The KNIME Node Wizard dialog.
After some processing, a new project will be displayed in the Package Explorer view of Eclipse with the project name you gave it in the wizard dialog.
|  Make sure that the checkbox `Include sample code in generated classes` is checked. This will include the code of the aforementioned _Number Formatter_ node in the generated files.   
---|---  
![eclipse](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/eclipse.png)
Figure 4. A view of Eclipse after the KNIME Node Wizard has run.
In the `Package Explorer` view of Eclipse (left side) you should now see three projects. The two projects `org.apache.xmlbeans` and `org.knime.sdk.setup` which you imported in the SDK Setup, and the project `org.knime.examples.numberformatter` that you just created using the KNIME Node Wizard.


## Test the Example Extension 
At this point, all parts that are required for a new KNIME Extension are contained in your Eclipse workspace and are ready to run. To test your node, follow the instructions provided in the **Launch KNIME Analytics Platform Section** of the SDK Setup. After you started KNIME Analytics Platform from Eclipse, the _Number Formatter_ node will be available at the root level of the node repository. Create a new workflow using the node (see Figure below), inspect the input and output tables, and play around with the node.
![knime](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/knime.png)
Figure 5. KNIME Analytics Platform development version started from Eclipse showing an example workflow. The _Number Formatter_ node contained in the Eclipse workspace is displayed at the bottom of the node repository.
The node will perform simple rounding of numbers from the input table. To change the number of decimal places the node should round to, change the digit contained in the format String that can be entered in the node configuration (e.g. `%.2f` will round to two decimal places,the default value is `%.3f`). After you are done, close KNIME Analytics Platform.
## Project Structure 
Next, let’s review the important parts of the extension project you’ve just created. First, we’ll have a look at the files located in `org.knime.examples.numberformatter`.
The files contained in this folder correspond to the actual node implementation. There are four Java classes implementing what the node should do, how the dialog and the view looks like, one XML file that contains the node description, and an image which is used as the node icon (in this case a default icon) displayed in the workflow view of KNIME Analytics Platform. Generally, a node implementation comprises of the following classes: `NodeFactory`, `NodeModel`, `NodeDialog`, `NodeView`. In our case, these classes are prefixed with the name you gave the node in the KNIME Node Wizard, i.e. NumberFormatter.
  * `NumberFormatterNodeFactory.java`
The `NodeFactory` bundles all parts that make up a node. Thus, the factory provides creation methods for the `NodeModel`, `NodeDialog`, and `NodeView`. Furthermore, the factory will be registered via a KNIME extension point such that the node is discoverable by the framework and will be displayed in the node repository view of KNIME Analytics Platform. The registration of this file happens in the `plugin.xml` (see description of the `plugin.xml` file below).
  * `NumberFormatterNodeModel.java`
The `NodeModel` contains the actual implementation of what the node is supposed to do. Furthermore, it specifies the number of inputs and outputs of a node. In this case the node model implements the actual number formatting.
  * `NumberFormatterNodeDialog.java` (optional)
The `NodeDialog` provides the dialog window that opens when you configure (double click) a node in KNIME Analytics Platform. It provides the user with a GUI to adjust node specific configuration settings. In the case of the _Number Formatter_ node this is just a simple text box where the user can enter a format String. Another example would be the file path for a file reader node.
  * `NumberFormatterNodeView.java` (optional)
The NodeView provides a view of the output of the node. In the case of the _Number Formatter_ node there will be no view as the output is a simple table. Generally, an example for a view could be a tree view of a node creating a decision tree model.
  * `NumberFormatterNodeFactory.xml`
This XML file contains the node description and some metadata of the node. The root element must be a `<knimeNode> … </knimeNode>` tag. The attributes of this tag further specify the location of the node icon (`icon=”…​”`) and the type of the node (`type=”…​”`). Note that this is the type you selected in the dialog of the Node Wizard earlier. The most common types are `Source`, `Manipulator`, `Predictor`, `Learner`, `Sink`, `Viewer`, and `Loop`. The description of the node is specified in the children of the root tag. Have a look at the contents of the file for some examples. The `.xml` must be located in the same package as the `NodeFactory` and it has to have the same name (only the file ending differs).
  * `default.png`
This is the icon of the node displayed in the workflow editor. The path to the node icon is specified in the `NumberFormatterNodeFactory.xml` (`icon` attribute of the `knimeNode` tag). In this case the icon is just a placeholder displaying a question mark. For your own node, replace it with an appropriate image representative of what the node does. It should have a resolution of 16x16 pixels.


Apart from the Java classes and the factory `.xml`, which define the node implementation, there are two files that specify the project configuration:
  * `plugin.xml` and `META-INF/MANIFEST.MF`
These files contain important configuration data about the extension project, like dependencies to other plug-ins and the aforementioned extension points. You can double click on the `plugin.xml` to open an Eclipse overview and review some of the configuration options (e.g. the values we entered in KNIME Node Wizard are shown on the overview page under `General Information` on the left). However, you do not have to change any values at the moment.


![plugin xml qualifier](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/plugin_xml_qualifier.png)
Figure 6. Eclipse overview of the `plugin.xml and MANIFEST.MF`.
## Number Formatter Node Implementation 
Once you have reviewed the project structure, we have a look at some implementation details. We will cover the most important parts as the example code in the project you created earlier already contains detailed comments in the code of the implemented methods (also have a look at the reference implementation in the `org.knime.examples.numberformatter` folder of the `knime-examples` repository).
Generally, the _Number Formatter_ node takes a data table as input and applies a user specified format String to each `Double` column of the input table. For simplicity, the output table only contains the formatted numeric columns as String columns. This basically wraps the functionality of the Java `String.format(…​)` function applied to a list of `Double` values into a node usable in KNIME Analytics Platform.
Let’s work through the most important methods that each node has to implement. The functionality of the node is implemented in the `NumberFormatterNodeModel.java` class:
```
protected NumberFormatterNodeModel() {
  super(1, 1);
}
```

The `super(1, 1)` call in the constructor of the node model specifies the number of output and input tables the node should have. In this case it is one input and one output table.
```
BufferedDataTable[] execute(final BufferedDataTable[] inData, final ExecutionContext exec)
```

The actual algorithm of the node is implemented in the `execute` method. The method is invoked only after all preceding nodes have been successfully executed and all data is therefore available at the input ports. The input table will be available in the given array `inData` which contains as many data tables as specified in the constructor. Hence, the index of the array corresponds to the port index of the node. The type of the input is `BufferedDataTable`, which is the standard type of all tabular data in KNIME Analytics Platform. The persistence of the table (e.g. when the workflow is saved) is automatically handled by the framework. Furthermore, a `BufferedDataTable` is able to handle data larger than the size of the main memory as the data will be automatically flushed to disk if necessary. A table contains `DataRow` objects, which in turn contain `DataCell` objects. `DataCell`s provide the actual access to the data. There are a lot of `DataCell` implementation for all types of data, e.g. a `DoubleCell` containing a floating point number in double precision (for a list of implementations have a look at the type hierarchy of the `DataCell` class). Additionally, each `DataCell` implements one or multiple `DataValue` interfaces. These define which access methods the cell has i.e. which types it can be represented as. For example, a `BooleanCell` implements `IntValue` as a `Boolean` can be easily represented as 0 and 1. Hence, for each `DataValue` there could be several compatible `DataCell` classes. The second argument `exec` of the method is the `ExecutionContext` which provides means to create/modify `BufferedDataTable` objects and report the execution status to the user. The most straightforward way to create a new `DataTable` is via the `createDataContainer(final DataTableSpec spec)` method of the `ExecutionContext`. This will create an empty container where you can add rows to. The added rows must comply with the `DataTableSpec` the data container was created with. E.g. if the container was created with a table specification containing two `Double` columns, each row that is added to the container must contain two `DoubleCells`. After you are finished adding rows to the container close it via the `close()` method and retrieve the `BufferedDataTable` with `getTable()`. This way of creating tables is also used in the example code (see `NumberFormatterNodeModel.java`). Apart from creating a new data container, there are more powerful ways to modify already existing input tables. However, these are not in the scope of this quickstart guide, but you can have a look at the methods of the `ExecutionContext`. The `execute` method should return an array of output `BufferedDataTable` objects with the length of the number of tables as specified in the constructor. These tables contain the output of the node.
```
DataTableSpec[] configure(final DataTableSpec[] inSpecs)
```

The `configure` method has two responsibilities. First, it has to check if the incoming data table specification is suitable for the node to execute with respect to the user supplied settings. For example, a user may disallow a certain column type in the node dialog, then we need to check if there are still applicable columns in the input table according to this setting. Second, to calculate the table specification of the output of the node based on the inputs. For example: imagine the _Number Formatter_ node gets a table containing two `Double` columns and one `String` column as input. Then this method should return a `DataTableSpec` (do not forget to wrap it in an array) containing two `DataColumnSpec` of type `String` (the `Double` columns will be formatted to `String`, all other columns are ignored). Analogously to the `execute` method, the `configure` method is called with an array of input `DataTableSpec` objects and outputs an array of output `DataTableSpec` objects containing the calculated table specification. If the incoming table specification is not suitable for the node to execute or does not fit the user provided configuration, throw an `InvalidSettingsException` with an informative message for the user.
```
saveSettingsTo(final NodeSettingsWO settings)
```

and
```
loadValidatedSettingsFrom(final NodeSettingsRO settings)
```

These methods handle the loading and saving of settings that control the behaviour of the node, i.e. the settings entered by the user in the node dialog. This is used for communication between the node model and the node dialog and to persist the user settings when the workflow is saved. Both methods are called with a `NodeSettings` object (in a read only (RO) and write only (WO) version) that stores the settings and manages writing or reading them to or from a file. The `NodeSettings` object is a key-value storage, hence it is easy to write or read to or from the settings object. Just have a look at the provided methods of the `NodeSettings` object in your Eclipse editor. In our example, we do not write settings directly to the `NodeSettings` object as we are using a `SettingsModel` object to store the user defined format String. `SettingsModel` objects already know how to write and read settings from the `NodeSettings` (via methods that accept `NodeSettings`) and help to keep settings synchronization between the model and dialog simple. Furthermore, they can be used to create simple dialogs where the loading and saving of settings is already taken care of.
You can find the actual algorithm of the _Number Formatter_ node in the `execute` method in the `NumberFormatterNodeModel.java` class. We encourage you to read through the code of the above mentioned classes to get a deeper understanding of all parts of a node. For a more thorough explanation about how a node should behave consult the KNIME Noding Guidelines.
## Deploy your Extension 
This section describes how to manually deploy your Extension after you have finished the implementation using the _Number Formatter_ Extension as example. There are two options:
### Option 1: Local Update Site (recommended) 
The first option is to create a local Update Site build, which can be installed using the standard KNIME Analytics Platform update mechanism.
To create a local Update Site build, you need to create a `Feature` project that includes your extension. A Feature is used to package a group of plug-ins together into a single installable and updatable unit. To do so, go to `File → New → Other…` , open the `Plug-in Development` category, select `Feature Project` and click the **Next** button.
![start feature wizard](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/start_feature_wizard.png)
Figure 7. The Feature Project Wizard start dialogs.
Enter the following values in the `Feature Properties` dialog window:
  * Project ID: `org.knime.examples.numberformatter.feature`
  * Feature Name: `Number Formatter`
  * Feature Version: _leave as is_
  * Feature Vendor: `<your_name>`
  * Install Handler Library: _leave empty_


Replace `<your_name>` with the name that you like to be the author of the created extension. Additionally, choose a location for the new Feature Project (e.g. next to the _Number Formatter_ Extension) and click the **Next** button. On the next dialog choose `Initialize from the plug-ins list:` and select the `org.knime.examples.numberformatter` plug-in (you can use the search bar to easily find the plug-in). The plug-ins selected here are the ones that will be bundled into an installable unit by the Feature. Of course, you can edit that list later on. Finally, hit the **Finish** button.
![run feature wizard](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/run_feature_wizard.png)
Figure 8. The Feature Project Wizard dialogs.
After the wizard has finished, you will see a new project in your Eclipse Package Explorer with the `Project ID` you gave it earlier and Eclipse will automatically open an overview of the `feature.xml` (you can also open this view by double clicking on the `feature.xml` file located in the Feature Project). The Feature overview looks similar to the `plugin.xml` overview, be careful not to confuse them. You can view/modify the list of included plug-ins by selecting the `Included Plug-ins` tab at the bottom of the overview dialog.
|  Additionally to the information you entered in the Feature Project Wizard, you should provide a detailed Feature description, license and copyright information in the Feature meta data. This can be done by selecting the `Information` tab at the bottom of the overview dialog. This information will be displayed to the user during installation of the Feature.   
---|---  
![feature overview marked](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/feature_overview_marked.png)
Figure 9. Eclipse overview of the `feature.xml`. The link to create an Update Site Project is marked in red.
Next, you need to publish the Feature on a local Update Site. For this, first create an `Update Site Project` by clicking on the `Update Site Project` link on bottom right corner of the Eclipse overview dialog of the `feature.xml` (see figure above). This will start the Update Site Project Wizard.
![run update wizard](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/run_update_wizard.png)
Figure 10. The Update Site Project Wizard dialog.
On the shown dialog, enter the following:
  * Project name: `org.knime.examples.numberformatter.update`


Again, choose a location for the new Update Site Project and click the **Finish** button. Similar to the Feature Project Wizard, you will see a new project in your Eclipse Package Explorer with the `Project name` you gave it in the wizard dialog and Eclipse will automatically open an overview of the `site.xml` called `Update Site Map`. Again similar to a Feature, an Update Site bundles one or several Features that can be installed by the Eclipse update mechanism.
![update site overview](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/update_site_overview.png)
Figure 11. Eclipse overview of the `site.xml`. The Update Site in this image already contains one category called `number_formatting` where the `org.knime.examples.numberformatter.feature` was added to. This way the _Number Formatter_ Extension will be listed under this category during installation.
On the Eclipse overview of the `site.xml`, first create a new category by clicking on the **New Category** button. This will create a new default category shown in the tree view on the left. On the right, enter an `ID` like `number_formatting` and a `Name` like `Number Formatting`. This name will be displayed as a category and used for searching when the Feature is installed. Also, provide a short description of the category.
Second, select the newly created category from the tree view and click the **Add Feature…​** button. On the shown dialog, search for `org.knime.examples.numberformatter.feature` and click the **Add** button.
![update site feature selection](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/update_site_feature_selection.png)
Figure 12. The Feature Selection dialog.
At last, click the **Build All** button. This will build all Features added to the update site and create an installable unit that can be used to install the _Number Formatter_ Extension into an KNIME Analytics Platform instance.
|  The building of the Update Site might take some time. You can review the progress in the bottom right corner of Eclipse.   
---|---  
After building has finished, you can now point KNIME Analytics Platform to this folder (which now contains a local Update Site) to install the Extension. To do so, in KNIME Analytics Platform open the `Install New Software…​` dialog, click on the **Add** button next to the update site location, on the opening dialog click on **Local…​** , and choose the folder containing the Update Site. At last, give the local Update Site a name and click **OK**. Now, you can install the _Number Formatter_ Extension like any other plug-in.
|  Now that you have a working extension, why not sharing it with the community? Take a look at the following guide: Publish Your Extension on KNIME Community Hub  
---|---  
### Option 2: dropin 
The second option is to create a `dropin` using the `Deployable plug-ins and fragments` Wizard from within Eclipse. A `dropin` is just a `.jar` file containing your Extension that is simply put into the Eclipse `dropins` folder to install it.
To create a `dropin` containing your Extension, go to `File → Export → Plug-in Development → Deployable plug-ins and fragments` and click **Next**. The dialog that opens will show a list of deployable plug-ins from your workspace. Check the checkbox next to `org.knime.examples.numberformatter`. At the bottom of the dialog you are able to select the export method. Choose `Directory` and supply a path to a folder where you want to export your plugin to. At last click **Finish**.
![export](https://docs.knime.com/latest/analytics_platform_new_node_quickstart_guide/img/export.png)
Figure 13. The dialog of the deploy wizard.
After the export has finished, the selected folder will contain a `.jar` file containing your plugin. To install it into any Eclipse or KNIME Analytics Platform installation, place the `.jar` file in the `dropins` folder of the KNIME/Eclipse installation folder. Note that you have to restart KNIME/Eclipse for the new plugin to be discovered. In this example, the node is then displayed at the top level of the node repository in KNIME Analytics Platform.
## Further Reading 
  * For more information on development see the Developers Section of the KNIME website.
  * KNIME source code
  * If you have questions regarding development, reach out to us in the KNIME Development category of our forum.


  * Introduction
  * Set up a KNIME SDK
  * Create a New KNIME Extension Project
    * The KNIME Node Wizard
  * Test the Example Extension
  * Project Structure
  * Number Formatter Node Implementation
  * Deploy your Extension
    * Option 1: Local Update Site (recommended)
    * Option 2: dropin
  * Further Reading


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. KNIME Reporting Guide 


KNIME Analytics Platform 5.4
# KNIME Reporting Guide
IntroductionInstall KNIME Analytics PlatformWorking with the Reporting Extension in KNIMEStep 1: Read the dataStep 2: Add views to your workflow and create a componentStep 3: Customize the template of your reportStep 4: Write to fileShare your reportIntroduction
  * Introduction
  * Install KNIME Analytics Platform
  * Working with the Reporting Extension in KNIME
    * Step 1: Read the data
    * Step 2: Add views to your workflow and create a component
    * Step 3: Customize the template of your report
    * Step 4: Write to file
  * Share your report


Download PDF
## Introduction 
This guide describes the KNIME Reporting Extension, and shows how to create simple and advanced reports.
The KNIME Reporting Extension allows you to create static reports based on the results of your workflows. You can automatically generate and distribute customized reports for recurring events such as month-end close, quarterly performance, or on-demand statistics. For instance, you can send a PDF email report based on your data.
To visualize your results, you can include nodes from the KNIME Views Extension.
## Install KNIME Analytics Platform 
The Reporting Extension is included in the default installation of KNIME Analytics Platform version 5.2 or higher. It is recommended to download and install the latest version of the KNIME Analytics Platform. To do so, please refer to the KNIME Analytics Platform Installation Guide.
## Working with the Reporting Extension in KNIME 
In the following section, you will recreate the Reporting minimal example workflow.
This workflow demonstrates how to access and visualize data that is converted into a report with nodes from the KNIME Reporting Extension. If you have not already done so, you can download the resulting workflow from the KNIME Community Hub and reference it as needed.
As a starting point, you are provided with a basic data table. First, you want to access and visualize the data.
### Step 1: Read the data 
Download the `.csv` file from here. Then drag the `.csv` file on the workflow canvas. This will automatically open and configure a CSV Reader node.
### Step 2: Add views to your workflow and create a component 
#### Add views 
A crucial part to conveying a comprehensible overview of your data to the end user is visualization. To do this, you can use the nodes from the KNIME Views Extension. They integrate directly with the KNIME Reporting Extension.
In order to display the world population data, you can choose the chart that best fits your data. In this example, to visualize the data you can use a bar chart and view the population per country in a table.
To add the views, drag a connection to your workflow canvas. In the quick node insertion panel, locate the Bar Chart node and add it to your workflow. Repeat this step for the Table View node. Finally, your workflow will look like the one in Figure 1.
![03 add bar chart and table view nodes](https://docs.knime.com/latest/analytics_platform_reporting_guide/img/03_add_bar_chart_and_table_view_nodes.png)
Figure 1. Add view nodes to your workflow
|  View nodes from the KNIME JavaScript Views Extension or the KNIME Plotly Extension are not directly supported by the KNIME Reporting Extension.   
---|---  
##### Add rich text to your report 
The KNIME Views Extension also allows you to add text to your report. Search for the Text View node and then, drag it to your workflow canvas. Right click it to open the configuration dialog. It shows a preview of the text. You can manipulate it in the Rich Text Content editor on the right. At the top of the editor you can choose between the following formatting options (from left to right), as shown in Figure 2.
![03 text view configuration dialog](https://docs.knime.com/latest/analytics_platform_reporting_guide/img/03_text_view_configuration_dialog.png)
Figure 2. Text View configuration dialog
Text formatting
You can change the text formatting to:
  * Boldening (Ctrl B)
  * Italic (Ctrl I)
  * Underlined (Ctrl U).


Additionally, you can add bullet points, create a list and align your text (to the left, right, or center).
Click the ![16](https://docs.knime.com/latest/analytics_platform_reporting_guide/img/icons/menu-options.svg) icon to open a dropdown menu, which gives you access to more customization styles like adding code blocks or dividers to the text. The text style element also allows you to write in standard or small text, and to create headings. Alternatively, you can create a heading by adding number signs (#), followed by a space, in front of a word or phrase. The total amount of number signs should correspond to the heading level.
Flow variables
The contents of flow variables can be inserted by using the replacement syntax `$$["flow-variable-name"]`.
See the KNIME Components Guide to learn more about the usage of View nodes.
#### Create a component 
Fundamentally, the report you are creating consists of a component’s composite view. To include the view nodes in your report, wrap them in a component. First select the nodes, then click the _Create component_ button in the toolbar at the top. Open the component by double-clicking it while holding down the Ctrl key, or right-clicking and choosing _Component_ → _Open component_. The sub-workflow contained by the component is displayed.
#### Open the layout editor 
To customize the composite view, use the layout editor. Select _Open layout editor_ from the toolbar at the top. The layout editor will automatically create a layout, but it also allows you to customize it with a drag and drop grid. If you want to resort the order of the view elements, you can simply drag and drop them to the desired position. This is explained in further detail in the KNIME Components Guide.
|  In the layout preview the height of the Text View node is calculated depending on its content, but can be adjusted in the drag and drop grid. Click the cog icon in the upper right corner of the section corresponding to the Text View visualization node, as shown in Figure 3.   
---|---  
![03 text view customize in layout editor](https://docs.knime.com/latest/analytics_platform_reporting_guide/img/03_text_view_customize_in_layout_editor.png)
Figure 3. Customize the size of the text view in the layout editor
#### Enable the reporting function 
To enable the component to output the views for a report, tick the _Enable Reporting_ checkbox at the bottom of the layout editor and click _Finish_ , as shown in Figure 4.
![03 enable reporting](https://docs.knime.com/latest/analytics_platform_reporting_guide/img/03_enable_reporting.png)
Figure 4. Enable the reporting function by ticking the checkbox
This creates a Report input and a Report output port to your component. They will be visible as blue squares adjacent to the component once you exit it, as shown in Figure 5.
![03 component report output and report input port](https://docs.knime.com/latest/analytics_platform_reporting_guide/img/03_component_report_output_and_report_input_port.png)
Figure 5. Component with Report input and output ports
However, before you can execute the component, please proceed to Step 3: Customize the template of your report.
### Step 3: Customize the template of your report 
Before you can connect your component views to a reporting node, you need to customize the page size and orientation using the Report Template Creator node. Add it to your workflow and connect its output port to the existing Report input port on the left side of the component. Now your workflow should look like the one shown in Figure 6.
![03 report template creator node](https://docs.knime.com/latest/analytics_platform_reporting_guide/img/03_report_template_creator_node.png)
Figure 6. Report Template Creator node
Configure the Report Template Creator node to select the page size and orientation, as shown in Figure 7.
![03 report template creator configuration dialog](https://docs.knime.com/latest/analytics_platform_reporting_guide/img/03_report_template_creator_configuration_dialog.png)
Figure 7. Report Template Creator configuration dialog
Confirm with _Okay_ and execute the component. Now you can preview the output of the component by clicking the magnifier icon at the top right corner of the component.
|  If you want to add a second page to your workflow, use the Report Page Break node. It prevents your views from being cut off at the end of a page in your report file. You can find the Report Page Break node in the node repository. Add it in between two components to separate their contents with a page break on the report, as shown in Figure 8. ![03 add page break](https://docs.knime.com/latest/analytics_platform_reporting_guide/img/03_add_page_break.png) Figure 8. Add page break  
---|---  
### Step 4: Write to file 
In order to share the previously created content as a document, you need to save it to a file first. From the Report output port of the component creating the report, you can drag a connection and select compatible writer nodes. You can save your report as either a PDF or HTML file.
|  Make sure that the _Enable Reporting_ checkbox at the bottom of the component’s layout editor is selected and that a Report Template Creator node is connected to your component, as described in the previous steps. Otherwise, you cannot write the report to a file.   
---|---  
To save your report as a `.pdf` file, add the PDF Writer node to your workflow. This node allows you to write your report to a PDF file in a specified output location. By default on Windows, this is the user directory on your local computer, with the filename set to “report.pdf”, as indicated by the filepath below:
`C:\Users\<username>\report.pdf`
However, you can change the location path in the configuration dialog, e.g., to your workflow data area. This can be an operating system-dependent path on your local machine or a KNIME URL, as shown in Figure 9. You can also rename the PDF file as part of the filepath. Once executed, the file is saved to the specified location. If you make any changes to the workflow and want to overwrite the existing file, change the _If output file exists_ setting from the default _Fail_ to _Overwrite_ , as shown in Figure 9.
![03 report pdf writer configuration dialog](https://docs.knime.com/latest/analytics_platform_reporting_guide/img/03_report_pdf_writer_configuration_dialog.png)
Figure 9. Report PDF Writer configuration dialog
To access the PDF file, navigate to your KNIME workspace in the file explorer. If you have chosen to save your report as indicated in Figure 9, open your workflow folder and then the “data” subfolder. Here you will find your report in PDF format under the name you specified before.
To save your report as an HTML file, follow the same steps as before, but instead of adding the Report PDF Writer node, use the Report HTML Writer node.
## Share your report 
Once your report is finished you have the option to share it with others. For example, you can send it by email, as shown in this example workflow on the KNIME Hub.
For an overview of the extension, example workflows, and other resources, visit the collection page on the KNIME Hub.
  * Introduction
  * Install KNIME Analytics Platform
  * Working with the Reporting Extension in KNIME
    * Step 1: Read the data
    * Step 2: Add views to your workflow and create a component
    * Step 3: Customize the template of your report
    * Step 4: Write to file
  * Share your report


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. KNIME Analytics Platform User Guide 


KNIME Analytics Platform 5.4
# KNIME Analytics Platform User Guide
IntroductionWorkspacesUser interfaceEntry pageWorkflow editor &amp; nodesConnect to KNIME HubSwitch back to KNIME classic user interfaceSpace explorerOpen workflows from KNIME HubBuilding workflowsNode repositoryNode descriptionWorkflow descriptionKNIME AI AssistantWorkflow monitorNode monitorHelpManaging workflow states and logsNode reset and workflow executionKNIME log and node operationsConfiguring KNIME Analytics PlatformPreferencesSetting up knime.iniKNIME runtime optionsKNIME tablesData tableColumn typesSortingColumn renderingTable storageShortcutsGeneral actionsExecutionSelected node actionsWorkflow annotationsWorkflow editor modesWorkflow editor actionsComponent and metanode buildingZooming, panning and navigating inside the canvasPanel navigationOthersTroubleshootingDownloading or opening workflows from KNIME Community Hub is blocked by firewallIntroduction
  * Introduction
  * Workspaces
  * User interface
    * Entry page
    * Workflow editor & nodes
    * Connect to KNIME Hub
    * Switch back to KNIME classic user interface
    * Space explorer
    * Open workflows from KNIME Hub
    * Building workflows
    * Node repository
    * Node description
    * Workflow description
    * KNIME AI Assistant
    * Workflow monitor
    * Node monitor
    * Help
  * Managing workflow states and logs
    * Node reset and workflow execution
    * KNIME log and node operations
  * Configuring KNIME Analytics Platform
    * Preferences
    * Setting up knime.ini
    * KNIME runtime options
  * KNIME tables
    * Data table
    * Column types
    * Sorting
    * Column rendering
    * Table storage
  * Shortcuts
    * General actions
    * Execution
    * Selected node actions
    * Workflow annotations
    * Workflow editor modes
    * Workflow editor actions
    * Component and metanode building
    * Zooming, panning and navigating inside the canvas
    * Panel navigation
    * Others
  * Troubleshooting
    * Downloading or opening workflows from KNIME Community Hub is blocked by firewall


Download PDF
## Introduction 
This guide covers the basics of the KNIME Analytics Plaform usage, guiding you in the first steps with the platform but also providing more advanced information about the most important concepts together with indication on how to configure the platform.
![00 teaser](https://docs.knime.com/latest/analytics_platform_user_guide/img/00_teaser.png)
Figure 1. KNIME Analytics Platform
## Workspaces 
When you start KNIME Analytics Platform, the KNIME Analytics Platform launcher window appears and you are asked to define the KNIME workspace, as shown in Figure 2.
|  The KNIME workspace is a folder on the local computer to store KNIME workflows, node settings, and data produced by the workflow.   
---|---  
![02 launcher](https://docs.knime.com/latest/analytics_platform_user_guide/img/02_launcher.png)
Figure 2. KNIME Analytics Platform launcher
The workflows, components and data stored in the workspace are available through the space explorer in the side panel navigation.
You can switch the workspace in a later moment under _Menu_ , in the top right corner of the user interface, and select _Switch workspace_.
## User interface 
After selecting a workspace for the current project, click _Launch_. The KNIME Analytics Platform user interface - the KNIME Workbench - opens.
The active workflow of the KNIME Analytics Platform will be displayed after switching from an opened workflow. If you have open multiple workflows before you switch the perspective, only the active workflow and all loaded workflow tabs of the current KNIME Analytics Platform will be displayed in the KNIME Modern UI. For each workflow you will see a workflow tab after switching. After clicking the first tab (with the KNIME logo) you end up at the entry page.
![04 knime modern ui general layout](https://docs.knime.com/latest/analytics_platform_user_guide/img/04_knime_modern_ui_general_layout.png)
Figure 3. General user interface layout — application tabs, side panel, workflow editor and node monitor
![05 knime modern ui explanation](https://docs.knime.com/latest/analytics_platform_user_guide/img/05_knime_modern_ui_explanation.png)
Figure 4. User interface elements — workflow toolbar, node action bar, rename components and metanodes
In the next few sections we explain the functionality of these components of the user interface:
  * Entry page
  * Workflow editor & nodes
  * Connect to KNIME Hub
  * Space explorer
  * Node repository
  * Node description
  * Workflow description
  * Node monitor
  * Help menu


|  You can scale the interface by going to _Menu > Interface scale_.   
---|---  
### Entry page 
The entry page is displayed by clicking the _Home_ tab.
![03 knime modern ui entry page](https://docs.knime.com/latest/analytics_platform_user_guide/img/03_knime_modern_ui_entry_page.png)
Figure 5. Entry page to create or open workflows
Here you will find:
  * Recent, Local space, KNIME Community Hub. By default only the local workspace, and the link to connect to your personal KNIME Community Hub space are visible. To add a new mount point follow the instructions in the Connect to KNIME Hub section. Select:
    * _Recent_ to see your recently opened workflows and components
    * _Local space_ to navigate the existing workflows in your local system
    * _KNIME Community Hub_ (or one of the available mount points). Click _Sign in_ , provide your credentials and start navigating the available spaces.
  * Three example workflows to help you get started — You can dismiss the examples by clicking the button on the top right. To restore the examples click _Help > Restore examples on home tab_.
  * Create a new workflow by clicking the _+_ button


|  Since KNIME Analytics Platform version 5.2 you can also add a KNIME Server mount point.   
---|---  
### Workflow editor & nodes 
The workflow editor is where workflows are assembled. Workflows are made up of individual tasks, represented by nodes.
One way to create a new workflow is to go to the space explorer, click the three dots and select _Create workflow_ from the menu. Give the workflow a name and click _Create_.
|  Once you have a workflow open you can always create a new workflow by clicking the + in the tabs bar at the top of the user interface.   
---|---  
In the new empty workflow editor, create a workflow by dragging nodes from the node repository to the workflow editor, then connecting, configuring, and executing them.
#### Nodes 
In KNIME Analytics Platform, individual tasks are represented by nodes. Nodes can perform all sorts of tasks, including reading/writing files, transforming data, training models, creating visualizations, and so on.
##### Facts about nodes 
![05 node](https://docs.knime.com/latest/analytics_platform_user_guide/img/05_node.png)
Figure 6. A node in KNIME Analytics Platform
  * Each node is displayed as a colored box with input and output ports, as well as a status, as shown in Figure 6
  * The input port(s) hold the data that the node processes, and the output port(s) hold the resulting datasets of the operation
  * The data is transferred over a connection from the output port of one to the input port of another node.


|  For simplicity we refer to data when we refer to node input and output ports, but nodes can also have input and output ports that hold a model, a database query, or another type explained in Node Ports.   
---|---  
A node can be in different status as shown in the Figure 7.
![05 status](https://docs.knime.com/latest/analytics_platform_user_guide/img/05_status.png)
Figure 7. A node can exist in different status
##### Changing the status of a node 
The status of a node can be changed, either configuring, executing, or resetting it.
All these options can be found:
  * In the node action bar - click the different icons to configure, execute, cancel, reset and when available open the view.
![05 node action bar](https://docs.knime.com/latest/analytics_platform_user_guide/img/05_node_action_bar.png)
Figure 8. Action bar of a node
  * In the context menu of a node - open the context menu by right clicking a node.
![05 node context menu](https://docs.knime.com/latest/analytics_platform_user_guide/img/05_node_context_menu.png)
Figure 9. Context menu of a node


##### Identifying the node status 
The traffic light below each node shows the status of the node. When a node is configured, the traffic light changes from red to yellow, i.e. from "not configured" to "configured".
When a new node is first added to the workflow editor, its status is "not configured" - shown by the red traffic light below the node.
##### Configuring the node 
The node can be configured by adjusting the settings in its configuration dialog.
Open the configuration dialog of a node by either:
  * Double clicking the node
  * Clicking the _Configure_ button in the node action bar
  * Right clicking a node and selecting _Configure_ in the context menu
  * Or, selecting the node and pressing F6


##### Executing the node 
Some nodes have the status "configured" already when they are created. These nodes are executable without adjusting any of the default settings.
Execute a node by either:
  * Clicking the _Execute_ button in the node action bar
  * Right clicking the node and selecting _Execute_
  * Or, selecting the node and pressing F7


If execution is successful, the node status becomes "executed", which corresponds to a green traffic light. If the execution fails, an error sign will be shown on the traffic light, and the node settings and inputs will have to be adjusted as necessary.
##### Canceling execution of the node 
To cancel the execution of a node click the _Cancel_ button in the node action bar, or right click it and select _Cancel_ or select it and press F9.
##### Resetting the node 
To reset a node click the _Reset_ button in the node action bar, or right click it and select _Reset_ or select it and press F8.
|  Resetting a node also resets all of its subsequent nodes in the workflow. Now, the status of the node(s) turns from "executed" into "configured", the nodes' outputs are cleared.   
---|---  
##### Node ports 
A node may have multiple input ports and multiple output ports. A collection of interconnected nodes, using the input ports on the left and output ports on the right, constitutes a workflow. The input ports consume the data from the output ports of the predecessor nodes, and the output ports provide data to the successor nodes in the workflow.
Besides data tables, input and output ports can provide other types of inputs and outputs. For each type the pair of input and output port looks different, as shown in Figure 10.
An output port can only be connected to an input port of the same type - data to data, model to model, and so on.
Some input ports can be empty, like the data input port of the Decision Tree View node in Figure 10. This means that the input is optional, and the node can be executed without the input. The mandatory inputs, shown by filled input ports, have to be provided to execute the node.
![05 port types](https://docs.knime.com/latest/analytics_platform_user_guide/img/05_port_types.png)
Figure 10. Common port types
A tooltip gives a short explanation of the input and output ports. If the node is executed, the dimensions of the outcoming data are shown in its data output port. A more detailed explanation of the input and output ports is in the node description.
#### Adding nodes to the canvas 
Currently, there are three ways of adding nodes to your canvas to build your workflow:
  1. Drag and drop a node from the node repository,
  2. double-click on a node inside the node repository, or
  3. via the quick nodes adding panel. Double click on the canvas or drag and drop a node port (input or output) into the canvas to open the quick node adding panel. This panel contains up to 12 recommended nodes or you can search in the panel for the desired node, then click the desired node to add it to the canvas.


![06 knime modern ui quick nodes adding](https://docs.knime.com/latest/analytics_platform_user_guide/img/06_knime_modern_ui_quick_nodes_adding.png)
Figure 11. Quick nodes adding with recommended nodes
To use quick nodes adding you need to allow us to receive anonymous usage data. This is possible at the startup of the KNIME Analytics Platform or after switching to a new workspace by selecting _Yes_ in the “Help improve KNIME” dialog.
![07 help improve knime dialog](https://docs.knime.com/latest/analytics_platform_user_guide/img/07_help_improve_knime_dialog.png)
Figure 12. “Help improve KNIME” dialog
You can also activate it, via the _Open Preference_ button that is displayed in the quick nodes adding panel.
Click here to find out what is being transmitted. If you don’t want to do this anymore, you can deactivate it at any time in the KNIME Workflow Coach Preferences.
To open the preferences follow these steps:
  1. Click _Preferences_ in the top right corner of the user interface
  2. Go to _KNIME → Workflow Coach_
  3. Deactivate the setting _Node Recommendations by the Community_


![08 knime preferences workflow coach](https://docs.knime.com/latest/analytics_platform_user_guide/img/08_knime_preferences_workflow_coach.png)
Figure 13. Workflow Coach Preferences
Finally, if you are logged in to the Hub to use the KNIME AI Assistant (K-AI), you will be able to use the Build mode of the AI Assistant directly in the quick nodes adding panel. Log into the KNIME AI Assistant with your KNIME Hub account and open the quick nodes adding panel. Here click _Build with K-AI_ and you can add your prompt. K-AI will add nodes and relative comments to the canvas.
![08 kai quicknodes](https://docs.knime.com/latest/analytics_platform_user_guide/img/08_kai_quicknodes.png)
Figure 14. Build with K-AI at the quick nodes adding panel
You can go back to the quick nodes adding panel default mode by clicking the ![16](https://docs.knime.com/latest/analytics_platform_user_guide/img/icons/arrow-back.svg) icon.
#### How to select, move, copy, and replace nodes in a workflow 
Nodes can be moved into the workflow editor by dragging and dropping them. To copy nodes between workflows, select the chosen nodes, right click the selection, and select _Copy_ in the menu. In the destination workflow, right click the workflow editor, and select _Paste_ in the menu.
To select a node in the workflow editor, click it once, and it will be surrounded by a border. To select multiple nodes, draw a rectangle over the nodes with the mouse.
Replace a node by dragging a new node onto an existing node. Now the existing node will be covered with a colored box with an arrow and boxes inside as shown in Figure 15. Releasing the mouse replaces the node.
![05 replacing nodes](https://docs.knime.com/latest/analytics_platform_user_guide/img/05_replacing_nodes.png)
Figure 15. Replacing a node in a workflow
#### Comments and annotations 
You have two options in the workflow editor to document a workflow:
  * Node label - Add a comment to an individual node by double clicking the text field below the node and editing the text
![05 node label](https://docs.knime.com/latest/analytics_platform_user_guide/img/05_node_label.png)
Figure 16. Writing a node comment
  * Workflow annotation - Add a general comment to the workflow, right click the workflow editor and select _New workflow annotation_ in the menu. Now a text box will appear in the workflow editor.
|  To add a new annotation you can also change the mode to annotation by clicking the ![16](https://docs.knime.com/latest/analytics_platform_user_guide/img/icons/cursor.svg) icon in the top right corner of the user interface and select _Annotation mode_ , or press `T` to enter annotation mode.   
---|---  
![05 workflow annotation](https://docs.knime.com/latest/analytics_platform_user_guide/img/05_workflow_annotation.png)
Double click the workflow annotation to add text and format the text and change the color of the annotation outline. To change the format you can use the annotation bar or use the following syntax:
  * To create a heading, add number signs (`#`), followed by a space, in front of a word or phrase. The number of number signs you use should correspond to the heading level (`<h1>` to `<h6>`).
  * To create a bullet list, add a star sign (`*`) followed by a space.
  * To create a numbered list, add a number followed by a point (`1.`), followed by a space.
  * To make a text bold, italic, or underlined, select the text and press CTRL+b, CTRL+i, CTRL+u.
Finally you can click outside the annotation and click the annotation once again to move it around the canvas or to change its dimensions.


### Connect to KNIME Hub 
By default you can connect to your account on KNIME Community Hub from the _Home_ tab.
It is possible to add a new KNIME Hub instance by clicking _Preferences_ , in the top right corner of the user interface.
Go to _KNIME Explorer_ section and click _New…​_. In the window that opens select _KNIME Hub_ and add your Hub URL. Then click _Apply_.
Now the new KNIME Hub will show up in the _Home_ tab.
Sign in and select the space you want to work on. The content of the space and the related operations you can do on the items are visible in the space explorer.
### Switch back to KNIME classic user interface 
You can switch back to the classic KNIME Analytics Platform user interface under _Menu_ , in the top right corner of the user interface, and select _Switch to classic user interface_.
You can switch back to KNIME Modern UI at any time by pressing the button _Open KNIME Modern UI_ in the classic user interface, at the top right corner.
|  Really, really, _really_ important disclaimer Workflow elements such as connectors or annotations are visualized in a new way and may not look exactly like in the current KNIME Analytics Platform. Changes will therefore not look 100% the same.  
---|---  
### Space explorer 
The space explorer is where you can manage workflows, folders, components and files in a space, either local or remote on a KNIME Hub instance.
A space can be:
  * Your local workspace you selected at the start up of the KNIME Analytics Platform
  * One of your user’s spaces on KNIME Community Hub
  * One of your team’s spaces on KNIME Business Hub


You can switch to other spaces by:
  * Going to the _Home_ tab and selecting one of the available spaces. Here you can filter the space by clicking the ![16](https://docs.knime.com/latest/analytics_platform_user_guide/img/icons/lens.svg) icon:
![img filter space](https://docs.knime.com/latest/analytics_platform_user_guide/img/img-filter-space.png)
Figure 17. Filter the spaces
  * On the top of the space explorer you can sign in to any of the Hub or Server mount points and select a space. You will see the spaces grouped by owner when on KNIME Hub.
![img select space](https://docs.knime.com/latest/analytics_platform_user_guide/img/img-select-space.png)
Figure 18. Select a space to explore


|  If you have a workflow open right-click the workflow tab at the top and select _Reveal in space explorer_ to locate the workflow in the space explorer.   
---|---  
In the space explorer you can see:
  * Workflows
  * Folders
  * Data files
  * Components
  * Metanodes


Double click a workflow to open it in the workflow canvas and start adding nodes to the canvas from the node repository.
|  An overview on components and metanodes is available in the KNIME Components Guide.   
---|---  
Here you can click the three dots to select one of the following actions within the current space:
  * Create a new folder or a new workflow
  * Import a workflow
  * Add a file


![img space context menu](https://docs.knime.com/latest/analytics_platform_user_guide/img/img-space-context-menu.png)
Figure 19. Space context menu
You can also drop files to the canvas. KNIME will create the appropriate file reading node automatically and preconfigure it. Finally you can drop a component to the canvas to use the component in the current workflow.
Select an item from the current space and right click on it to access the item context menu.
  * If your in your local space, you will have the following options:
    * _Rename_
    * _Delete_
    * _Duplicate_
    * _Export_ (only available for workflows)
    * _Upload_ (if you are already connected to one of the available Hub mount points)
    * _Connect_ (to connect to the available Hub mount points without leaving the current view)
  * If you are navigating through a space in your KNIME Hub, you will have the following options:
    * _Rename_
    * _Delete_
    * _Duplicate_
    * _Download_
    * _Move to…​_
    * _Copy to…​_ (only available for workflows and components)
    * _Open in Hub.._ (only available for workflows and components)


### Open workflows from KNIME Hub 
Another possibility you have to open a workflow that exists on you KNIME Hub in the Analytics Platform is to drag and drop the workflow from the Hub in the browser.
To do so go to the Hub instance (e.g. your KNIME Business Hub instance or KNIME Community Hub) sign in with your account and navigate to the workflow you want to open.
Here you can drag and drop the workflow drag & drop element or the URL to the Analytics Platform and the workflow will open.
|  You can select a specific version of your workflow to view and interact with it in the Analytics Platform. If you do not select a specific version you will view and interact with the workflow in the current state.   
---|---  
### Building workflows 
When you create a new workflow, the canvas will be empty.
To build the workflow you will need to add nodes to it by dragging them from the node repository and connecting them. Alternatively you can drag an output port of a node to show the workflow coach which will suggest you the compatible nodes and directly connect them.
Once two nodes are added to the workflow editor, they can be connected by clicking the output port of the first node and release the mouse at the input port of the second node. Now, the nodes are connected. For some nodes you might have the ability to add specific ports. When hovering over these nodes you will see a `+` sign appearing. Click it to add a port. If the nodes supports different types of these dynamic ports a list will appear for you to scroll down to select the type of port you want to add.
You can also add a node between two nodes in a workflow. To do so drag the node from the node repository, and release it at its place in the workflow.
### Node repository 
Currently installed nodes are available in the node repository. You can add a node from the node repository into the workflow editor by drag and drop it into the workflow canvas, as explained in the section Building Workflows.
Search for a node by typing a search term in the search field on top of the node repository.
As shown in Figure 20, you can view your node repository as grid, list or tree view. To switch between the views click ![16](https://docs.knime.com/latest/analytics_platform_user_guide/img/icons/view-cards.svg) (grid view), ![16](https://docs.knime.com/latest/analytics_platform_user_guide/img/icons/list.svg) (list view), or ![16](https://docs.knime.com/latest/analytics_platform_user_guide/img/icons/unordered-list.svg) (tree view).
![08 node repository search](https://docs.knime.com/latest/analytics_platform_user_guide/img/08_node_repository_search.png)
Figure 20. Node repository with three view modes
### Node description 
You can access the node description, with information about the node function, the node configuration and the different ports available for the node in the following ways:
  * Select a node you added in the canvas, go to the side panel navigation and select the first option
  * Hover over a node in the node repository and click the info icon that appears. This will open the node description panel.


### Workflow description 
The description panel on the left of the KNIME Analytics Platform provides a description of the currently active workflow, or a selected component.
Click the pen icon to change the workflow description, add links to external resources and add tags.
Double click the workflow annotation to add text and format the text and change the color of the annotation outline. To change the format of the description you can use the formatting bar or use the following syntax:
  * To create a bullet list, add a star sign (`*`) followed by a space.
  * To create a numbered list, add a number followed by a point (`1.`), followed by a space.
  * To make a text bold, italic, or underlined, select the text and press CTRL+b, CTRL+i, CTRL+u.


|  To change a component description you need to first open the component. To do so, select the component, right-click and select _Component > Open component_ from the context menu.   
---|---  
### KNIME AI Assistant 
KNIME features an AI assistant designed to efficiently answer your queries about the KNIME platform and assist in constructing customized workflows, simplifying your data analysis tasks.
|  If you can not find the AI Assistant side panel, the AI Assistant might be deactivated by your administrator.   
---|---  
#### Usage 
To access the AI assistant, please log in to KNIME Hub.
![091 login to hub](https://docs.knime.com/latest/analytics_platform_user_guide/img/091_login_to_hub.png)
Figure 21. Login
If you have access to a KNIME Business Hub instance that is equipped with AI assistant support, you can select the specific instance to use via the AI Assistant preferences page and then log in via the AI assistant side panel.
![091 ai assistant preferences](https://docs.knime.com/latest/analytics_platform_user_guide/img/091_ai_assistant_preferences.png)
Figure 22. AI Assistant preferences page
To use the AI assistant, it is mandatory to first accept the terms outlined in the disclaimer. Please note that to deliver our services, KNIME shares data with OpenAI or Microsoft Azure. This includes all user queries, and for the **Build** mode, it includes the table specifications of selected nodes, such as column names and data types, but **not the data itself**.
The KNIME AI Assistant offers two modes:
  * A **Q &A** mode, and
  * A **Build** mode.


These can be selected using the toggle button located at the top of the side panel.
#### Q&A mode 
![091 example qa1](https://docs.knime.com/latest/analytics_platform_user_guide/img/091_example_qa1.png)
Figure 23. Q&A mode
In the Q&A mode, you can inquire about KNIME functionalities, including how to execute specific tasks, and receive informative answers.
These answers may feature recommendations for nodes effective in achieving the tasks at hand. If the suggested nodes are already installed, they can be directly dragged into the workflow. For nodes not yet installed, a link to the KNIME Hub is provided. You can then install these nodes via drag-and-drop from KNIME Hub.
By clicking the question mark located at the top of the answer, you will be provided with links to the sources that were used to generate the response.
Click the ![16](https://docs.knime.com/latest/analytics_platform_user_guide/img/icons/arrow-next-double.svg) icon to check _Additional Resources_ like workflows, forum posts, and KNIME Documentation.
|  You can leave a feedback if the answer was useful to you by hovering over the answer and clicking the thumbs up or thumbs down icons that appear.   
---|---  
![img-example-qa2\]](https://docs.knime.com/latest/analytics_platform_user_guide/img/091_example_qa2.png)
Figure 24. Q&A mode
#### Build mode 
The Build mode is engineered to extend workflows in response to a query. The set of nodes available to the Build mode is currently limited, but many more nodes will be added in the future. It is important to note that in Build mode, workflows cannot currently be initiated from scratch. You are required to select pre-existing nodes that already supply data, and from there, the workflow is dynamically expanded according to your query.
![img-example-build-workflow\]](https://docs.knime.com/latest/analytics_platform_user_guide/img/091_example_build_workflow.png)
Figure 25. Build mode
You can also use the Build mode of the KNIME AI Assistant directly in the workflow canvas. To do so open the quick nodes adding panel by dragging and dropping into the canvas the port of the node you want to use as starting point to extend your workflow. Select _Build with K-AI_ and insert your prompt.
#### Deactivation 
If you want to deactivate the AI assistant and hide it from the side panel, follow these steps:
  * **Using _Preferences_ :**
    1. Go to _Preferences > KNIME Modern UI > AI Assistant_.
    2. Uncheck the option _Enable the KNIME AI Assistant_.
  * **Using the`knime.ini` file:**
    1. Open the `knime.ini` file.
    2. Add the following entry:
```
-Dorg.knime.ui.feature.ai_assistant=false
```



|  The setting in the `knime.ini` file will always override the setting in the _Preferences_.   
---|---  
**Additional Information:**
The AI Assistant might also be deactivated via customization profiles. However, the setting in the `knime.ini` file will always take precedence over the customization profile.
The AI Assistant status setting will have the following prioritization:
  1. `knime.ini` file
  2. Customization profile
  3. _Preferences_


### Workflow monitor 
Access the workflow monitor tab from the side panel navigation of the user interface, shown in Figure 26. Here, you can find errors and warnings that might arise from the execution of your workflow.
![img workflow monitor](https://docs.knime.com/latest/analytics_platform_user_guide/img/img-workflow-monitor.png)
Figure 26. Workflow monitor
When a node error or a node warning occurs you can click the ![16](https://docs.knime.com/latest/analytics_platform_user_guide/img/icons/arrow-right.svg) icon to select the node that is causing the issue in the workflow.
|  If the node is in a component or a metanode this will automatically navigate to the level where the node that is causing the issue is present.   
---|---  
### Node monitor 
The node monitor tab is located on the bottom part of the user interface shown in Figure 27. It is especially useful to inspect intermediate output tables in the workflow.
![092 node monitor](https://docs.knime.com/latest/analytics_platform_user_guide/img/092_node_monitor.png)
Figure 27. Node monitor
Here you can choose to show the flow variables or a preview of the output data at any port of a selected node in the active workflow.
Switch to _Statistics_ in order to see some basic statistics of the data.
You can also detach the table or the statistics view and open it in a new window. To do so click the ![16](https://docs.knime.com/latest/analytics_platform_user_guide/img/icons/open-in-new-window.svg) icon in the respective view (_Table_ or _Statistics_). This allows you to open multiple table or statistics for multiple nodes in your workflow.
Read more about the data table shown in the node monitor in the KNIME tables section.
### Help 
By clicking the _Help_ button in the top right corner of the user interface you can see multiple useful links like:
  * Keyboard shortcuts to speed up your workflow building process without relying on a computer mouse.
  * Some learning resources like cheat sheets, getting started guide and documentation
  * The KNIME Forum to ask the community about workflow building, tips and tricks
  * About page for KNIME Analytics Platform - also with currently installed version and access to _Installation Details_
  * Additional credits about open source software components


## Managing workflow states and logs 
### Node reset and workflow execution 
When a node is reset, the node status changes from "executed" to "configured," and the output of the node is no longer available. When saving a workflow in an executed state, the data used in the workflow is saved as well. This means that larger datasets result in larger file sizes. Therefore, it is recommended to reset workflows before saving them if the dataset can be accessed without any restrictions. A reset workflow only saves the node configurations, not the results.
### KNIME log and node operations 
Resetting a node does not undo the operation executed before. All operations done during creation, configuration, and execution of a workflow are reported in the `knime.log` file.
Click _Menu_ > _Show KNIME log in File Explorer_ in the top right corner of the user interface to see the `knime.log` location folder. The `knime.log` file has a limited size, and after reaching it the rows will be overwritten from the top.
## Configuring KNIME Analytics Platform 
### Preferences 
Preferences can be open by clicking _Preferences_ in the top right corner of the Analytics Platform.
Here, a list of subcategories is displayed in the dialog that opens. Each category contains a separate dialog for specific settings like database drivers, available update sites, and appearance.
![13 knime preferences modern ui](https://docs.knime.com/latest/analytics_platform_user_guide/img/13_knime_preferences_modern_ui.png)
Figure 28. Open KNIME Preferences window from KNIME Analytics Platform
#### Network connections 
Selecting _General > Network Connections_ in the list of subcategories, allows you to define the networking and proxy setup of KNIME Analytics Platform.
![13 proxy preferences](https://docs.knime.com/latest/analytics_platform_user_guide/img/13_proxy_preferences.png)
Figure 29. The Network Connections preferences page
As show in Figure 29, the _Active Provider_ can be set to three options:
  * The _Direct_ provider bypasses all proxies.
  * The _Manual_ provider uses the proxy configuration you see on the page.
  * The _Native_ provider checks for proxy settings on the OS and loads them into preferences.


Manual proxy entries are distinguished by protocol, being HTTP, HTTPS, or SOCKS. For example, the HTTPS entry only affects requests to hosts which use that protocol, such as `https://www.knime.com`.
You can use the Proxy Diagnostics workflow on your local Analytics Platform to investigate the settings on your machine and potential connection issues. To do so, download the workflow, execute it locally, and open the view of the Proxy Diagnostics Viewer.
##### Native proxies 
Native proxies come from OS-specific, static or dynamic sources. A static proxy configuration is a hard-coded setting including the proxy host and its corresponding port. Dynamic proxy configuration is based on proxy auto-config (PAC) scripts. Proxies from this source will be labeled as _Dynamic_ in the preferences.
Below you find an overview of which sources are supported in the different OSs.
Table 1. Native proxies support per OS | Static source | Dynamic source  
---|---|---  
**Windows** | Proxies from the Windows system settings at _Network & Internet > Proxy > Manual proxy setup > Use a proxy server_. | PAC proxies defined at _Network & Internet > Proxy > Automatic proxy setup > Use setup script_. Note that this source does not work for all services. See below for more information.  
**macOS** | Proxies are coming from the macOS system proxy settings. | Not supported.  
**Linux** | First, the environment variables `http_proxy`, `https_proxy`, `no_proxy` are checked for proxies, then GNOME system settings at _Network > Network Proxy_ are. | Not supported.  
As mentioned above, the native proxy support on Linux is limited to GNOME systems. Additionally, loading proxies from GNOME settings requires adding this entry to the knime.ini file.
`-Dorg.eclipse.core.net.enableGnome=true`
|  Dynamic proxies work well for core functionality of the KNIME Analytics Platform, such as fetching updates, reloading update sites, or installing extensions. However, they are not supported for node execution in general, with some exceptions. For example, the _KNIME REST Client Extension_ does support dynamic proxy sources.   
---|---  
##### Proxy authentication 
KNIME supports basic authentication at proxies, i.e. using a username and a password. You can set the proxy credentials on the same preferences page by enabling the _Requires Authentication_ checkbox. The credentials are stored in the Eclipse secure storage.
If credentials are not found, the corresponding service in the KNIME Analytics Platform will receive the HTTP response 407.
To resolve missing proxy credentials, follow these steps:
  1. First, check that you correctly entered the credentials for the relevant protocol entry.
  2. If you are still seeing node or log messages like `Unable to tunnel through proxy. Proxy returns "HTTP/1.1 407 Proxy Authentication Required"`, it is likely that you are missing a property in the `knime.ini` file. See this FAQ entry for more information.


##### Proxy exclusion 
On the preferences page, you can also exclude individual hosts from using the proxy. This makes sense for local or internal hosts, for example `localhost` or `127.0.0.1`. Next to exact matching hostnames, using wildcards `*` allows you to exclude a range of hosts. It is important not to include the protocol of the URL, for example `\https://`.
Be aware that the exclusion pattern is matched to every HTTP-redirect. For example, excluding only the host `knime.com` will result in your request still using a proxy, since `knime.com` redirects to `www.knime.com` which was not excluded. For these cases, you can use the wildcard patterns, such as the pattern `*knime.com`.
#### KNIME 
Selecting _KNIME_ in the list of subcategories, allows you to define the log file log level. By default it is set to _DEBUG_. This log level helps developers to find reasons for any unexpected behavior.
Directly below, you can define the maximum number of threads for all nodes. Separate branches of the workflow are distributed to several threads to optimize the overall execution time. By default the number of threads is set to twice the number of CPUs on the running machine.
In the same dialog, you can also define the folder for temporary files.
Check the last option _Yes, help improve KNIME._ to agree to sending us anonymous usage data. This agreement activates the node recommendations in the quick node adding panel.
##### KNIME Modern UI 
In the KNIME Modern UI category you can select:
  * Which nodes to include in the node repository and node recommendations
  * Which action is associated with the mouse wheel.
  * If you want to be asked to close the open projects when switching between Modern and Classic UI
  * If you want to confirm node configuration changes in order for them to be applied
  * How you want the node configuration dialogs to show up in the UI:
    * _Open in new window_ : The default option is to open them into a new window
    * _Embedded inside application (experimental)_ : With the release of KNIME Analytics Platform 5.4 we have introduced the possibility to see the node configuration dialogs embedded inside the application. Please notice that this option is still experimental.
  * Under _AI Assistant_ you can also, which KNIME Hub the AI assistant connects to. Here you can also disable the KNIME AI Assistant. The KNIME AI Assistant is enabled by default.


##### KNIME classic user interface 
The _KNIME_ category, contains a subcategory _KNIME classic user interface_. In this dialog, you can define the console view log level. By default it is set to "WARN", because more detailed information is only useful for diagnosis purposes.
Further below, you can select which confirmation dialogs are shown when using KNIME Analytics Platform. Choose from the following:
  * Confirmation after resetting a node
  * Deleting a node or connection
  * Replacing a connection
  * Saving and executing workflow
  * Loading workflows created with a nightly build


In the same dialog, you can define what happens if an operation requires executing the previous nodes in the workflow. You have these three options:
  * Execute the nodes automatically
  * Always reject the node execution
  * Show a dialog to execute or not


The following options allow you to define whether workflows should be saved automatically and after what time interval, also whether linked components and metanodes should be automatically updated. You can also define visual properties such as the border width of workflow annotations.
##### Table backend 
Starting with KNIME Analytics Platform version 4.3 a new Columnar Backend is introduced, in order to optimize the use of main memory in KNIME Analytics Platform, where cell elements in a table are represented by Java objects by reviewing the underlying data representation.
The KNIME Columnar Table Backend extension addresses these issues by using a different underlying data layer (backed by Apache Arrow), which is based on a columnar representation.
The type of table backend used can be defined:
  * At the workflow level. Open a workflow and select the Description tab from the side panel navigation. Click the ![16](https://docs.knime.com/latest/analytics_platform_user_guide/img/icons/cog.svg) icon in the top right corner of the description panel. A workflow configuration dialog will open. Here, in the tab _Table Backend_ you can select the desired backend for this specific workflow from the menu.
![img workflow config](https://docs.knime.com/latest/analytics_platform_user_guide/img/img-workflow-config.png)
Figure 30. Configure a workflow to use Columnar Backend
  * As default for all new workflows created. Open the _KNIME Preferences_ and select _Table Backend_ under KNIME in the left pane of the preferences window. Here you can select _Columnar Backend_ as _Table backend for new workflows_ , as shown in Figure 31.
![13 table backend preferences](https://docs.knime.com/latest/analytics_platform_user_guide/img/13_table_backend_preferences.png)
Figure 31. The Table Backend preferences page


The parameters relative to memory usage of the Columnar Backend can also be configured. Go to _File_ → _Preferences_ and select _Table Backend_ → _Columnar Backend_ under KNIME in the left pane of the preferences window, as shown in Figure 32.
![13 columnar storage preferences](https://docs.knime.com/latest/analytics_platform_user_guide/img/13_columnar_storage_preferences.png)
Figure 32. The Columnar Backend preferences page
Note that the caches of the Columnar Backend that reside in the off-heap memory region require an amount of memory in addition to whatever memory you have allotted to the heap space of your KNIME’s Java Virtual Machine via the `-Xmx` parameter in the `knime.ini`. When altering the sizes of these cache via the preferences page, make sure not to exceed your system’s physical memory size as otherwise you might encounter system instability or even crashes.
|  For a more detailed explanation of the Columnar Backend technical background please refer to this post on KNIME Blog.   
---|---  
**High memory usage on Linux:** On some Linux systems KNIME Analytics Platform can allocate more system memory than expected when using the Columnar Backend. This is caused by an unfavorable interaction between the JVM and the `glibc` native memory allocator.
To work around this issue, you can reduce the number of allowed malloc areas by running KNIME Anayltics Patform with the environment variable `MALLOC_ARENA_MAX` set to `1`.
### Setting up knime.ini 
When installing KNIME Analytics Platform, configuration options are set to their defaults. The configuration options, i.e. options used by KNIME Analytics Platform, range from memory settings to system properties required by some extensions.
You can change the default settings in the `knime.ini` file. The `knime.ini` file is located in the installation folder of KNIME Analytics Platform.
|  To locate the `knime.ini` file on macOS, open Finder and navigate to the installed Applications. Next, right click the KNIME application, select _Show Package Contents_ in the menu, and navigate to _Contents_ , and open _Eclipse_.  
---|---  
Edit the `knime.ini` file with any plaintext editor, such as Notepad (Windows), TextEdit (macOS) or gedit (Linux).
The entry `-Xmx1024m` in the `knime.ini` file specifies how much memory KNIME Analytics Platform is allowed to use. The setting for this value will depend on how much memory is available in the running machine. We recommend setting it to approximately one half of the available memory, but this value can be modified and personalized. For example, if the computer has 16GB of memory, the entry might be set to `-Xmx8G`.
Besides the memory available, you can define many other settings in the `knime.ini` file. Find an overview of some of the most common settings in Table 2 or in this complete list of the configuration options.
Table 2. Common configuration settings in knime.ini file **Setting** | **Explanation**  
---|---  
`-Xmx`
  * default value: `1024m`
  * example: `-Xmx16G`

| Sets the maximum amount of memory available for KNIME Analytics Platform.  
`-Dknime.compress.io`
  * default value: `SNAPPY`
  * possible values: `[SNAPPY|GZIP|NONE]`
  * example: `-Dknime.compress.io=SNAPPY`

| Determines which compression algorithm (if any) to use when writing temporary tables to disk.  
`-Dorg.knime.container.cellsinmemory`
  * default value: 5,000
  * possible values: any value between 0 and 2,147,483,647
  * example: `-Dorg.knime.container.cellsinmemory=100,000`

| This setting defines the size of a "small table". Small tables are attempted to be kept in memory, independent of the Table Caching strategy. By increasing the size of a small table, the number of swaps to the disk can be limited, which comes at the cost of reducing memory space available for other operations.  
`-Dknime.layout_editor.browser`
  * default value version 4.7.2: swt
  * possible values: `[cef|swt]`
  * example: -Dknime.layout_editor.browser=cef

| This setting defines which browser should be used to display the layout editor.  
`-Dknime.table.cache`
  * default value: `LRU`
  * possible values: `[LRU|SMALL]`
  * example: `-Dknime.table.cache=SMALL`

| Determines whether to attempt to cache large tables (i.e., tables that are not considered to be "small"; see setting `-Dorg.knime.container.cellsinmemory`) in memory. If set to `LRU`, large tables are evicted from memory in least-recently used (LRU) order or when memory becomes scarce. If set to `SMALL`, large tables are always flushed to disk.  
`-Dknime.url.timeout`
  * default value: 1,000 ms
  * example: `-Dknime.url.timeout=100`

| This property is now discouraged, use the properties `knime.url.connectTimeout` and `knime.url.readTimeout` for setting separate URL timeouts instead. This property sets both timeouts to the same value.  
`-Dknime.url.connectTimeout`
  * default value: 5,000 ms
  * example: `-Dknime.url.connectTimeout=8000`

| When trying to connect to a web host via a URL, this value defines a timeout for the request. Increase the value if you see error messages from nodes or in the KNIME logs regarding connection timeout issues.  
`-Dknime.url.readTimeout`
  * default value: 20,000 ms
  * example: `-Dknime.url.readTimeout=32000`

| When trying to read data from a URL, this value defines a timeout for the request. Increase the value if a reader node fails. A too high timeout value may lead to slow websites blocking dialogs in KNIME Analytics Platform.  
`-Dchromium.block_all_external_requests`
  * default value: false
  * example: `-Dchromium.block_all_external_requests=true`

| This configuration setting when set to `true` blocks all the external requests made by Chromium Embedded Framework.  
`-Dknime.python.cacerts`
  * default value: ENV
  * example: `-Dknime.python.cacerts=ENV`

| This configuration setting controls which certificate authorities Python processes trust. If set to `ENV`, the Python process trusts the certificate authorities that are set in the Python environment. If set to `AP`, the Python process trusts the certificate authorities that the Analytics Platform trusts.  
### KNIME runtime options 
KNIME’s runtime behavior can be configured in various ways by passing options on the command line during startup. Since KNIME is based on Eclipse, all Eclipse runtime options also apply to KNIME.
KNIME also adds additional options, which are described below.
#### Command line arguments 
Listed below are the command line arguments processed by KNIME. They can either be specified permanently in the `knime.ini` in the root of the KNIME installation, or be passed to the KNIME executable. Please note that command line arguments must be specified _before_ the system properties (see below) i.e. before the `-vmargs` parameter. Note that headless KNIME applications, such as the batch executor, offer quite a few command line arguments. They are not described here but are printed if you call the application without any arguments. 

`-checkForUpdates`
     If this arguments is used, KNIME automatically checks for updates during startup. If new versions of installed features are found, the user will be prompted to install them. A restart is required after updates have been installed.  
---  
#### Java system properties 
Listed below are the Java system properties with which KNIME’s behavior can be changed. They can either be specified permanently in the `knime.ini` in the root of the KNIME installation, or be passed to the KNIME executable. Please note that system properties must be specified _after_ the `-vmargs` parameter. The required format is `-DpropName=propValue`.
#### General properties  

`org.knime.core.maxThreads=_<number>_`
     Sets the maximum number of threads that KNIME is using for executing nodes. By default this number is 1.5 times the number of cores. This property overrides the value from the KNIME preferences page.  
--- 

`knime.tmpdir=_<directory>_`
     Sets the default directory for temporary files KNIME files (such as data files). This property overrides the value from the preferences pages and is by default the same as the `java.io.tmpdir` . 

`knime.synchronous.io=_(true|false)_`
     Can be used to enforce the sequential processing of rows for KNIME tables. By default, each table container processes its rows asynchronously in a number of (potentially re-used) threads. The default value is `false`. Setting this field to `true` will instruct KNIME to always handle rows sequentially and synchronously, which in some cases may be slower. 

`knime.async.io.cachesize=_<number>_`
     Sets the batch size for non-sequential and asynchronous handling of rows (see knime.synchronous.io). It specifies the amount of data rows that are handled by a single container thread. The larger the buffer, the smaller the synchronization overhead but the larger the memory requirements. This property has no effect if rows are handled sequentially. The default value is 10. 

`knime.domain.valuecount=_<number>_`
     The number of nominal values kept in the domain when adding rows to a table. This is only the default and may be overruled by individual node implementations.If no value is specified a default of 60 will be used. 

`org.knime.container.threads.total=_<number>_`
     Sets the maximum number of threads that can be used to write KNIME native output tables. By default this number equals the number of processors available to the JVM. _Note:_ This value has to be greater than 0. 

`org.knime.container.threads.instance=_<number>_`
     Sets the maximum number of threads that can be used to write a _single_ KNIME native output table. By default this number equals the number of processors available to the JVM. _Note:_ This value has to be greater than 0 and cannot be larger than `org.knime.container.threads.total`. 

`knime.discourage.gc=_(true|false)_`
     If set to true, discourages KNIME from triggering a full stop-the-world garbage collection. Note that (a) individual nodes are allowed to disregard this setting and (b) the garbage collector may independently decide that a full stop-the-world garbage collection is warranted. Set to true by default. 

`org.knime.container.minspace.temp=_<number>_`
     Java property to specify the minimum free disc space in MB that needs to be available. If less is available, no further table files & blobs will be created (resulting in an exception). 

`knime.columnar.chunksize=__<number>__`
     The columnar table backend horizontally divides tables into batches and vertically divides these batches into column chunks. This property controls the initial size of these chunks and thereby the number of rows per batch. A chunk is the smallest unit that must be materialized to access a single value. Changing this value can therefore impact memory footprint and overall performance. Do not change this value unless you have good reasons. The default value is 28,000. 

`knime.columnar.reservedmemorymb=_<number>_`
     The columnar table backend caches table data off-heap. To this end, it requires memory in addition to the JVM’s heap memory, whose size is controlled via the -Xmx parameter. If no explicit cache sizes are set in the preferences, the default memory available for caching is computed as follows: Total physical memory minus reserved memory minus 1.25 times heap memory. The reserved memory size in this equation (in MB) can be configured via this property. The default is 4,096. 

`knime.columnar.verbose=_(true|false)_`
     Setting this property to true activates verbose debug logging in the columnar table backend. - 

`knime.disable.rowid.duplicatecheck=_(true|false)_`
     Enables/disables row ID duplicate checks on tables. Tables in KNIME are supposed to have unique IDs, whereby the uniqueness is asserted using a duplicate checker. This property will disable this check. **Warning:** This property should not be changed by the user. 

`knime.disable.vmfilelock=_(true|false)_`
     Enables/disables workflow locks. As of KNIME 2.4 workflows will be locked when opened; this property will disable the locking (allowing multiple instances to have the same workflow open). **Warning:** This property should not be changed by the user. 

`knime.database.timeout=_<number>_`
     Sets the timeout in seconds trying to establish a connection to a database. The default value is 15 seconds. 

`knime.database.fetchsize=_<number>_`
     Sets the fetch size for retrieving data from a database. The default value depends on the used JDBC driver. 

`knime.database.batch_write_size=_<number>_`
     Sets the batch write size for writing data rows into a database. The default value is 1, that is one row at a time. 

`knime.database.enable.concurrency=_(true|false)_`
     Used to switch on/off the database connection access (applies only for the same database connection). Default is true, that is all database accesses are synchronized based on single connection; false means off, that is, the access is not synchronized and may lead to database errors. 

`knime.logfile.maxsize=_<number>_[mk]`
     Allows one to change the maximum log file size (default is 10 MB). Values must be integer, possibly succeeded by "m" or "k" to denote that the given value is in mega or kilo byte. 

`knime.settings.passwords.forbidden=_(true|false)_`
     If _true_ , nodes using passwords as part of their configuration (e.g. DB connection or SendEmail) will not store the password as part of the workflow on disc. Instead a null value is stored, which will cause the node’s configuration to be incorrect (but valid) after the workflow is restored from disc. Default is _false_. 

`knime.repository.non-instant-search=_(true|false)_`
     Allows to disable the live update in the node repository search. - 

`knime.macosx.dialogworkaround=_(true|false)_`
     Allows to disable the workaround for freezes when opening node dialogs under macOS. - 

`knime.data.bitvector.maxDisplayBits=_<number>_`
     Sets the maximum number of bits that are display in string representations of bit vectors. - 

`knime.xml.disable_external_entities=_(true|false)_`
     If set to true, all nodes that parse XML files will not read external entities defined via a DTD. This is usually only useful when running as an executor on the server and you want prevent XXE attacks.  
#### Plug-in dependent properties 
These properties only affect some plug-ins and are only applicable if they are installed. 

`org.knime.cmlminblobsize=_<number>_[mMkK]`
     Allows to change the minimum size in bytes (or kilobyte or megabytes) a CML molecule must have before it is stored in a blob cell. Otherwise it is stored inline. The latter is a bit faster but needs more memory. The default is 8kB.  
--- 

`org.knime.ctabminblobsize=_<number>_[mMkK]`
     Allows to change the minimum size in bytes (or kilobyte or megabytes) a Ctab molecule must have before it is stored in a blob cell. Otherwise it is stored inline. The latter is a bit faster but needs more memory. The default is 8kB. 

`org.knime.mol2minblobsize=_<number>_[mMkK]`
     Allows to change the minimum size in bytes (or kilobyte or megabytes) a Mol2 molecule must have before it is stored in a blob cell. Otherwise it is stored inline. The latter is a bit faster but needs more memory. The default is 8kB. 

`org.knime.molminblobsize=_<number>_[mMkK]`
     Allows to change the minimum size in bytes (or kilobyte or megabytes) a Mol molecule must have before it is stored in a blob cell. Otherwise it is stored inline. The latter is a bit faster but needs more memory. The default is 8kB. 

`org.knime.rxnminblobsize=_<number>_[mMkK]`
     Allows to change the minimum size in bytes (or kilobyte or megabytes) a Rxn molecule must have before it is stored in a blob cell. Otherwise it is stored inline. The latter is a bit faster but needs more memory. The default is 8kB. 

`org.knime.sdfminblobsize=_<number>_[mMkK]`
     Allows to change the minimum size in bytes (or kilobyte or megabytes) a SDF molecule must have before it is stored in a blob cell. Otherwise it is stored inline. The latter is a bit faster but needs more memory. The default is 8kB.  
## KNIME tables 
### Data table 
Very common input and output ports of nodes are data input ports and data output ports, which correspond to the black triangles in Figure 33.
![14 ports](https://docs.knime.com/latest/analytics_platform_user_guide/img/14_ports.svg)
Figure 33. Data input and output port
A data table is organized by columns and rows, and it contains a number of equal-length rows. Elements in each column must have the same data type.
The data table shown in Figure 34 is produced by a CSV Reader node, which is one of the many nodes with a black triangle output port for data output. To open the table, click the node. Execute the node if it is not yet execute. The table will show up in the node monitor.
The output table has row numbers, unique RowIDs and column headers. The RowIDs are automatically created by the reader node, but they can also be defined manually. The RowIDs and the column headers can therefore be used to identify each data cell in the table. Missing values in the data are shown by a red question mark in a circle.
At the top of the node monitor you can select which output port you want to view via the tabs and the flow variable tab, which shows the available flow variables in the node output and their current values. Next row will indicate the table dimensions, meaning how many rows and how many columns there are in the table at that specific output port. Here you can also use the toggle to switch to _Statistics_. This tab shows the meta information of the table, like the columns names, columns types and some other statistics data.
![14 example table](https://docs.knime.com/latest/analytics_platform_user_guide/img/14_example_table.png)
Figure 34. Data output in KNIME Analytics Platform
### Column types 
The basic data types in KNIME Analytics Platform are `Integer`, `Double`, and `String`, along with other supported data types such as `Long`, `Boolean` value, `JSON`, `URI`, `Document`, `Date&Time`, `Bit vector`, `Image`, and `Blob`. KNIME Analytics Platform also supports customized data types, for example, a representation of a molecule.
Switch to the _Statistics_ view in an output table, to see the data types of the columns in the data table, as shown in Figure 35. For numerical values, only the range of the values in the data is shown. For string values, the different values appearing in the data are shown.
![14 data types](https://docs.knime.com/latest/analytics_platform_user_guide/img/14_data_types.png)
Figure 35. Data types and data domain in "Spec" tab
The reader nodes in KNIME Analytics Platform assign a data type to each column based on their interpretation of the content. If the correct data type of a column is not recognized by the reader node, the data type can be corrected afterwards. There are nodes available to convert data types. For example: String to Number, Number to String, Double to Int, String to Date&Time, String to JSON, and String to URI.
Many of the special data types are recognized as `String` by the reader nodes. To convert these `String` columns to their correct data types, use the Column Type Auto Cast node.
When you use the File Reader node to read a file you can convert the column types directly via the node configuration dialog. To do so go to the Transformation tab in the configuration dialog and change the type of the desired column, as shown in Figure 36.
![14 file reader dialog](https://docs.knime.com/latest/analytics_platform_user_guide/img/14_file_reader_dialog.png)
Figure 36. Change column type in File Reader node
### Sorting 
Rows in the table view output can be sorted by values in one column by clicking the up (ascending) and down (descending) arrow that appears hovering over the column name in the header. Note that this sorting only affects the current output view and has no effect on the node output.
To sort rows in an output table permanently, use the Sorter node. Use the Column Resorter node to reorder columns.
### Column rendering 
In a table view output, you can also change the way in which numeric values are displayed in a data table. For example, it is possible to display numeric values as percentages, with full precision, or replace digits by a grey scale or bars. To see these and other rendering options for a column, click the carat icon in the column header, and select the desired available renderer, as shown in Figure 37. Note that these changes are temporary and have no effect on the node output.
![14 data renderers](https://docs.knime.com/latest/analytics_platform_user_guide/img/14_data_renderers.png)
Figure 37. Rendering data in table view
If you hover over table cells containing data of type JSON, XML, or images (`svg` or `png`) and click the ![16](https://docs.knime.com/latest/analytics_platform_user_guide/img/icons/data_value_view.svg) a content preview will be shown, rendered in a readable format.
![img cell renderers](https://docs.knime.com/latest/analytics_platform_user_guide/img/img-cell-renderers.png)
Figure 38. Rendering data in table cells for JSON, XML and images
### Table storage 
When executed, many KNIME nodes generate and provide access to tabular data at their output ports. These tables might be small or large and, therefore, might fit into the main memory of the executing machine or not. Several options are available for configuring which tables to hold in memory as well as when and how to write tables to disk. These options are outlined in this section.
#### In-memory caching 
KNIME Analytics Platform differentiates between small and large tables. Tables are considered to be small (large) when they are composed of up to (more than) 5000 cells. This threshold of 5000 cells can be adjusted via the `-Dorg.knime.container.cellsinmemory` parameter in the `knime.ini` file. KNIME Analytics Platform always attempts to hold small tables in memory, flushing them to disk only when memory becomes scarce.
In addition, KNIME Analytics Platform attempts to keep recently used large tables in memory while sufficient memory is available. However, it writes these tables asynchronously to disk in the background, such that they can be dropped from memory when they have not been accessed for some time or when memory becomes scarce. You can configure the memory consumption of a specific node to never attempt to hold its tables in memory and, instead, write them to disk on execution. This is helpful if you know that a node will generate a table that cannot be held in memory or if you want to reduce the memory footprint of a node.
![memory policy lru](https://docs.knime.com/latest/analytics_platform_user_guide/img/memory-policy-lru.png)
Figure 39. Configuring a node’s memory policy
Alternatively, by putting the line `-Dknime.table.cache=SMALL` into the `knime.ini` file, KNIME Analytics Platform can be globally configured to use a less memory-consuming, albeit much slower caching strategy. This strategy only ever keeps small tables in memory.
#### Disk storage 
KNIME Analytics Platform compresses tables written to disk to reduce the amount of occupied disk space. By default, KNIME Analytics Platform uses the Snappy compression algorithm to compress its tables. However, you can configure KNIME Analytics Platform to use GZIP compression or no compression scheme at all via the `-Dknime.compress.io` parameter in the `knime.ini` file.
#### Columnar Backend 
Starting with KNIME Analytics Platform version 4.3 a new Columnar Backend is introduced. This extension addresses these issues by using a different underlying data layer (backed by Apache Arrow), which is based on a columnar representation.
For information on how to set up this type of backend please refer to the Table backend section.
## Shortcuts 
Shortcuts in KNIME Analytics Platform allow you to speed up your workflow building process. Navigate to the _Help_ button in the top right corner of the user interface. Select _Show keyboard shortcuts_. In the shortcuts window, the shortcut name is displayed on the left and the respective key sequence on the right. You can filter shortcuts according to their name and key.
|  The listed shortcuts are available for the KNIME Modern UI. They cannot be changed at the moment. Eclipse preferences have no impact on them.  
---|---  
### General actions 
Table 3. The supported shortcuts Action | Mac | Windows & Linux  
---|---|---  
Close workflow |  ⌘ W |  Ctrl + W  
Create workflow |  ⌘ N |  Ctrl + N  
Switch to next opened workflow |  ⌘ ⇥ |  Ctrl + Tab  
Switch to previous opened workflow |  ⌘ ⇧ ⇥ |  Ctrl + Shift + Tab  
Save |  ⌘ S |  Ctrl + S  
Undo |  ⌘ Z |  Ctrl + Z  
Redo |  ⌘ ⇧ Z |  Ctrl + Shift + Z  
Delete |  ⌫ ⌦ |  Delete  
Copy |  ⌘ C |  Ctrl + C  
Cut |  ⌘ X |  Ctrl + X  
Paste |  ⌘ V |  Ctrl + V  
Export |  ⌘ E |  Ctrl + E  
Select all objects |  ⌘ A |  Ctrl + A  
Deselect all objects |  ⌘ ⇧ A |  Ctrl + Shift + A  
Copy selected table cells |  ⌘ C |  Ctrl + C  
Activate the filter input field |  ⌘ ⇧ F |  Ctrl + Shift + F  
Copy selected table cells and corresponding header |  ⌘ ⇧ C |  Ctrl + Shift + C  
Close any dialog unsaved |  Esc |  Esc  
### Execution 
Table 4. The supported shortcuts for execute nodes, reset and cancel node execution Action | Mac | Windows & Linux  
---|---|---  
Configure |  F6 |  F6  
Configure flow variables |  ⇧ F6 |  Shift + F6  
Execute all |  ⇧ F7 |  Shift + F7  
Cancel all |  ⇧ F9 |  Shift + F9  
Reset all |  ⇧ F8 |  Shift + F8  
Execute |  F7 |  F7  
Open view |  F10 |  F10  
Cancel |  F9 |  F9  
Reset |  F8 |  F8  
Resume loop* |  ⌘ ⌥ F8 |  Ctrl + Alt + F8  
Pause loop* |  ⌘ ⌥ F7 |  Ctrl + Alt + F7  
Step loop* |  ⌘ ⌥ F6 |  Ctrl + Alt + F6  
Close dialog and execute node |  ⌘ ↩ |  Ctrl + ↵  
* Find out more about loop commands in the KNIME Analytics Platform Flow Control Guide.
### Selected node actions 
Table 5. The supported shortcuts related selected node actions Action | Mac | Windows & Linux  
---|---|---  
Activate the n-th output port view |  ⇧ 1-9 |  Shift + 1-9  
Activate flow variable view |  ⇧ 0 |  Shift + 0  
Detach the n-th output port view |  ⇧ ⌥ 1-9 |  Shift + Alt + 1-9  
Detach flow variable view |  ⇧ ⌥ 0 |  Shift + Alt + 0  
Detach active output port view |  ⇧ ⌥ ↩ |  Shift + Alt + ↵  
Edit node comment |  F2 |  F2  
Select (next) port |  ⌃ P |  Alt + P  
Move port selection |  ← → ↑ ↓ |  ← → ↑ ↓  
Apply label changes and leave edit mode |  ⌘ ↩ |  Ctrl + ↵  
### Workflow annotations 
Table 6. The supported shortcuts related to add workflow documentation via formatted workflow annotations Action | Mac | Windows & Linux  
---|---|---  
Edit annotation |  F2 |  F2  
Bring to front |  ⌘ ⇧ PageUp |  Ctrl + Shift + PageUp  
Bring forward |  ⌘ PageUp |  Ctrl + PageUp  
Send backward |  ⌘ PageDown |  Ctrl + PageDown  
Send to back |  ⌘ ⇧ PageDown |  Ctrl + Shift + PageDown  
Normal text |  ⌘ 0 |  Ctrl + Alt + 0  
Headline 1 - 6 |  ⌘ ALT 1 - 6 |  Ctrl + Alt + 1-6  
Bold |  ⌘ B |  Ctrl + B  
Italic |  ⌘ I |  Ctrl + I  
Underline |  ⌘ U |  Ctrl + U  
Strikethrough |  ⌘ ⇧ S |  Ctrl + Shift + S  
Ordered list |  ⌘ ⇧ 7 |  Ctrl + Shift + 7  
Bullet list |  ⌘ ⇧ 8 |  Ctrl + Shift + 8  
Add or edit link |  ⌘ K |  Ctrl + K  
Increase height |  ⌥ ↓ |  Alt + ↓  
Decrease height |  ⌥ ↑ |  Alt + ↑  
Increase width |  ⌥ ← |  Alt + ←  
Decrease width |  ⌥ → |  Alt + →  
### Workflow editor modes 
Table 7. The supported shortucts for different modes of the workflow editor Action | Mac | Windows & Linux  
---|---|---  
Selection mode (default) |  V |  V  
Pan mode |  P |  P  
Annotation mode |  T |  T  
Leave Pan or Annotation mode |  ESC |  ESC  
### Workflow editor actions 
Table 8. The supported shortcuts for workflow editor actions Action | Mac | Windows & Linux  
---|---|---  
Quick add node |  ⌘ . |  Ctrl + .  
Connect nodes |  ⌘ L |  Ctrl + L  
Connect nodes by flow variable port |  ⌘ K |  Ctrl + K  
Disconnect nodes |  ⌘ ⇧ L |  Ctrl + Shift + L  
Disconnect nodes' flow variable ports |  ⌘ ⇧ K |  Ctrl + Shift + K  
Select node inside the quick nodes panel |  ← → ↑ ↓ |  ← → ↑ ↓  
Add node from quick nodes panel |  ↩ |  ↵  
Moving the selection rectangle to the next element |  ← → ↑ ↓ |  ← → ↑ ↓  
Select multiple elements |  hold ⇧ and press ←/→/↑/↓ then select via ↩ |  hold Shift and press ←/→/↑/↓ then select via ↵  
Move selected elements up |  ⌘ ⇧ ↑ |  Ctrl + Shift + ↑  
Move selected elements down |  ⌘ ⇧ ↓ |  Ctrl + Shift + ↓  
Move selected elements right |  ⌘ ⇧ → |  Ctrl + Shift + →  
Move selected elements left |  ⌘ ⇧ ← |  Ctrl + Shift + ←  
### Component and metanode building 
Table 9. The supported shortcuts related to build components and metanodes Action | Mac | Windows & Linux  
---|---|---  
Create metanode |  ⌘ G |  Ctrl + G  
Create component |  ⌘ J |  Ctrl + J  
Open component or metanode |  ⌘ ⌥ ↩ |  Ctrl + Alt + ↵  
Open parent workflow |  ⌘ ⌥ ⇧ ↩ |  Ctrl + Alt + Shift + ↵  
Expand metanode |  ⌘ ⇧ G |  Ctrl + Shift + G  
Expand component |  ⌘ ⇧ J |  Ctrl + Shift + J  
Rename component or metanode |  ⇧ F2 |  Shift + F2  
Open layout editor |  ⌘ D |  Ctrl + D  
Open layout editor of selected component |  ⌘ ⇧ D |  Ctrl + Shift + D  
### Zooming, panning and navigating inside the canvas 
Table 10. The supported shortcuts related to zooming and panning Action | Mac | Windows & Linux  
---|---|---  
Fit to screen |  ⌘ 2 |  Ctrl + 2  
Fill entire screen |  ⌘ 1 |  Ctrl + 1  
Zoom in |  ⌘ |  Ctrl + +  
Zoom out |  ⌘ - |  Ctrl + -  
Zoom to 100% |  ⌘ 0 |  Ctrl + 0  
Pan |  _hold_ “SPACE” and drag |  _hold_ “SPACE” and drag  
### Panel navigation 
Table 11. The supported shortcut related to panel navigation Action | Mac | Windows & Linux  
---|---|---  
Hide or show side panel |  ⌘ P |  Ctrl + P  
### Others 
Table 12. Other supported shortcuts Action | Mac | Windows & Linux  
---|---|---  
Reset interface scale |  ⌘ ⌥ 0 |  Ctrl + Alt + 0  
## Troubleshooting 
Find here a list of common problems with resolutions regarding KNIME Analytics Platform.
### Downloading or opening workflows from KNIME Community Hub is blocked by firewall 
To solve this issue whitelist the following in your firewall:
```
https://*.amazonaws.com/
https://*.hub.knime.com
```

  * Introduction
  * Workspaces
  * User interface
    * Entry page
    * Workflow editor & nodes
    * Connect to KNIME Hub
    * Switch back to KNIME classic user interface
    * Space explorer
    * Open workflows from KNIME Hub
    * Building workflows
    * Node repository
    * Node description
    * Workflow description
    * KNIME AI Assistant
    * Workflow monitor
    * Node monitor
    * Help
  * Managing workflow states and logs
    * Node reset and workflow execution
    * KNIME log and node operations
  * Configuring KNIME Analytics Platform
    * Preferences
    * Setting up knime.ini
    * KNIME runtime options
  * KNIME tables
    * Data table
    * Column types
    * Sorting
    * Column rendering
    * Table storage
  * Shortcuts
    * General actions
    * Execution
    * Selected node actions
    * Workflow annotations
    * Workflow editor modes
    * Workflow editor actions
    * Component and metanode building
    * Zooming, panning and navigating inside the canvas
    * Panel navigation
    * Others
  * Troubleshooting
    * Downloading or opening workflows from KNIME Community Hub is blocked by firewall


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. KNIME Workflow Invocation Guide 


KNIME Analytics Platform 5.4
# KNIME Workflow Invocation Guide
IntroductionKNIME Workflow Invocation (external clients)KNIME Workflow ServicesKNIME Workflow Invocation (external clients)Workflow Invocation nodesKNIME Workflow ServicesKNIME Workflow Services nodesIntroduction
  * Introduction
    * KNIME Workflow Invocation (external clients)
    * KNIME Workflow Services
  * KNIME Workflow Invocation (external clients)
    * Workflow Invocation nodes
  * KNIME Workflow Services
    * KNIME Workflow Services nodes


Download PDF
## Introduction 
Workflows can be deployed as REST Services on a KNIME Server, and can be tailored to almost any of our REST based needs and make the KNIME stack a powerful integration tool. We can use workflows to enhance existing third party REST endpoints, create brand new REST services, or expose several systems as one REST interface. We can even use these workflows to manage the KNIME Server itself. You can have information about how to access the KNIME Server REST API in the KNIME Server User Guide.
The following guide, instead, focuses on how to create:
  1. _Callee workflows_ : These workflows can expose data to a caller, either a _call workflow_ or a REST interface
  2. _Call workflows_ : These workflows can call other workflows either locally or deployed to a KNIME Server


In order to invoke workflows via REST, a KNIME Server installation is needed, but it is possible to build sets of caller/callee workflows which can be used within KNIME Analytics Platform.
Two sets of nodes are available to build these type of workflows. One set of nodes (see KNIME Workflow Invocation (external clients) section) is primarily made to define APIs for 3rd party clients, whereas the set of nodes which are available as Labs starting with KNIME Analytics Platform version 4.5, is for KNIME-use only (see KNIME Workflow Services section).
### KNIME Workflow Invocation (external clients) 
When uploading KNIME Workflows on KNIME Server they immediately are also deployed as a REST Service. It is then possible to interact with these workflows via the KNIME Server REST API interface from any REST client.
In order to do these operations the workflows that need to be called (_callee workflows_) will need to have special nodes that expose different data types to the REST interface.
For more information about how to build workflows that are able to use this functionality please go to the KNIME Workflow Invocation (external clients) section.
### KNIME Workflow Services 
Moreover, it is also possible to invoke workflows that are deployed to KNIME Server or that exist on the local workspace of your KNIME Analytics Platform by using other workflows (_call workflows_).
Workflow Services make it much easier to call KNIME workflows from other workflows. Rather than invoking standardized (but limiting) JSON-based APIs, it is now possible to create and connect directly to KNIME native API endpoints.
This means that it is possible to share many more KNIME data types, beyond what is possible with JSON, e.g., text documents, decision trees, deep learning models, and many more.
Workflow Services also allow you to define multiple input and output nodes of either consistent or varying data types. These nodes can then be efficiently called with the Call Workflow Service node.
For more information about how to build workflows that are able to use this functionality please go to the KNIME Workflow Services section.
## KNIME Workflow Invocation (external clients) 
### Workflow Invocation nodes 
In KNIME Analytics Platform node repository you can find, under _Workflow Abstraction_ > _Workflow Invocation_ , some useful nodes to interact with your workflows via REST interface and/or by call workflows.
![img workflow invocation](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-workflow-invocation.png)
These nodes can be divided into the following categories, depending on their functionality:
  * Container nodes. Every type of Container Input node defines the exact format (JSON) the workflow needs to be called with, and the Container Output node how the output looks like.
    * Container Input nodes. Container Input nodes can receive different kind of data formats by an external caller and/or from a REST interface and make them available to the workflow. The following are different kind of inputs the Container Input nodes can receive:
      * JSON
      * Row
      * Table
      * File
      * Variable
      * Credentials
    * Container Output nodes. Container Output nodes can send different kind of data formats to an external caller and/or the REST interface.
  * Call nodes. These nodes can be used to call other workflows that resides either locally or on a KNIME Server. These nodes use the very same interface/mechanism used by the Container Nodes. You can use these nodes to build a _call workflow_ which can be used to call a _callee workflow_ in order to either test the service deployed for 3rd party clients, or to use a service from within a KNIME workflow that you also want not only to make accessible to 3rd party clients. In case you want to build a workflow service to be use only within KNIME we recommend to use the new set of nodes described in the KNIME Workflow Services section.


Each of the Container nodes is able to expose data (Output) or to receive data (Input) from a set of specific callers (either via REST interface or by/to a caller workflow).
Table 1. Container nodes and type of calls pairings **Container Input** | **Caller**  
---|---  
Credentials | Call Workflow (Table Based)  
Row | 
  * Call Workflow (Row Based)
  * REST interface

  
Table | Call Workflow (Table Based)  
JSON | 
  * Call Workflow (Row Based)
  * REST interface

  
File | REST interface  
Variable | REST interface  
**Container Output** | **Caller**  
---|---  
Row | 
  * Call Workflow (Row Based)
  * REST interface

  
Table | 
  * Call Workflow (Table Based)
  * REST interface

  
JSON | 
  * Call Workflow (Row Based)
  * REST interface

  
File | REST interface  
#### Container nodes and REST interface 
Below is an explanation of the usage of the nodes that can be called only via a REST interface. For clarity we are going to indicate the `curl` command you need to use to invoke the REST endpoints. You can use anyways any of the several tools available to invoke REST endpoints, such as RESTClient, SoapUI, Postman, or KNIME Analytics Platform.
|  You can read a hands on guide about building workflows for KNIME Server REST API on KNIME blog.   
---|---  
##### Container Input (Variable) node 
This node receives flow variables from an external caller like the REST interface and makes them available to the workflow. A configured parameter makes the Container Input (Variable) visible from the external caller and enables the external caller to send the variables to the Container Input (Variable) node.
In the configuration dialog of the Container Input (Variable) node you can choose a name for the variable input parameter and append to it unique ID values. You can also add a description for the variable input parameter.
By default the node will accept any input. To be accepted the input will need to be well formed and it can contain an arbitrary amount of variables. The input has to be a JSON object. Each property (key/value) pair that it might contain represents a variable, where the key property is the variable name and the value property is the variable value. The type of the variable is determined by using the JSON type of the value property.
Four basic data types are supported:
  * _String_ : A string of characters.
  * _Integer_ : An integer number.
  * _Double_ : A floating point decimal number.
  * _Boolean_ : A truth value that can be either `true` or `false`.


You can also define a number of template variables specification. Only the input that will match those templates will be accepted and their value exposed to the workflow. To do so check the option _Require input to match template variables specification_ and click _Add_ to add the templates.
![img container input var template](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-container-input-var-template.png)
If you define only one template variable you can also check the option _Use simplified JSON format_. This will allow the external input format to be simpler by using the value directly instead of an object that defines the variables as properties. For example, if this option is enabled you can define a variable by using the following simplified `InputParameters`:
```
{
  ...
  "parameter-name": <value>,
  ...
}
```

instead of the object notation:
```
{
  ...
  "parameter-name": {
    "variable-name": <value>
  },
  ...
}
```

Please note that in this case the variable will always have the same name as the parameter name without the unique ID appended.
You can also feed flow variables from an another node output, e.g. you can define variables via the Variable Creator node, and set those input variables as template, either by replacing or merging the template variables already defined, if any.
Once you have deployed the workflow where the Container Input (Variable) node is present to your KNIME Server you can execute it via REST API. You can either access it via the SWAGGER UI, by right clicking the deployed workflow from KNIME Explorer in KNIME Analytics Platform and selecting _Show API definition_ from the context menu. In SWAGGER UI choose _executing_ to see the POST request specifications of the workflow. Under _Request body_ section you will see, if defined, the template variables with either simplified JSON or object notation format.
An example of a curl command to execute a POST request with a variable input template is provided, where `{{baseUrl}}` is the URL of your KNIME Server and `{{path}}` is the path relative to the Server file system to the workflow that you want to execute.
```
curl --location -g --request POST '{{baseUrl}}/rest/v4/repository/{{path}}:execution' \
--data-raw '"{
 "variable-input": {
  "variable-input": "test"
 }
}"'
```

##### Container Input/Output (File) nodes 
The Container Input (File) and Contained Output (File) nodes can be used for file upload and download respectively.
Due to some limitations of the SWAGGER UI you can not specify how to call the Container Input (File) node since you would need a different content type for the input file, such as a multi-part content type instead of a JSON type and this is not possible to specify it via the SWAGGER UI.
Instead you will need to use another tool to invoke the REST endpoint. Here, we will give example commands based on `curl`.
##### Upload a file via REST API call 
In order to upload a file to a workflow you will need to add a Container Input (File) node to your workflow. When configuring this node you can give a name to the parameter successively identifying your input file, choose to append to it a unique ID, and give the input parameter a description. Then you can define the name of the Path type flow variable that contains the location of the local copy of the resource. Finally you can also decide to use a default file in case no external resource is uploaded, but an external resource will always take precedence over a default file. Only the path of a default file will be exposed.
Then, to upload a file to a REST API enabled workflow where the Container Input (File) node is present you can use the following `curl` command:
```
curl --location --request POST '{{baseUrl}}/rest/v4/repository/{{path}}:execution' \
--header 'Content-Type: multipart/form-data' \
--form 'input-file=@"{{file-path}}"'
```

where `{{baseUrl}}` is the URL of your KNIME Server and `{{path}}` is the path relative to the Server file system to the workflow that you want to execute. Via this command you are also defining the content type of the file you want to upload as `form-data` type and `{{file-path}}` is the path to that file.
**Download a file via REST API call**
In order to download a file from a workflow you will need to add a Container Output (File) node to your workflow. When configuring this node you can give a name to the parameter successively identifying your output file, choose to append to it a unique ID, and give the input parameter a description. Then you need to define the variable that contains the path to the resource to be exposed. This needs to be of the Path type.
Since, at the current status, the execution endpoint of the Server REST API deletes the respective job after a successful execution, any output files for download are deleted, too. Thus, in order to download a file from a REST API enabled workflow where the Container Output (File) node is present you will need to go through the following steps:
  1. Create a job for the workflow on KNIME Server: To do so send a POST request to the `jobs` endpoint
```
curl --location --request POST '{{baseUrl}}/rest/v4/repository/{{path}}:jobs'
```

  2. Execute the created job: To do so send a POST request to the `jobs` endpoint, adding the identifying ID of the job `{uuid}` which you will find in the output response of the previous request under `"id"`
```
curl --location --request POST '{{baseUrl}}/rest/v4/jobs/{uuid}'
```

  3. Download the created file: To do so send a GET request to the `output-resources` endpoint
```
curl --location --request GET curl '{{baseUrl}}/rest/v4/jobs/{uuid}/output-resources/{parameter-name}' \
-o {parameter-name}
```

where `{parameter-name}` is the one configured in the Container Output (File) node configuration dialog and that you can find in the output response of the previous request under `"outputResources"`.
  4. Finally, you can optionally delete the executed job: To do so send a DELETE request to the `jobs` endpoint
```
curl --location --request DELETE '{{baseUrl}}/rest/v4/jobs/{uuid}
```



## KNIME Workflow Services 
To make it much easier to call KNIME workflows from other workflows it is possible to use the KNIME Workflow Services set of nodes.
This set of KNIME Labs nodes can be used to build and use workflows as native KNIME services.
One advantage of these nodes is that data being passed between the caller and the callee do not need to be serialized into/from JSON-objects like it is the case when the Container Input/Output nodes are used together with Call (Local) Workflow nodes, making the process more efficient.
Moreover this new set of nodes makes use of dynamic ports functionality, which allows to use only one node with adjustable port types.
We recommend to use Container Input/Output nodes exclusively in workflows which are intended to be called from external REST clients.
Use the Workflow Service Input/Output nodes instead to build workflows intended to be called from within another workflow (i.e. using Call Workflow Service node).
### KNIME Workflow Services nodes 
In KNIME Analytics Platform node repository you can find, under _KNIME Labs_ > _Workflow Abstraction_ , some useful nodes to build workflows that work as native KNIME services.
![img workflow service](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-workflow-service.png)
#### Workflow Service Input/Output nodes 
Use these two nodes to receive (Input) or send (Output) a data table or any other port object.
  * Workflow Service Input node: This node can receive a data table or any other object from a KNIME workflow that calls this workflow using the Call Workflow Service node. When adding the node to your workflow an optional data table input port and a data table output port are shown. Click the three button dynamic port button on the left bottom corner of the node to _Exchange Callee Workflow Input port_ type.
![img workflow service input porttype](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-workflow-service-input-porttype.png)
You can then choose between any available port type.
![img workflow service input choose porttype](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-workflow-service-input-choose-porttype.png)
Finally, in the node configuration dialog you can choose a _Parameter name_ to assign to the input object.
![img workflow service input config](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-workflow-service-input-config.png)
The parameter name is supposed to be unique, but this is not enforced. In case multiple Workflow Service Input nodes define the same parameter name, KNIME will make them unique by appending the node’s node ID, e.g., "input-table" becomes "input-table-7".
  * Workflow Service Output node: This node can send a data table or any other object to a KNIME workflow that executes this workflow using the Call Workflow Service node. When adding the node to your workflow a data table input port and a data table output port are shown. Click the three button dynamic port button on the left bottom corner of the node to _Exchange Callee Workflow Output port_ type.
![img workflow service output porttype](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-workflow-service-output-porttype.png)
You can then choose between any available port type.
![img workflow service output choose porttype](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-workflow-service-output-choose-porttype.png)
Finally, in the node configuration dialog you can choose a _Parameter name_ to assign to the output object.
![img workflow service output config](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-workflow-service-output-config.png)
The parameter name is supposed to be unique, but this is not enforced. In case multiple Workflow Service Input nodes define the same parameter name, KNIME will make them unique by appending the node’s node ID, e.g., "output-table" becomes "output-table-7".


An example of a callee workflow using this set of nodes is available on the KNIME Hub.
![img callee workflow](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-callee-workflow.svg)
#### Call Workflow Service node 
Use this node in a workflow to call another workflow and obtain the results for further processing in the workflow. The workflow will be able to receive inputs via Workflow Service Input nodes and return outputs using Workflow Service Output nodes.
This node comes with an optional KNIME Server connection. Use the KNIME Server Connector node to connect to a KNIME Server in case the callee workflow has been deployed to a KNIME Server installation. In the node configuration dialog you can then set parameters to manage the connection to the KNIME Server. If you want to call a workflow in your local workspace you can just leave this port unconnected.
You can insert the _Workflow Path_ or _Browse workflows_ to choose the callee workflow.
![img call workflow service config](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-call-workflow-service-config.png)
A button _Adjust node ports_ in the node configuration dialog will appear in case the node ports do not match the parameters of the callee workflow. Click the button to adjust the node ports.
![img call workflow service config adjust](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-call-workflow-service-config-adjust.png)
Each Workflow Service Input node in the workflow to be called will create an input port on this node when finishing the configuration of the Call Workflow Service node. Similarly, each Workflow Service Output node will create an output port on the node.
![img node with ports](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-node-with-ports.svg)
An example of a call workflow using this set of nodes is available on the KNIME Hub.
![img call workflow](https://docs.knime.com/latest/analytics_platform_workflow_invocation_guide/img/img-call-workflow.svg)
  * Introduction
    * KNIME Workflow Invocation (external clients)
    * KNIME Workflow Services
  * KNIME Workflow Invocation (external clients)
    * Workflow Invocation nodes
  * KNIME Workflow Services
    * KNIME Workflow Services nodes


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. Publish Your Extension on KNIME Community Hub 


KNIME Analytics Platform 5.4
# Publish Your Extension on KNIME Community Hub
Publish your extensionMake your extension trustedMake your extension trusted the first timeKeep your extension trustedFrequently Asked QuestionsPublish your extension
  * Publish your extension
  * Make your extension trusted
    * Make your extension trusted the first time
    * Keep your extension trusted
  * Frequently Asked Questions


Download PDF
With KNIME it is possible to write extensions using Java or Python. To know more about how to build your nodes and put them into extensions you can check the following guides:
  * Create a New Java based KNIME Extension
  * Create a New Python based KNIME Extension


Once you have created an extension, you can publish it for free, open source, and without additional licenses. Publishing means that you contribute the extension and make it available via KNIME Community Hub to the community.
This guide will explain you how to publish your extension in two steps on KNIME Community Hub and how make it a trusted extension.
## Publish your extension 
This section explains the procedure to take your extension code, build the extension, put it on our update sites and publish it on KNIME Community Hub.
The first step is to send us an email to community-contributions@knime.com with the following information:
  * Name of the corresponding forum account (the extension will be linked with this account on Hub and also this account will have access to the build jobs in Jenkins)
  * Name of the extension
  * Short description of the extension (1-2 sentences)
  * Vendor or contributing entity (academic insitute, developing group, private person, company)
  * Name of maintaining contact person
  * Email of maintaining contact person
  * Link to the code base of your contribution (your GitHub repository or alike)


Additionally, log in once to hub.knime.com with the forum account so we can grant it access to our Jenkins infrastructure.
After you sent the email and logged in to hub.knime.com, we will:
  * Add you to our developer mailing list
  * Set up the build jobs in Jenkins (one nightly build job and one for the current release)
  * Give your forum account access to the build jobs
  * Send you links to these build jobs
  * Inform you if there are any issues
  * Put the extension on KNIME Community Hub
  * Inform you that you have a new _experimental Open Source extension_


## Make your extension trusted 
![verify logo](https://docs.knime.com/latest/development_contribute_extension/img/verify-logo.png)
Making your extension trusted shows others that your extension complies with node-specific standards, is well-tested, without critical security concerns, and that during installation no signing of unstrusted content needs to be done. This will be shown via the trusted logo on KNIME Community Hub.
This section explains the two steps to make your extension a trusted extension and to make sure it stays trusted in the future. Of course, you can take care of one or all of these steps already during development. The requirements for these two procedures are also outlined.
### Make your extension trusted the first time 
#### How to become trusted 
  1. Have a look at the requirements to become trusted
  2. Adjust your extension to comply with them
  3. Inform us


Then, together with you, we will:
  * Review if your extension meets the requirements
  * If yes, make your extension trusted


#### Requirements to become trusted 
  * Each node of the extension is covered in a test workflow
  * Compliance with noding guidelines
  * No known security issues (the build job runs automatically a CVE check)
  * Release notes or changelog in the public repository
  * Responsive contact to react to users (we need a current email address and forum user name from you)


### Keep your extension trusted 
#### How to stay trusted 
As the KNIME Analytics Platform, security issues and third-party libraries evolve, extensions need to be reevaluated from time to time (usually before every feature release (every six months)).
  1. Have a look at the requirements to stay trusted
  2. If necessary: adjust your extension to comply with them
  3. Inform us


#### Requirements to stay trusted 
  * Updates comply to the above points (nodes are covered in test workflows, compliance to noding guidelines, no security issues, release notes present)
  * Valid contact and active support in the forum
  * Backwards compatibility: the test workflows and also the nodes in existing workflows behave and produce _exactly_ the same results in newer versions just as they did in older versions; see noding guidelines
  * Maintenance: make sure that your extension is working with the latest two AP versions
  * This evaluation will happen before every feature release (5.3 → 5.4 and so on)
  * If the requirements cannot be met anymore, the extension will be an experimental extension again - of course: as soon as you have time, we can make the extension trusted again


## Frequently Asked Questions 
**What is a build job and what is Jenkins?** A build job is a program which takes code and transforms it into a functional application or part of an application. In your case, a build job takes your code and transforms it into an extension, which can be installed in the application KNIME Analytics Platform and is then a part of it.
Jenkins is a tool which allows us (KNIME) to automate the process defined by a build job.
**When will the extensions be updated? Every night?** The extension will only be updated if the maintainer or we trigger a new build job.
For the build jobs of a release, this will (and should) only be done if changes to the content have been made.
Extensions will be updated on demand, e.g. when you hit the "Run" button to build your extension in Jenkins. For a job of a certain release you usually want to do this before a release or if you explicitly want a new version to be built.
**What is an update site?** An update site is a place (e.g. a repository on our servers or a location on your machine) which contains extensions installable by the KNIME Analytics Platform. A prominent example is our KNIME Analytics Platform Update Site or its zipped form which can be downloaded and used locally on your machine.
-
**On which update sites is my extension?** There are different update sites containing different types of community extensions: extensions with a commercial interest are on the Partner update site, whereas free open source extensions are on the experimental or trusted Open Source update sites, respectively. Check out the differences between experimental and trusted extensions. The update site for nightly extensions is meant for testing your extension or providing quick updates to your peer group. Make use of it!
Your extension appears - depending on your extension type - on the update site for
  * experimental Open Source extensions: `https://update.knime.com/community-contributions/<current-version>`
  * trusted Open Source extensions: `https://update.knime.com/community-contributions/trusted/<current-version>`
  * Partner extensions: `https://update.knime.com/partner/<current-version>`
  * nightly extensions: `https://update.knime.com/community-contributions/trunk`


**What are the advantages of making my extension trusted?** Making your extension trusted shows others that your extension complies with node-specific standards, is well-tested, without critical security concerns, and that during installation no signing of unstrusted content needs to be done. This will be shown via the trusted logo on KNIME Community Hub.
![verify logo](https://docs.knime.com/latest/development_contribute_extension/img/verify-logo.png)
**Can I publish now and make the extension trusted later?** Yes. The two processes of publishing and making trusted can be done together, but are also completely independent from each other.
**What are the differences between a trusted and an experimental Open Source extension?** The differences are the characteristics of a trusted extension:
  * Each node of the extension is covered in a test workflow
  * Compliance with noding guidelines
  * No known security issues
  * Release notes or changelog in the public repository
  * Valid contact and active support in the forum
  * Backwards compatibility: the test workflows and also the nodes in existing workflows behave in newer versions just as they did in older versions
  * Maintenance for the last two feature release versions of the KNIME Analytics Platform


**What happens if a trusted extension does not meet therequirements to stay trusted anymore?** If a trusted extension does not meet the requirements to stay trusted anymore, we will reach out to the maintaining persons. We will assess together whether there is a possibility to make the extension meet the criteria again. If we cannot work out a solution together, we might have to temporarily remove the trusted status for the current and future KNIME Analytics Platform versions (until, of course, the maintaining person makes the extension trusted again). This could lead to a change of update site, the trusted logo not being displayed on KNIME Community Hub and that an additional step during installation of the extension is required where the installing person is asked to trust unsigned content. We appreciate your understanding and collaboration in maintaining the quality and trustworthiness of KNIME extensions!
**How do I provide integration tests?** To ensure that the nodes of your extension behave as expected you can verify their behaviour via integration tests. Such tests in the KNIME Analytics Platform are often done via test workflows.
Ensure that your workflow works as expected and will continue to do so:
  * Create test workflows covering all of your nodes. You can read this article to help you test your nodes.
  * Upload the test workflows
    1. Create in your KNIME Analytics Platform a new mount point for the Community Server
      1. Click the Mount Point icon.
![testflow1](https://docs.knime.com/latest/development_contribute_extension/img/testflow1.png)
      2. Click 'New'.
![testflow2](https://docs.knime.com/latest/development_contribute_extension/img/testflow2.png)
      3. Double-click 'KNIME-Community Server'.
![testflow3](https://docs.knime.com/latest/development_contribute_extension/img/testflow3.png)
      4. _Apply and Close_
    2. Open the Server; login with your forum credentials; you can insert your test workflows in _Testflows — > Trunk —> <your project name>_; create it if it does not exist already.
![testflow4](https://docs.knime.com/latest/development_contribute_extension/img/testflow4.png)
    3. Upload your test workflows to the folder of your project


**What does it mean to maintain my extension?** Maintenance has two major aspects:
  * If you are notified about new security issues, address them within four weeks
  * When we release a new version of KNIME, we inform you beforehand via the mailing list and the forum about changes and updates of the underlaying infrastructure; please address them (preferably in new branches for the corresponding KNIME version in your repository)


Besides that, we will create new build jobs for new releases and help you if you have any questions.
**How can I add Python nodes to my Java extension?** Requirements:
  * An existing Java extension
  * A Python extension
  * A Python environment with https://anaconda.org/knime/knime-extension-bundling installed


Procedure:
  1. Execute the script `build_python_extension.py` in the required Python extension
  2. Further instructions are given by `build_python_extension.py --help` and will be outlined upon execution of the script for how to test the extension and how to make the Python nodes available with a Jenkins job


**How can I change the description of my extension on KNIME Community Hub?**
If it is a Java extension, the description is set via the `description` tag in the `feature.xml` in the feature plugin. See here for an example.
If it is a Python extension, the description is set via the `long_description` parameter in the `knime.yml`.
|  The Community Hub is regularly updated only during feature releases; please email us if you wish to have an update of your description in between.   
---|---  
  * Publish your extension
  * Make your extension trusted
    * Make your extension trusted the first time
    * Keep your extension trusted
  * Frequently Asked Questions


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.
## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. KNIME Integrated Deployment Guide 


KNIME Analytics Platform 5.4
# KNIME Integrated Deployment Guide
IntroductionInstallationCreate a workflowCapture segments of a workflowConfiguration of the Capture Workflow End nodeCombine captured workflow segmentsSave and execute a captured workflowWrite a workflow object as production workflowDeploy a production workflow to KNIME ServerRead a production workflowExecute a production workflow deployed to KNIME ServerExecute a production workflow locallyExtract the workflow summaryIntroduction
  * Introduction
  * Installation
  * Create a workflow
    * Capture segments of a workflow
    * Configuration of the Capture Workflow End node
    * Combine captured workflow segments
  * Save and execute a captured workflow
    * Write a workflow object as production workflow
    * Deploy a production workflow to KNIME Server
    * Read a production workflow
    * Execute a production workflow deployed to KNIME Server
    * Execute a production workflow locally
    * Extract the workflow summary


Download PDF
## Introduction 
The traditional data science process starts with raw data and ends up to the creation of a model. That model is then usually moved into daily production. Integrated Deployment allows not only the model but all the associated preparation and post-process steps to be identified and reused in production in an automatic way, and without having to shift between different tools.
The Integrated Deployment nodes allow you to capture the segments of the workflow needed for running in a production environment, the model or library itself as well as the data preparation. These captured subsets are saved automatically as workflows with all the relevant settings and transformations and can be run at any time, both on KNIME Analytics Platform for model validation and on KNIME Server for model deployment.
In this guide we will explain how to use the nodes that are part of the KNIME Integrated Deployment Extension, as well as other useful nodes that allow the use of this feature in different environments.
|  You can access more material about Integrated Deployment on this website section.  
---|---  
## Installation 
|  If you have installed KNIME Analytics Platform 4.5 or greater you can skip this part.  
---|---  
You can install KNIME Integrated Deployment Extension from:
  * KNIME Hub: go to the KNIME Integrated Deployment Extension page on KNIME Hub. Here, drag and drop the squared yellow icon to the workbench of KNIME Analytics Platform.
  * KNIME Analytics Platform: go to _File_ → _Install KNIME Extensions…_ in the toolbar and find KNIME Integrated Deployment under KNIME Labs Extensions or type Integrated Deployment in the search bar.


For more detailed instructions refer to the Installing Extensions and Integrations section on the KNIME Analytics Platform Installation Guide.
Now the nodes are available in the node repository under _KNIME Labs_ → _Integrated Deployment_ , as shown in Figure 1.
![02 integrated deployment nodes repository](https://docs.knime.com/latest/integrated_deployment_guide/img/02_integrated_deployment_nodes_repository.png)
Figure 1. The Integrated Deployment nodes in the node repository
## Create a workflow 
In this section we will explain how to build a workflow, capture some segments, and finally combine them into a new workflow object. In this new workflow object you can also store the input and output data.
|  All the workflows shown in this section are available on KNIME Hub.  
---|---  
### Capture segments of a workflow 
With the Integrated Deployment nodes you can capture the segments of the workflow that you need. To do this you can use Capture Workflow Start and Capture Workflow End nodes, as shown in Figure 2.
![03 capture workflow part](https://docs.knime.com/latest/integrated_deployment_guide/img/03_capture_workflow_part.svg)
Figure 2. A segment of a workflow is captured
Capture Workflow Start node marks the start of a workflow segment to be captured. Capture Workflow End node marks the end of a workflow segment to be captured. The entire workflow segment within the scope of these two nodes is then available at the workflow output port of the Capture Workflow End node. Nodes that have out-going connections to a node that is part of the scope but are not part of the scope themselves are represented as static inputs but not captured.
In Figure 3 an example of the corresponding captured workflow is shown. For an explanation on how to generate the captured workflow please refer to the Store a captured workflow section.
![03 captured workflow part](https://docs.knime.com/latest/integrated_deployment_guide/img/03_captured_workflow_part.svg)
Figure 3. The captured workflow
### Configuration of the Capture Workflow End node 
#### Input data 
You can store input data with your captured workflow segment. These data will be used by default when running the captured workflow segment unless you provide a different input. Storing input data will add data to the Container Input (Table) node in the captured workflow segment. In this way data are saved with the workflow when writing it out or deploying it to KNIME Server and are available when executing the workflow locally or on KNIME Server.
To store input tables, right-click the Capture Workflow End node and choose _Configure…​_ from the context menu. In the node configuration dialog that opens, shown in Figure 4, check _Store input tables_ option. Here you can also choose the maximum number of rows to store with the captured workflow.
![03 capture workflow end config](https://docs.knime.com/latest/integrated_deployment_guide/img/03_capture_workflow_end_config.png)
Figure 4. The Capture Workflow End node configuration dialog
#### Add or remove input or output ports in Capture Workflow nodes 
In Capture Workflow Start and Capture Workflow End nodes you can choose to have how many input and output ports of different types as necessary, as shown in Figure 5.
![03 add ports](https://docs.knime.com/latest/integrated_deployment_guide/img/03_add_ports.svg)
Figure 5. Add/remove input/output ports to and from Capture Workflow Start/End nodes
  * To add input ports:
    1. Click the three dots in the left-bottom corner of the Capture Workflow Start/End node
    2. Choose _Add Captured workflow inputs port_ from the context menu that opens
    3. Choose the port type from the drop-down menu in the window that opens
  * To remove input ports:
    1. Click the three dots in the left-bottom corner of the Capture Workflow Start/End node
    2. Click _Remove Captured workflow inputs port_.


#### Rename captured workflow and input/output ports 
In the Capture Workflow End node you can change the name of the captured segment of the workflow, as well as inputs and output ports. Right-click the Capture Workflow End node and choose _Configure…​_ from the context menu. In the node configuration dialog that opens, you can give the captured workflow segment a custom name in the _Settings_ tab, as shown in Figure 4. To rename input and output ports instead go to _Input and Output IDs_ tab, as shown in Figure 6.
![03 capture workflow end config io ports](https://docs.knime.com/latest/integrated_deployment_guide/img/03_capture_workflow_end_config_io_ports.png)
Figure 6. The Capture Workflow End node configuration dialog, _Input and Output IDs_ tab.
### Combine captured workflow segments 
You can capture more than one segment of the workflow with multiple couples of Capture Workflow Start/End nodes and combine all the different captured segments into a single new workflow object. To combine the different captured segments use the Workflow Combiner node. The Workflow Combiner node can take as many as necessary workflow objects as input and generates a combined captured workflow. To add or remove input ports click the three dots in the left-bottom corner of the Workflow Combiner and choose _Add(Remove) workflow model port_ in the context menu. An example with the relative captured workflow is shown in Figure 7.
![03 combined workflows](https://docs.knime.com/latest/integrated_deployment_guide/img/03_combined_workflows.svg)
Figure 7. Different segments of a workflow are captured and combined in a new workflow
#### Input/output port mapping 
Free output ports from one workflow segment are connected to the free input ports of the consecutive workflow segment. However the pairing of the output and input ports can be manually configured.
Workflow segments connected to consecutive ports to the Workflow Combiner node form a pair. For each pair you can select if and how to connect the outputs of the first workflow segment to the inputs of the consequent workflow segment. By default the inputs of each workflow segment are automatically connected with the outputs of the predecessor. In case the default pairing cannot be applied, e.g. because of a non-matching number of inputs and outputs or incompatible port types, the node requires manual configuration in order to be executed.
If the default configuration is not applicable or you want to manually configure it, right-click the Workflow Combiner node and choose _Configure…​_ from the context menu to open the node configuration dialog, shown in Figure 8
![03 workflow combiner config](https://docs.knime.com/latest/integrated_deployment_guide/img/03_workflow_combiner_config.png)
Figure 8. The Workflow Combiner node configuration dialog
Here, you can choose the pairing manually. For example, the first pane in Figure 8 shows the pairing between a workflow segment (`Preprocessed`) that has two data output ports `preprocessed_out1` and `preprocessed_out2` with a workflow segment (`String To Number`) connected to the consequent port of the Workflow Combiner node. From the drop-down menu you can choose to connect any or `<NONE>` of the output ports of the `Preprocessed` workflow segment to the input port of the `String To Number` workflow segment.
Please, note that only compatible port types are shown and can be connected.
## Save and execute a captured workflow 
In this section we will explain how to save, deploy and execute the captured workflow, in a local or connected file system or on KNIME Server.
|  All the workflows shown in this section are available on KNIME Hub.  
---|---  
The captured workflow can be:
  1. Written locally or to a file system using the Workflow Writer node
  2. Deployed to KNIME Server with the Deploy Workflow to Server node
  3. Directly executed with the Workflow Executor node


### Write a workflow object as production workflow 
#### Write locally 
You can write the newly stored workflow object to the local file system as production workflow using the Workflow Writer node. Connect the Workflow Writer node editor to the output workflow object port (squared solid black port) of the workflow you want to write, i.e. the output port of Workflow Combiner node or Capture Workflow End node, as shown in Figure 9.
![04 write locally](https://docs.knime.com/latest/integrated_deployment_guide/img/04_write_locally.svg)
Figure 9. An example workflow to write a production workflow to the local file system
Now, you can open the Workflow Writer node configuration dialog, shown in Figure 10.
![04 workflow writer config](https://docs.knime.com/latest/integrated_deployment_guide/img/04_workflow_writer_config.png)
Figure 10. The Workflow Writer node configuration dialog
Here, you can setup:
  * Output location: You can choose to write a captured workflow with a specific path relative to the local file system, a specific mountpoint or relative to current mount point, current workflow or current workflow data area.
  * Workflow: You can choose to overwrite the workflow if one with the same name already exists in the chosen location or to fail. You can also give the captured workflow a custom name.
  * Deployment options: You can choose between writing the workflow, writing the workflow and opening it in explorer, or export the workflow as `knwf` archive.


In the _Inputs and outputs_ tab of the Workflow Writer node configuration dialog, shown in Figure 11, you can also choose to add input and output nodes.
![04 workflow writer config io tab](https://docs.knime.com/latest/integrated_deployment_guide/img/04_workflow_writer_config_io_tab.png)
Figure 11. The Workflow Writer node configuration dialog, _Input and outputs_ tab
#### Write to a connected file system 
You can also connect the Workflow Writer node to a connected file system, e.g. SharePoint, Amazon S3, Google Cloud Storage. First you need to authenticate to the chosen connected file system and then connect to it. Then, right-click the three dots in the left-bottom corner of the Workflow Writer node and from the context menu choose _Add File System Connection port_. A squared solid cyan connection port will appear and it can be connected to the relative output file system connection port of the chosen connected file system connector node. An example workflow with SharePoint connected file system is shown in Figure 12.
![04 write to sharepoint](https://docs.knime.com/latest/integrated_deployment_guide/img/04_write_to_sharepoint.svg)
Figure 12. An example workflow to write a production workflow to a connected file system
Now the Workflow Writer node configuration dialog shows the remote file system in the _Output location_ section of the _Settings_ tab and you can choose to save the captured workflow into a location on that file system.
|  Please note that if you are writing to a connected file system the workflow can only be written or exported as `knwf` archive, and it is not possible to prompt its opening as a new explorer tab on KNIME Analytics Platform.  
---|---  
### Deploy a production workflow to KNIME Server 
You can deploy a production workflow to KNIME Server with the Deploy Workflow to Server node, following these steps:
  1. Set up a connection to the server with the KNIME Server Connection node
  2. Connect the KNIME Server Connection node to the Deploy Workflow to Server node via the squared solid light blue KNIME Server connection port
  3. Connect the Deploy Workflow to Server node to the output workflow object port of the workflow you want to write, i.e. the squared solid black output port of Workflow Combiner node or Capture Workflow End node, as shown in Figure 13.
![04 deploy server](https://docs.knime.com/latest/integrated_deployment_guide/img/04_deploy_server.svg)
Figure 13. An example workflow to deploy a production workflow to KNIME Server
  4. Open the Deploy Workflow to Server configuration dialog, shown in Figure 14, by right-clicking the node and choosing _Configure…​_ from the context menu.
![04 deploy workflow server config](https://docs.knime.com/latest/integrated_deployment_guide/img/04_deploy_workflow_server_config.png)
Figure 14. The Deploy Workflow to Server node configuration dialog


In the Deploy Workflow to Server node configuration dialog you can setup the following options:
  * Choose folder on KNIME Server: You can choose the folder on the server file system to which deploy the newly captured workflow.
  * Workflow: You can choose to overwrite a workflow if one with the same name already exists in the chosen location or to fail. You can also give the written workflow a custom name.
  * Deployment options: You can choose to create a snapshot and add an optional snapshot comment in order to keep track of the changes made to the workflow on the server.


In the _Inputs and outputs_ tab of the Deploy Workflow to Server node configuration dialog, shown in Figure 15, you can also choose to add input and output nodes to the workflow. This will add Container Input/Output (Table) nodes to the production workflow deployed to KNIME Server.
![04 deploy workflow server config io tab](https://docs.knime.com/latest/integrated_deployment_guide/img/04_deploy_workflow_server_config_io_tab.png)
Figure 15. The Deploy Workflow to Server node configuration dialog, _Input and outputs_ tab
|  If the captured workflow deployed to Server is executed on the Server (see Execute a captured workflow deployed to KNIME Server section) via Call Workflow node the Container Input/Output (Table) nodes are mandatory.   
---|---  
### Read a production workflow 
You can read a workflow that has been saved in any standard or connected file system or deployed to KNIME Server by using the Workflow Reader node.
#### Select the production workflow to read 
You can read workflows from standard file systems just by configuring the Workflow Reader node to read from either _Local File System_ or _Mountpoint_ and _Relative to_ in the node configuration dialog as shown in Figure 16.
![img config wf reader](https://docs.knime.com/latest/integrated_deployment_guide/img/img-config-wf-reader.png)
Figure 16. The configuration dialog of a Workflow Reader node
Alternatively you can read workflows stored on a remote file system or deployed to a KNIME Server. To do so you need to add a connection port to the Workflow Reader node by clicking the three dots on the left bottom corner of the node and use a Connector node to connect to the desired remote file system.
An example to read a workflow deployed to KNIME Server is shown in Figure 17.
![img read workflow server](https://docs.knime.com/latest/integrated_deployment_guide/img/img-read-workflow-server.svg)
Figure 17. The Workflow Reader node to read a workflow deployed to a KNIME Server
Please be aware that if you are reading your workflow from a file system that is not a KNIME file system, i.e. a remote file system that is not KNIME Server or your local system standard file system, you will be able to read the workflows that are saved as `.knwf` files.
|  It is not possible to read KNIME components.   
---|---  
#### Configuration options of the Workflow Reader node 
In the _Settings_ tab of the configuration dialog of the Workflow Reader node, not only you can choose the workflow location from which read the workflow but also, under _Read options_ pane you can:
  * Under _Workflow name_ : choose a name for the workflow you read in
  * Check _Remove input and output nodes_ : remove all Container Input and Container Output nodes from the workflow you read in. These nodes are then implicitly represented, allowing you to, e.g., combine the resulting workflow segment with other workflow segments via the new implicit input and output ports via the Workflow Combiner node, or to execute the workflow segment providing the desired input data via the Workflow Executor node. You can then assign an _Input ID prefix_ and/or an _Output ID prefix_ to the implicit input and output ports of the workflow segment. The input and output IDs will then be created as follows: `"<Input ID prefix>n"` and `"<Output ID prefix>m"` with `n` and `m` run from 1 to _the maximum number of input/output ports_.


### Execute a production workflow deployed to KNIME Server 
You can execute a production workflow:
  * On KNIME Server or on KNIME WebPortal
  * On KNIME Server from the KNIME Analytics Platform client. In this way you can also provide additional data in case you want to run the production workflow on new data. To do this you can use a Call Workflow node, connected to the KNIME Server Connection node via the KNIME Server connection port. If needed, i.e. if no input data was stored with the production workflow, input data have to be provided, as shown in Figure 18.
![04 call workflow](https://docs.knime.com/latest/integrated_deployment_guide/img/04_call_workflow.svg)
Figure 18. An example workflow to execute the production workflow from KNIME Server with some input data


### Execute a production workflow locally 
You can also execute the workflow locally by using the Workflow Executor node.
|  The Workflow Executor node can only execute a workflow that is captured from the same workflow the Workflow Executor node is part of.   
---|---  
The node ports match the input and output ports of the workflow. Connect the Workflow Executor node to the output workflow object port of the workflow you want to write, i.e. the output workflow object port of Workflow Combiner node or Capture Workflow End node, and, if needed, the input data port to a data table. An example is shown in Figure 19.
![04 execute locally](https://docs.knime.com/latest/integrated_deployment_guide/img/04_execute_locally.svg)
Figure 19. An example workflow to execute the production workflow locally
You can adjust the needed input and output ports to match those of the workflow to be executed by opening the node configuration dialog shown in Figure 20, and clicking _Auto-adjust ports (carried out on apply)_.
![04 workflow executor config](https://docs.knime.com/latest/integrated_deployment_guide/img/04_workflow_executor_config.png)
Figure 20. The Workflow Executor node configuration dialog
### Extract the workflow summary 
You can generate a workflow summary using the Workflow Summary Extractor node. It creates a document in either XML or JSON format which contains workflow meta data and makes it available at the node output as data table. The workflow summary is a detailed and structured description of a workflow including its structure, configuration of the nodes, port specifications, node and workflow annotations.
You can connect the input workflow port of the Workflow Summary Extractor node to any other output port of the same type to select the workflow to be processed.
In the configuration dialog of the Workflow Summary Extractor node you can choose the format of the output between JSON and XML and assign a name other than the default one to the column of the final table which will contain the workflow summary extracted.
  * Introduction
  * Installation
  * Create a workflow
    * Capture segments of a workflow
    * Configuration of the Capture Workflow End node
    * Combine captured workflow segments
  * Save and execute a captured workflow
    * Write a workflow object as production workflow
    * Deploy a production workflow to KNIME Server
    * Read a production workflow
    * Execute a production workflow deployed to KNIME Server
    * Execute a production workflow locally
    * Extract the workflow summary


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. KNIME Expressions Guide 


KNIME Analytics Platform 5.4
# KNIME Expressions Guide
IntroductionExpression nodesGeneral BehaviourExpression nodeExpression Row Filter nodeVariable Expression nodeExpression LanguageValue types and literalsInput data accessOperatorsFunctionsConstantsIntroduction
  * Introduction
  * Expression nodes
    * General Behaviour
    * Expression node
    * Expression Row Filter node
    * Variable Expression node
  * Expression Language
    * Value types and literals
    * Input data access
    * Operators
    * Functions
    * Constants


Download PDF
## Introduction 
In this guide you will find documentation about:
  * The Expression nodes: Learn how to perform versatile data manipulation in KNIME workflows using the KNIME Expression Language.
  * The KNIME Expression Language: Find guidance for the syntax, semantics, and usage of the KNIME Expression Language.


## Expression nodes 
There are currently three nodes available in KNIME Analytics Platform that allow you to use the KNIME Expression Language to manipulate your data within KNIME workflows:
  * **Expression**: Enables row-by-row data manipulation to add or replace columns.
  * **Expression Row Filter**: Filters rows based on a condition.
  * **Variable Expression**: Allows you to create or modify flow variables.


Simply drag and drop one of the nodes from the node repository and connect it.
### General Behaviour 
You can use the KNIME Expression nodes to perform manipulation of your data. The nodes use the KNIME Expression Language that you can find explained in the next section.
You can write your expression in the expression editor, using the input data available from the input panel which can be found on the left side. On the right side you can find the catalog of all the available functions with documentation about their usage. You can filter them and also expand or collapse the available categories.
Most Expression nodes support multiple expressions that will be evaluated in sequence. You can add a new expression by clicking the `Add expression` button. Each single expression editor has a _control bar_ in the top-right corner that allows you to move the expression up or down, duplicate it, or delete it.
Every expression editor has an output section attached below it that allows to configure the settings of the output. Editors will be evaluated from the top to the bottom, so you can use the result of one editor in the next one.
There is a button to evaluate the expression and generate a preview of the result. This is useful to check if the expression is correct and to see the result of the manipulation.
Additionally, the node integrates with the KNIME AI Assistant extension, which offers AI-assisted expression generation and modification, further simplifying the process. By asking K-AI for assistance, you can receive suggestions for expressions based on the column names and column types in your table.
|  Even with K-AI enabled, no data from the table is sent to the AI service.   
---|---  
### Expression node 
You can use the Expression node to perform row-by-row manipulation of your data and add or replace column data.
Open the node configuration dialog and you will see something like the following:
![00 overview expressions](https://docs.knime.com/latest/knime_expressions_guide/img/00_overview_expressions.png)
Figure 1. KNIME Expression node overview
In the _Output column_ section at the bottom of each expression editor you can choose if you want to output the result of the expression in a new column and give the column a desired name or replace the existing column.
|  Find an example on how to use the expression node on KNIME Community Hub.   
---|---  
You can click the _Evaluate first 10 rows_ button or select the amount of rows you want to evaluate by clicking the ![16](https://docs.knime.com/latest/knime_expressions_guide/img/icons/arrow-dropdown.svg) icon. You can choose between 10 (default), 100, 1000.
|  Take into account that this will take more time based on the number of rows to be evaluated.   
---|---  
### Expression Row Filter node 
You can use the Expression Row Filter node to remove rows based on a condition defined in the KNIME Expression Language.
Open the node configuration dialog and you will see something like the following:
![01 overview filter expressions](https://docs.knime.com/latest/knime_expressions_guide/img/01_overview_filter_expressions.png)
Figure 2. KNIME Expression Row Filter node overview
The Expression Row Filter node has a single expression editor where you define the condition for filtering. If the expression evaluates to `FALSE`, the row will be removed from the output table. If it evaluates to `TRUE`, the row will remain in the output. The output of the expression must therefore be a `BOOLEAN` value (for more details regarding types, see the Types section).
If you need to remove rows based on multiple conditions, you can use logical operators like `and`, `or`, and `not` to combine these conditions (see the Logical Operators section for more details and note the example in Figure 2).
You can click the _Evaluate first 10 rows_ button or select the amount of rows you want to evaluate by clicking the ![16](https://docs.knime.com/latest/knime_expressions_guide/img/icons/arrow-dropdown.svg) icon. You can choose between 10 (default), 100, 1000.
|  The limit is applied to the input rows, not the output rows. If any rows are filtered out, the preview will display fewer rows than the set limit. Take into account that the more rows you evaluate, the more time it will take.   
---|---  
### Variable Expression node 
You can use the Variable Expression node to create or modify flow variables using the KNIME Expression Language. This node is useful when you want to create a new flow variable or modify an existing one based on the values of other flow variables. There is no table input for this node.
Open the node configuration dialog and you will see something like the following:
![02 overview flow variable expressions](https://docs.knime.com/latest/knime_expressions_guide/img/02_overview_flow_variable_expressions.png)
Figure 3. KNIME Variable Expression node overview
In the _Output flow variable_ section at the bottom of each expression editor you can choose if you want to output the result of the expression as a new flow variable and give the flow variable a desired name or replace an existing flow variable.
You can click the _Evaluate_ button to evaluate the expression and generate a preview of the result. This is useful to check if the expression is correct and all flow variables are available as expected.
|  As explained later in this guide, the KNIME Expression Language only supports one type of integral numbers (`INTEGER`) while KNIME Analytics Platform supports two types of integral numbers for flow variables: `IntType` and `LongType`. Per default the output of a numerical expression will be of the flow variable type `LongType`. If you want to output a `IntType`, there is a dropdown menu in the output section of each expression editor where you can select the desired type.   
---|---  
## Expression Language 
The KNIME Expression Language is a specialized language designed for data manipulation and analysis within KNIME workflows. Its purpose is to provide an intuitive and efficient way for users to perform calculations, string manipulations, and row or column-based operations without the need for extensive programming knowledge. This document serves as a guide for the syntax, semantics, and usage of the KNIME Expression Language.
### Value types and literals 
The KNIME Expression Language supports several basic value types, each serving a specific kind of data. Some operations are only valid for a subset of value types. This is described where relevant. Every type can be optional, see the section about `MISSING` for more information.
|  ¹ KNIME Expression Language uses slightly different types than KNIME Analytics Platform uses for column types and flow variables. The latter two differ only in the identifier names. In the following the types of the expression language are described and how they map to the combined types noted as [_column type_ , _flow variable type_] in KNIME Analytics Platform.   
---|---  
#### `BOOLEAN`
The value type `BOOLEAN` is used for logical values, that are either true or false. If handling optional values, i.e., type `BOOLEAN | MISSING`, Kleene’s three-valued logic will be applied. For more details see section General rules for comparison operators.
`BOOLEAN` literals are either `TRUE` or `FALSE`.
`BOOLEAN` in the expression language maps to the `[Boolean, BooleanType]`¹ in KNIME Analytics Platform.
#### Number types - `INTEGER` and `FLOAT`
The KNIME Expression Language only supports one type of integral numbers (`INTEGER`) and one type of floating point numbers (`FLOAT`). For simplicity, different precisions are not supported.
|  For operations applied to `INTEGER` and `FLOAT`, such as `5 * 3.14`, the `INTEGER` value is converted to `FLOAT` automatically. This may cause a loss of precision for very large numbers.   
---|---  
##### `INTEGER`
The value type `INTEGER` is used for whole numbers. `INTEGER` literals are written in decimal form as digits between `0` and `9` with optional `_` for visual separation. The first digit cannot be `0` unless the number is `0` itself.
|  The values are represented as 64-bit signed two’s-complement integers resulting in a value range from `-9_223_372_036_854_775_808` to `9_223_372_036_854_775_807` (inclusive).   
---|---  
KNIME Analytics Platform types `[Number (integer), IntType]` and `[Number (long), LongType]`¹ are mapped to `INTEGER` in the expression language without loss of precision. The output of an expression that returns the `INTEGER` expression type will be of the column type `Number (long)`. For flow variables, there is a dropdown menu in the output section of each expression editor where you can select the desired type, i.e., `IntType` or `LongType`.
##### `FLOAT`
The value type `FLOAT` is used for numbers with fractional parts.
A `FLOAT` number is written with a decimal point. The decimal point can be at any position in the number, even at the beginning or end, like `0.123` or `.123` or `123.` You can use underscores `_` to separate digits to make large numbers easier to read, like `1_000.567_890` is the same as `1000.567890`
You can write `FLOAT` numbers using scientific notation, which is useful for very large or very small numbers. In scientific notation, `e` or `E` is used to denote "times ten to the power of". You can also use a plus `+` or minus `-` sign after `e` or `E` to indicate positive or negative exponents, so that `1.23e4` or `1.23E+4` means `1.23 * 10^(+4)` or `12300`
|  The syntax for `FLOAT` literals is similar to the syntax used in in the Python programming language. The values are represented as double-precision floating point numbers (64bit IEEE 754) This results in a value range from `4.9E-324` to `1.8E+308 (inclusive)` and a precision of about 15 decimal digits.   
---|---  
FLOAT in the expression language maps to the `[Number (double), DoubleType]`¹ column type in KNIME Analytics Platform, and has the same precision.
#### `STRING`
The value type `STRING` is used for sequences of Unicode characters (text). The values are represented as a sequence of characters enclosed in double quotes `"text"` or single quotes `'text'`.
|  New lines in strings are permitted, so the string can span multiple lines without using a special character.   
---|---  
```
"multi-line
string"
->
multi-line
string
```

##### Escape sequences 
The backslash `\` can be used for escape sequences. A backslash that does not match one of the following escape sequences is an invalid syntax.
Table 1. Escape sequences Escape sequence | Description | Example  
---|---|---  
`\<newline>` | Backslash and new line in input text ignored | "xyz \ `abc"` → `xyz abc`  
`\\` | Escaping the backslash itself | `"\\something\\"` → \something\  
`\'` | Escaping single quotes | `"\'quoted text\'"` → 'quoted text'  
`\"` | Escaping double quotes | `"\"quoted text\""` → "quoted text"  
`\b` | ASCII backspace causes the cursor to move backwards across the previous character | `"Hello, W\bWorld!"` → Hello, World!  
`\r` | ASCII carriage return causes the cursor to move to the beginning of the line | `"Hello,\rWorld!"` → World!  
`\n` | ASCII linefeed causes the cursor to move to the next line. Note that on unix-like systems, this is the only character used for newlines and on Windows systems, it is used in combination with `\r` | `"Hello,\nWorld!"` → Hello, World!  
`\t` | ASCII horizontal tab causes the cursor to move to the next tab stop | `"Hello,\tWorld!"` → Hello, World!  
`\uxxxx` | Unicode characters can be used as escape sequences. The `xxxx` part is a 16-bit hex value | `"\u0041"` → A `"\u00E4"` → ä `"\u2328"` → ⌨  
Escape sequences are replaced from left to right and the resulting character of an escape sequence cannot be part of another escape sequence.
`STRING` in the expression language maps to `[String, StringType]`¹ in KNIME Analytics Platform.
#### `MISSING`
The value type `MISSING` is used for missing values. It is used to represent the absence of a value in a cell or row, either because the value was missing in the input data or because the value could not be computed.
All types above except for `MISSING` can be extended to allow missing values during evaluation. This is denoted by `<TYPE> | MISSING`, so a column of type `INTEGER | MISSING` can contain both `INTEGER` values and `MISSING` values.
##### Literal `MISSING`
The literal for a missing value is `MISSING`. It is case-sensitive and must be written in uppercase. The literal, i.e., the explicit use of `MISSING` in an expression is not interchangeable with the optional type. So, while `some_function($["column with only MISSING values"])` is valid, `some_function(MISSING)` is not and will result in a syntax error. For an expression that just returns `MISSING` without any further operation also a syntax error will be raised as the type of the expression would not be defined.
### Input data access 
#### Row access 
To retrieve the value from a column in the current row, two syntax options are available:
Use `$["column name"]` for any column name, including those with spaces or special characters. The column name reference between the square brackets follows to rules of `STRING` literals.
For column names consisting solely of letters, numbers, and underscores (without starting with a number), a shorthand syntax `$column_name` is allowed.
Column names are case-sensitive.
  * `$["Customer ID"]` Value of the column “Customer ID”
  * `$["Column with a \"double\" quote"]` Value of the column ‘Column with a “double” quote'
  * `$customer_id` Value of the column “customer_id”


There are also special identifiers to access the
  * `$[ROW_NUMBER]` to get the current row number, starting at 1.
  * `$[ROW_INDEX]` to get the current row index, starting at 0.
  * `$[ROW_ID]` to get the RowID, such as "Row99".


|  The row number, row index, and row ID are not column names and therefore are not enclosed in quotes. Shorthand syntax is not allowed for these special identifiers.   
---|---  
#### Row offsets 
Sometimes it is necessary to access values from other rows in the table to perform calculations. The KNIME Expression Language allows the use of `$["column_name", offset]` to reference previous or next rows relative to the current one.
The offset is a static number and **_must not_** be an expression itself.
Negative offsets point to previous rows, positive offsets to rows next the current row. Replacing a column will only take effect after evaluating the expression for the whole table. This means that the expression only uses the original data from that column.
  * `$["column_name", -1]` Value of the column “column_name” from the previous row


|  Using an offset will necessarily access values from rows that does not exist. In this case, the result will be `MISSING`.   
---|---  
#### Flow Variable access 
Flow variables are accessed using syntax similar to row access, but with two dollar signs:
Use `$$["flow var name"]` for any flow variable name.
For flow variable names consisting only of letters, numbers, and underscores (without starting with a number), a shorthand syntax `$$flow_var_name` is permitted as for column names.
Flow variable names are case-sensitive.
### Operators 
The KNIME Expression Language supports a variety of operators for arithmetic, comparison, and logical operations. The following sections describe the operators available in the language and their respective rules and behaviors.
#### Comments 
Text following a `#` symbol is considered a comment and is ignored by the interpreter. Comments can be used to annotate code for clarity. Comments can be placed on a separate line or at the end of a line of code.
```
# This is a comment
1 + 2 # This is another comment but "1 + 2" is the expression
```

#### Arithmetic 
The following table lists the arithmetic operators available in the KNIME Expression Language, along with their descriptions and typing notes. Arithmetic operations apply also to optional types. If one or both of the operands is missing, the result is missing. For clarity, we omit the optional type in the following table.
Table 2. Arithmetic operators Name | Operator | Description | Typing notes  
---|---|---|---  
Addition | `+` | Yields the sum of two numbers. | Applicable to `INTEGER` and `FLOAT`.  
Subtraction | `-` | Yields the difference of two numbers. Can also be used as a unary operator to negate the operand. | Applicable to `INTEGER` and `FLOAT`.  
Multiplication | `*` | Yields the product of two numbers. | Applicable to `INTEGER` and `FLOAT`.  
Division | `/` | Yields the quotient of two numbers. | Applies to `INTEGER` and `FLOAT`. The result is always of type `FLOAT`.  
Floor Division | `//` | Yields the quotient of two numbers floored to the next `INTEGER` number. | Only applicable to `INTEGER` values.  
Exponentiation | `**` | Yields the power of two numbers. | Applicable to `INTEGER` and `FLOAT`.  
Remainder | `%` | Yields the remainder from the division of the first argument by the second. | Applicable to `INTEGER` and `FLOAT`.  
If both operands are of the same type, the result is of the same type if not specified otherwise for the specific operator. If one or both of the operand types is optional, the result is optional. If the operands are of type `INTEGER` and `FLOAT` (order irrelevant), the `INTEGER` value is converted to the closest value of type `FLOAT`, and the result is of type `FLOAT`.
##### Division by zero 
Dividing a number by zero with the division, floor division or remainder operator yields a runtime warning. The output of the operation is defined via the following rules.
Table 3. Division by zero ruleset Name | Operator | Condition | Output  
---|---|---|---  
Division | `/` | The first operand is 0 | `0. / 0 → NaN`  
|  | Both operands have the same sign | `1. / 0 → INFINITY`  
|  | The operands have different signs | `-1. / 0 → -INFINITY`  
Floor Division | `//` | - | 0 Floor division returns always an `INTEGER`.  
Remainder | `%` | The first operand is of type `FLOAT` | NaN  
|  | Both of the operands are of type `INTEGER` | 0  
#### Comparison 
Comparison operators are used to compare two numeric values (`FLOAT` and `INTEGER`) and yield a `BOOLEAN` result. There are two kinds of comparison operators: ordering and equality.
Comparisons never return optional results. This ensures that optionals are not propagated through comparison. Therefore, it is less likely that the result of an expression is optional.
#### General rules for comparison operators 
Comparison operators compare numeric types, while comparing to `MISSING` always returns `FALSE`. Equality operators work on all types as long as both operands are of the same type or `MISSING`, with the exception that `INTEGER` and `FLOAT` can be checked for equality, too.
Table 4. Comparison operators Name | Operator | Kind | Notes  
---|---|---|---  
Less Than | `<` | Ordering |   
Less Than or Equal To | `<=` | Ordering | Note that `MISSING <= MISSING` is `TRUE`  
Greater Than | `>` | Ordering |   
Greater Than or Equal To | `>=` | Ordering | Note that `MISSING >= MISSING` is `TRUE`  
Equal | `=` or `==` | Equality |   
Not Equal | `!=` or `<>` | Equality | Same as `not (a == b)`  
#### Logical operators 
The logical operators `and`, `or`, and `not` apply to `BOOLEAN` types as well as to their optional `BOOLEAN | MISSING` types. `and` and `or` are binary operators while `not` is a unary operator.
If both of the operands are of type `BOOLEAN`, the result is of type `BOOLEAN`. If the type of one or both of the operands is optional, i.e., `BOOLEAN | MISSING`, missing values are interpreted as unknown according to Kleene’s three-valued logic.
Table 5. Logical operators Name | Operator | Description | Examples  
---|---|---|---  
Logical AND | `and` | Yields `TRUE` if both operands are `TRUE`. Yields `FALSE` if at least one operand is `FALSE` and `MISSING` otherwise . | `TRUE and FALSE` → `FALSE` `TRUE and MISSING` → `MISSING` `FALSE and MISSING` → `FALSE`  
Logical OR | `or` | Yields `FALSE` if both operands are `FALSE`. Yields `TRUE` if at least one operand is `TRUE`, otherwise `MISSING`. | `TRUE or FALSE` → `TRUE` `TRUE or MISSING` → `TRUE` `MISSING or FALSE` → `MISSING`  
Logical NOT | `not` | Yields `TRUE` if the operand is `FALSE`. Yields `FALSE` if the operand is `TRUE` and `MISSING` if the operand is `MISSING`. | `not TRUE` → `FALSE` `not FALSE` → `TRUE` `not MISSING` → `MISSING`  
#### String concatenation 
The operator `+` can also be used for string concatenation if at least one of the operands is of type `STRING` or `STRING | MISSING`.
|  A literal `MISSING` is not a supported type and will result in a syntax error. The output type of a string concatenation is always a `STRING`. Missing values in the input data are mapped to the string `“MISSING”`.   
---|---  
```
"Hello" + " " + "World"           -> "Hello World"
"Hello" + 42                -> "Hello42"
"Hello" + $["column with missing value"]  -> "HelloMISSING"
"Hello" + MISSING              -> Syntax error
```

#### Missing coalescing operator 
The missing coalescing operator `??` is a binary operator that returns the left operand if it is not `MISSING`, otherwise it returns the right operand. Both operands must have the same type and the result is of the same type. If both operands are `MISSING` values, the result is `MISSING`. Even though it will rarely be useful, you can pass `MISSING` as one of the arguments to `??`. However, `MISSING ?? MISSING` is treated as syntax error.
```
1 ?? 2       -> 1
MISSING ?? 2    -> 2
MISSING ?? MISSING -> Syntax error
```

#### Operator precedence 
Operator precedence defines the order in which operations are evaluated in an expression, when it contains more than one operator in series. You can always use parentheses `(`, `)` to enforce a specific order of evaluation. The following table lists the operators in order of precedence, from highest to lowest.
  1. Missing Coalescing (`??`)
  2. Exponentiation (`**`)
  3. Negation (unary `-`)
  4. Multiplication (`*`), Division (`/`), Remainder (`%`), Integer Division (`//`)
  5. Addition (`+`), Subtraction (`-`)
  6. Comparison operators (`<`, `<=`, `>`, `>=`, `!=`, `<>`, `=`, `==`)
  7. Logical NOT (`not`)
  8. Logical AND (`and`)
  9. Logical OR (`or`)


Operations with higher precedence are evaluated before those with lower precedence. Operations with the same precedence level are evaluated from left to right except for `**` which is evaluated from right to left. In the following we give some examples to illustrate the precedence of the operators.
Table 6. Operator precedence examples Expression | With Parenthesis | Result | Explanation  
---|---|---|---  
`1 + 2 * 3` | `1 + (2 * 3)` | `7` | multiplication is evaluated first  
`1 + 2 ** 3 * 4` | `1 + (( 2 ** 3 ) * 4)` | `33` | Exponentiation is evaluated before multiplication and multiplication is evaluated before addition  
`2 * 2 ** 3 ** 2` | `2 * (2 ** (3 ** 2))` | `1024` | Exponentiation is evaluated first and from right to left  
`TRUE or FALSE and FALSE` | `TRUE or (FALSE and FALSE)` | `TRUE` | `and` is evaluated first  
### Functions 
There are two types of functions that are usable in an expression that will be distinguished in the following sections: (1) Row-wise functions that apply row-wise to generate a new value from one or multiple values of each input row and (2) Aggregation functions that apply to a whole column to generate a single value that can be used for each evaluated row.
There is a function catalog available in the editor to help with the selection of functions and their arguments by providing detailed descriptions and examples. You will find built-in Constants there, too.
#### Row-wise functions 
Functions evaluated row-wise are always lower-case and calls are made using the function name followed by parentheses containing any arguments:
`function_name(arg1, arg2, …​)`
Each function has a specific number of arguments and types that it expects. If the arguments do not match the expected types, a type error is raised. The return type of a function is determined by the function itself and is not necessarily the same as the input types.
|  Every function returns some value and there are no `void` functions. Functions can be nested, i.e., a function call can be an argument to another function.   
---|---  
If there are multiple arguments, they must be separated by commas. Each argument can be any valid expression. You may optionally include a trailing comma after the last argument.
**Examples:**
```
sqrt(4)                 -> 2
pow(abs(-sqrt(3.14**2)),2)       -> 3.14
if(TRUE, "true branch","false branch") -> "true branch"
```

#### Aggregation functions 
Aggregation Functions are a special set of functions prefixed with `COLUMN_` that calculate aggregations over whole columns, such as their minimum, maximum, or mean values, for example, `COLUMN_MIN("Column Name")` .
|  The aggregation functions take a string literal `"Column name"` instead of a value from a row (`$["column name"]` or `$column_name`) as input.   
---|---  
In aggregation functions we offer to provide arguments positionally and by name of the argument. Positional arguments are always first, followed by named arguments. Named arguments are always provided as `arg_name=value`.
Let’s illustrate that for the aggregation function `COLUMN_AVERAGE(column, ignore_nan)`
  * Only positional arguments: `COLUMN_AVERAGE("Column Name", TRUE)`
  * Only named arguments: `COLUMN_AVERAGE(column="Column Name",ignore_nan=TRUE)`
  * Mixed arguments: `COLUMN_AVERAGE("Column Name", ignore_nan=TRUE,)`


### Constants 
The KNIME Expression Language provides a set of predefined constants that can be used in expressions. These constants are used to represent common mathematical values and special values. The following constants are predefined and can be used in expressions:
Table 7. Constants Name | Symbol | Type | Description  
---|---|---|---  
Truth value | `TRUE` | BOOLEAN | The boolean value `true`.  
False value | `FALSE` | BOOLEAN | The boolean value `false`.  
Euler’s number `e` | `E` | FLOAT | Euler’s number, ~2.71828, used as the base of natural logarithms and in exponential functions.  
Pi or `π` | `PI` | FLOAT | The constant Pi, ~3.14159, the ratio of a circle’s circumference to its diameter.  
Positive Infinity | `INFINITY` | FLOAT | A special constant representing positive infinity.  
Not a Number | `NaN` | FLOAT | A special constant representing "Not a Number".  
Smallest positive float | `TINY_FLOAT` | FLOAT | The smallest positive float value representable by this computer.  
Largest positive float | `MAX_FLOAT` | FLOAT | The largest positive value that can be represented as a FLOAT.  
Smallest negative float | `MIN_FLOAT` | FLOAT | The smallest negative value that can be represented as a FLOAT.  
Largest positive integer | `MAX_INTEGER` | INTEGER | The largest positive value that can be represented as an INTEGER.  
Smallest negative integer | `MIN_INTEGER` | INTEGER | The smallest negative value that can be represented as an INTEGER.  
Missing value | `MISSING` | MISSING | A special constant representing a missing value.  
  * Introduction
  * Expression nodes
    * General Behaviour
    * Expression node
    * Expression Row Filter node
    * Variable Expression node
  * Expression Language
    * Value types and literals
    * Input data access
    * Operators
    * Functions
    * Constants


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

## Cookie settings
We use cookies and similar technologies to improve user experience and analyze website traffic. For these reasons, we may share your site usage data with our analytics partners. If you do not wish this, click here. For more information read our privacy policy
Accept and close
![](https://docs.knime.com/img/logo.svg) Docs
KNIME Hub Blog Forum Events
  1. KNIME Documentation
  2. KNIME Analytics Platform 5.4 
  3. Create a New Python based KNIME Extension 


KNIME Analytics Platform 5.4
# Create a New Python based KNIME Extension
IntroductionQuickstart TutorialPrerequisitesWriting your first Python node from scratchPython Node Extension SetupDevelopment and distributionDefining a KNIME Node in Python: Full APIDefining custom port objectsNode port configurationSpecifying the node categoryDefining the node’s configuration dialogNode view declarationAccessing flow variablesVersioning your extensionDeprecation of nodesImproving the node description with MarkdownShare your extensionSetupOption 1: Bundling a Python extension to share a zipped update siteOption 2: Publish your extension on KNIME Community HubCustomizing the Python executableRegistering Python extensions during developmentOther TopicsLoggingGateway cachingTroubleshootingFind debug informationHow to update Python versionDevelop multiple extensions at onceErrors during loadColumn is of type long, but int was wantedLZ4/jnijavacpp.dll/Columnar Table Backend errorCould not create instance errorSSL error during executionInstallation TroubleshootingOffline installationCustom conda environment location in case of Windows long path installation problemsProxy IssuesIntroduction
  * Introduction
  * Quickstart Tutorial
    * Prerequisites
    * Writing your first Python node from scratch
  * Python Node Extension Setup
    * Development and distribution
  * Defining a KNIME Node in Python: Full API
    * Defining custom port objects
    * Node port configuration
    * Specifying the node category
    * Defining the node’s configuration dialog
    * Node view declaration
    * Accessing flow variables
    * Versioning your extension
    * Deprecation of nodes
    * Improving the node description with Markdown
  * Share your extension
    * Setup
    * Option 1: Bundling a Python extension to share a zipped update site
    * Option 2: Publish your extension on KNIME Community Hub
  * Customizing the Python executable
  * Registering Python extensions during development
  * Other Topics
    * Logging
    * Gateway caching
  * Troubleshooting
    * Find debug information
    * How to update Python version
    * Develop multiple extensions at once
    * Errors during load
    * Column is of type long, but int was wanted
    * LZ4/jnijavacpp.dll/Columnar Table Backend error
    * Could not create instance error
    * SSL error during execution
  * Installation Troubleshooting
    * Offline installation
    * Custom conda environment location in case of Windows long path installation problems
    * Proxy Issues


Download PDF
## Introduction 
As explained in the Extensions and Integrations Guide, KNIME Analytics Platform can be enhanced with additional functionality provided by a vast array of extensions and integrations. Often, installing an extension adds a collection of new nodes to the node repository of KNIME Analytics Platform.
With the v4.6 release of KNIME Analytics Platform, we introduce the possibility to write KNIME node extensions completely in Python. This includes the ability to define node configuration and execution, as well as dialog definition. A Pythonic API to design those nodes is now available, as well as debugging functionality within KNIME Analytics Platform. This means deploying pure-Python KNIME extensions containing nodes – including their Python environment needed for execution – using a locally built update site is now possible.
In this guide, we offer a tutorial to get you started with writing your KNIME nodes using Python, as well as how to setup a shareable Python extension containing your nodes, together with a complete definition of the API.
## Quickstart Tutorial 
This section provides a basic extension template, and walks you through the essential development steps to help you get started with using the API.
### Prerequisites 
  1. Set up `conda`.
To get started with developing Python node extensions, you need to have `conda` installed. Here is the quickest way:
     * Go to the Miniconda website
     * Download the appropriate installer for your OS
     * For Windows and macOS: run the installer executable
     * For Linux: execute the script in terminal (see here for help)
  2. With `conda` set up, extract basic.zip to a convenient location.
In the `basic` folder, you should see the following file structure:
```
.
├── tutorial_extension
│   │── icon.png
│   │── knime.yml
│   │── LICENSE.TXT
│   └── my_extension.py
├── config.yml
├── my_conda_env.yml
├── Example_with_Python_node.knwf
└── README.md
```

  3. During development, you can edit the source files in any text editor. However, in order to make use of autocompletion for the API, as well as to allow debugging via the `debugpy` package, we recommend using an editor that is able to set `conda` environments as the Python interpreter. Here are the setup steps for **Visual Studio Code** :
     * Download and install Visual Studio Code
     * Install the Python extension
![02 vscode python extension](https://docs.knime.com/latest/pure_python_node_extensions_guide/img/02_vscode_python_extension.png)
     * In the bottom right corner of the editor, you should be able to select the Python interpreter that you would like to use during development. After Step 4 of Tutorial 1, you will have the `my_python_env` environment available in the list of Python interpreters. By selecting the environment, you will be able to make full use of autocompletion.
![02 select python interpreter](https://docs.knime.com/latest/pure_python_node_extensions_guide/img/02_select_python_interpreter.png)


### Writing your first Python node from scratch 
This is a quickstart guide that will walk you through the essential steps of writing and running your first Python node extension containing a single node. We will use `tutorial_extension` as the basis. The steps of the tutorial requiring modification of the Python code in `my_extension.py` have corresponding comments in the file, for convenience.
For an extensive overview of the full API, please refer to the Defining a KNIME Node in Python: Full API section, as well as our Read the Docs page.
  1. Install KNIME Analytics Platform version 4.6.0 or higher.
  2. Go to _File_ → _Install KNIME Extensions…_ , enter ''Python'' in the search field, and look for _KNIME Python Extension Development (Labs)_. Alternatively, you can manually navigate to the _KNIME Labs Extensions_ category and find the extension there. Select it and proceed with installation.
  3. The `tutorial_extension` will be your new extension. Familiarize yourself with the files contained in that folder, in particular:
     * `knime.yml`, which contains important metadata about your extension.
     * `my_extension.py`, which contains Python definitions of the nodes of your extension.
     * `config.yml`, just outside of the folder, which contains the information that binds your extension and the corresponding `conda`/Python environment with KNIME Analytics Platform.
  4. Create a `conda`/Python environment containing the `knime-python-base` metapackage, together with the node development API `knime-extension` for the KNIME Analytics Platform you are using. If you are using `conda`, you can create the environment by running the following command in your terminal (macOS/Linux) or Anaconda Prompt (Windows):
```
conda create -n my_python_env python=3.11 knime-python-base=5.4 knime-extension=5.4 -c knime -c conda-forge
```

If you would like to install the packages into an environment that already exists you can run the following command _from within that environment_ :
```
conda install knime-python-base=5.4 knime-extension=5.4 -c knime -c conda-forge
```

Note that you **must** append both the `knime` and `conda-forge` channels to the commands to install the mandatory packages. To install additional packages, for your specific use case, we recommend using the `conda-forge` channel.
```
conda install -c conda-forge <additional_pkg_name>
```

  5. Edit the `config.yml` file located just outside of the `tutorial_extension` (for this example, the file already exists with prefilled fields and values, but you would need to manually create it for future extensions that you develop). The contents should be as follows:
```
<extension_id>:
  src: <path/to/folder/of/template>
  conda_env_path: <path/to/my_python_env>
  debug_mode: true
```

where:
     * `<extension_id>` should be replaced with the `group_id` and `name` values specified in `knime.yml`, combined with a dot.
For our example extension, the value for `group_id` is `org.tutorial`, and the value for `name` is `first_extension`, therefore the `<extension_id>` placeholder should be replaced with `org.tutorial.first_extension`.
     * The `src` field should specify the path to the `tutorial_extension` folder.
For instance, `/Users/Bobby/Development/python_extensions/tutorial_extension`
     * Similarly, the `conda_env_path` field should specify the path to the `conda`/Python environment created in Step 4. To get this path, run the `conda env list` command in your Terminal/Anaconda Prompt, and copy the path displayed next to the appropriate environment (`my_python_env` in our case).
     * The `debug_mode` is an optional field, which, if set to `true`, will tell KNIME Analytics Platform to use the latest changes in the `configure` and `execute` methods of your Python node class whenever those methods are called.
|  Enabling `debug_mode` will affect the responsiveness of your nodes.   
---|---  
  6. We need to let KNIME Analytics Platform know where the `config.yml` is in order to allow it to use our extension and its Python environment. To do this, you need to edit the `knime.ini` of your KNIME Analytics Platform installation, which is located at `<path-to-your-KAP>/knime.ini`.
Append the following line to the end, and modify it to have the correct path to `config.yml`:
`-Dknime.python.extension.config=<path/to/your/config.yml>`
|  The forward slash `/` has to be used on all OS, also on Windows.   
---|---  
  7. Start your KNIME Analytics Platform.
  8. The ''My Template Node'' node should now be visible in the Node Repository.
  9. Import and open the `Example_with_Python_node.knwf` workflow, which contains our test node:
    1. Get familiar with the table.
    2. Study the code in `my_extension.py` and compare it with the node you see in KNIME Analytics Platform. In particular, understand where the node name and description, as well as its inputs and outputs, come from.
    3. Execute the node and make sure that it produces an output table.
  10. Build your first configuration dialog:
In `my_extension.py`, uncomment the definitions of parameters (marked by the ''Tutorial Step 10'' comment). Restart your KNIME Analytics Platform, re-drag your node from the node repository into the workflow, and you should be able to double-click the node and see configurable parameter.
Take a minute to see how the names, descriptions, and default values compare between their definitions in `my_extension.py` and the node dialog.
  11. Add your first port:
To add a second input table to the node, follow these steps (marked by the ''Tutorial Step 11'' comment; you will need to restart KNIME Analytics Platform):
    1. Uncomment the `@knext.input_table` decorator.
    2. Change the `configure` method’s definition to reflect the changes in the schema.
    3. Change the `execute` method to reflect the addition of the second input table.
  12. Add some functionality to the node:
With the following steps, we will append a new column to the first table and output the new table (the lines requiring to be changed are marked by the ''Tutorial Step 12'' comment):
    1. To inform downstream nodes of the changed schema, we need to change it in the return statement of the `configure` method; for this, we append metadata about a column to the output schema.
    2. Everything else is done in the `execute` method:
       * we transform both input tables to pandas dataframes and append a new column to the first dataframe
       * we transform that dataframe back to a KNIME table and return it
  13. Use your parameters:
    1. In the `execute` method, uncomment the lines marked by the ''Tutorial Step 13'' comment.
    2. Use a parameter to change some table content; we will use a lambda function for a row-wise multiplication using the `double_param` parameter.
  14. Start logging and setting warnings:
Uncomment the lines marked by "Tutorial Step 14" in the `execute` method:
    1. Use the `LOGGER` functionality to inform users, or for debugging.
    2. Use the `execute_context.set_warning("A warning")` to inform users about unusual behaviour.
    3. If you want the node to fail, you can raise an exception. For instance: `raise ValueError("This node failed just because")`.
  15. Congratulations, you have built your first functioning node entirely in Python!


## Python Node Extension Setup 
A Python node extension needs to contain a YAML file called `knime.yml` that gives general information about the node extension, which Python module to load, and what conda environment should be bundled with the extension.
knime.yml:
```
name: myextension # Will be concatenated with the group_id to form the extension ID
author: Jane Doe
env_yml_path:  # Path to the Conda environment yml, from which the environment for this extension will be built when bundling
extension_module: my_extension # The .py Python module containing the nodes of your extension
description: My New Extension # Human readable bundle name / description
long_description: This extension provides functionality that everyone wants to have. # Text describing the extension (optional)
group_id: org.knime.python3.nodes # Will be concatenated with the name to form the extension ID
version: 0.1.0 # Version of this Python node extension. Must use three-component semantic versioning for deployment to work.
vendor: KNIME AG, Zurich, Switzerland # Who offers the extension
license_file: LICENSE.TXT # Best practice: put your LICENSE.TXT next to the knime.yml; otherwise you would need to change to path/to/LICENSE.txt
#Optional: If you do not have dependencies on other extensions, you do not need feature_depencendies and their entries
feature_dependencies:
 - org.knime.features.chem.types 5.4.0 # If you want to specify the version - note that this specifies the version being greater equal 5.4.0
 - org.knime.features.chem.types    # If the version does not matter
```

The `id` of the extension will be of the form `group_id.name`. It needs to be a unique identifier for your extension, so it is a good idea to encode your username or company’s URL followed by a logical structure as `group_id` in order to prevent `id` clashes. For example, a developer from KNIME could encode its URL to `org.knime` and add `python3` to indicate that the extension is a member of `nodes`, which are part of `python3`.
Feature dependencies: if your extension depends on another extension, you can specify it as a bullet point of `feature_dependencies`. Optionally, you can add a specific minimum version to it.
Example: You use data types like `SmilesValue` of the `KNIME Base Chemistry Types & Nodes` extension in your extension. You have that extension already installed and want to make sure that everybody who uses your extension will also have this extension installed. Then you can go to _Help > About KNIME Analytics Platform > Installation Details_ and check the id of `KNIME Base Chemistry Types & Nodes`, which is `org.knime.features.chem.types.feature.group`. Take the id without `.feature.group` and you have the string of the feature dependency: `org.knime.features.chem.types`
Note that the `env_yml_path` field, which specified the path to the YAML configuration of the `conda` environment required by your extension, is needed when bundling your extension. During development, KNIME Analytics Platform uses the environment specified in the `config.yml` file.
The path containing the `knime.yml` will then be put on the `Pythonpath`, and the extension module specified in the YAML will be imported by KNIME Analytics Platform using `import <extension_module>`. This Python module should contain the definitions of KNIME nodes. Each class decorated with `@knext.node` within this file will become available in KNIME Analytics Platform as a dedicated node.
**Recommended project folder structure:**
```
.
├── my_extension
│  ├── icons
│  │  └── my_node_icon.png
│  ├── knime.yml
│  ├── LICENSE.txt
│  ├── my_conda_env.yml
│  └── my_extension.py
└── config.yml
```

See Tutorial 1 above for an example.
### Development and distribution 
As you develop your Python extension, you are able to run and debug it locally by setting the `knime.python.extension.config` system property in your KNIME Analytics Platform’s `knime.ini` to point to the `config.yml`, or in the launch configuration’s VM arguments in Eclipse. See the Registering Python extensions during development and Customizing the Python executable sections at the end of this guide for more information.
In order to share your Python extension with others, please refer to the Bundling your Python Extension Nodes section.
## Defining a KNIME Node in Python: Full API 
We provide a `conda` package that includes the full API for node development in Python - `knime-extension` (see Tutorial 1 for help in setting up your development `conda` environment). To enable helpful code autocompletion via `import knime.extension as knext`, make sure your IDE of choice’s Python interpreter is configured to work in that `conda` environment when you are developing your Python node extension (see here for help with Visual Studio Code, here for PyCharm, here for Sublime Text, or here for general information on integrating your IDE with `conda`).
A Python KNIME node needs to implement the `configure` and `execute` methods, so it will generally be a class. The node description is _automatically generated from the docstrings_ of the class and the `execute` method. The node’s location in KNIME Analytics Platform’s _Node Repository_ , as well as its icon, are specified in the `@knext.node` decorator.
A simple example of a node does nothing but pass an input table to its output unmodified. Below, we define a class `MyNode` and indicate that it is a KNIME node by decorating it with `@knext.node`. We then ''attach'' an input table and an output table to the node by decorating it with `@knext.input_table` and `@knext.output_table` respectively. Finally, we implement the two required methods, `configure` and `execute`, which simply return their inputs unchanged.
```
import knime.extension as knext
@knext.node(name="My Node", node_type=knext.NodeType.MANIPULATOR, icon_path="..icons/icon.png", category="/")
@knext.input_table(name="Input Data", description="The data to process in my node")
@knext.output_table("Output Data", "Result of processing in my node")
class MyNode:
  """Short description is in the first line next to the three double quotes here. It it displayed in overviews when a whole category in the node repository is selected.
  Here begins the normal description: This node description will be displayed in KNIME Analytics Platform.
  """
  def configure(self, config_context, input_table_schema):
    return input_table_schema
  def execute(self, exec_context, input_table):
    return input_table
```

`@knext.node’s configuration options are:
  * **name** : the name of the node in KNIME Analytics Platform.
  * **node_type** : the type of the node, one of:
    * `knext.NodeType.MANIPULATOR`: a node that manipulates data.
    * `knext.NodeType.LEARNER`: a node learning a model that is typically consumed by a PREDICTOR.
    * `knext.NodeType.PREDICTOR`: a node that predicts something typically using a model provided by a LEARNER.
    * `knext.NodeType.SOURCE`: a node producing data.
    * `knext.NodeType.SINK`: a node consuming data.
    * `knext.NodeType.VISUALIZER`: a node that visualizes data.
    * `knext.NodeType.OTHER`: a node that doesn’t fit any of the other node types.
  * **icon_path** : module-relative path to a 16x16 pixel PNG file to use as icon.
  * **category** : defines the path to the node inside KNIME Analytics Platform’s _Node Repository_.


### Defining custom port objects 
Besides tables, a node can also consume or produce other port objects and it is possible to define custom port objects for your extension. You can do so by extending `knext.PortObject` and `knext.PortObjectSpec` with your custom implementation. In order to use these objects in your node, you need to define a custom port type via the `knext.port_type` function that takes your `PortObject` and `PortObjectSpec` classes as well as a human-readable name for your port type and an optional id. Here is an example:
Let’s start with the `PortObjectSpec`:
```
import knime.extension as knext
class MyPortObjectSpec(knext.PortObjectSpec):
  def __init__(self, spec_data: str) -> None:
    super().__init__()
    self._spec_data = spec_data
  def serialize(self) -> dict:
    return {"spec_data": self._spec_data}
  @classmethod
  def deserialize(cls, data: dict) -> "MyPortObjectSpec":
    return cls(data["spec_data"])
  @property
  def spec_data(self) -> str:
    return self._data
```

The `serialize` and `deserialize` methods are used by the framework to store and load the spec.
_Note_ : The `deserialize` method must be a `classmethod`.
The `spec_data` property is just an example for custom code and you can add arbitrary methods to your spec class as you see fit.
Next we implement the `PortObject`:
```
import pickle
class MyPortObject(knext.PortObject):
  def __init__(self, spec: MyPortObjectSpec, model) -> None:
    super().__init__(spec)
    self._model = model
  def serialize(self) -> bytes:
    return pickle.dumps(self._model)
  @classmethod
  def deserialize(cls, spec: MyPortObjectSpec, data: bytes) -> "MyPortObject":
    return cls(spec, pickle.loads(data))
  def predict(self, data):
    return self._model.predict(data)
```

The PortObject class must have a `serialize` and `deserialize` method that are called by the framework to persist and restore the object. Again note that `deserialize` has to be a `classmethod`.
The `predict` property is again just an example for custom code that your port object class may contain.
Finally, we create a custom port type to be used as input or output of a node:
```
my_model_port_type = knext.port_type(name="My model port type", object_class=MyPortObject, spec_class=MyPortObjectSpec)
```

The `knext.port_type` method ties the `PortObject` and `PortObjectSpec` together and provides a human-readable name to refer to the custom port type.
It is also possible to specify a custom ID for the port type via the `id` argument. Note that the id must be unique! If you don’t provide a custom ID, then the framework generates one of the format `your_extension_id.your_module_name.your_port_object_class_name`. For example if your extension has the id `org.company.extension` and you implement a `MyPortObject` in the module `my_extension`, then the generated id is `org.company.extension.my_extension.MyPortObject`.
Note that there are also connection port objects that can hold non-serializable objects. You can find information about that in the API documentation for `knime.extension.ConnectionPortObject`.
Check out the next section to learn how to declare your custom port type as input or output of your node.
### Node port configuration 
The input and output ports of a node can be configured by decorating the node class with `@knext.input_table`, `@knext.input_port`, and respectively `@knext.output_table` and `@knext.output_port`. An image output port can be added with the `@knext.output_image` decorator. Additionally, a node producing a view should be decorated with the `@knext.output_view` decorator.
These port decorators have the following properties:
  * they take `name` and `description` arguments, which will be displayed in the node description area inside KNIME Analytics Platform;
  * they must be positioned _after_ the `@knext.node` decorator and _before_ the decorated object (e.g. the node class);
  * their order determines the order of the port connectors of the node in KNIME Analytics Platform.


The `@knext.input_table` and `@knext.output_table` decorators configure the port to consume and respectively produce a KNIME table. The `@knext.output_image` decorator configures the port to produce a PNG or SVG image.
If you want to receive or send other data, e.g. a trained machine learning model, use `@knext.input_port` and `@knext.output_port`. These decorators _have an additional argument_ , `port_type`, used to identify the type of port objects going along this port connection. Only ports with equal `port_type` can be connected. See the previous section to learn how to specify your own port type.
The port configuration determines the expected signature of the `configure` and `execute` methods:
  * In the `configure` method, the first argument is a `ConfigurationContext`, followed by one argument per input port. The method is expected to return **as many parameters as it has output ports configured**. The argument and return value types corresponding to the input and output ports are:
    * for **table** ports, the argument/return value must be of type `knext.Schema`. If the return table consists of only one column, the return value can also be of type `knext.Column`;
    * for **image** ports, the argument/return value must be of type `knext.ImagePortObjectSpec` with the appropriate image format configured
    * for **custom** ports, the argument/return value must be of your custom implementation of `knext.PortObjectSpec`. If we take the example from the previous section, the type would be `MyPortObjectSpec`.
Note that the order of the arguments and return values must match the order of the input and output port declarations via the decorators.
  * The arguments and expected return values of the `execute` method follow the same schema: one argument per input port, one return value per output port. For image outputs the returned value must be of the type `bytes`.


Examples how to use `knext.Schema` and `knext.Column`` (see the API):
```
def configure(self, config_context): # no input table
 """ This node creates a table with a single column """
 ktype = knext.string()
 # OR
 ktype = knext.int32() # OR knext.double(), knext.bool_(), knext.list_(knext.string()), knext.struct(knext.int64(), knext.bool_()),...
 # OR
 import datetime
 ktype = datetime.datetime
 return knext.Column(ktype, "Date and Time")
```

```
def configure(self, config_context): # no input table
 """ This node creates two tables with two columns each """
 ktype1 = knext.string()
 import knime.types.chemistry as cet # needs the extension `KNIME Base Chemistry Types & Nodes` installed
 ktype2 = cet.SdfValue
 schema1 = knext.Schema([ktype1, ktype2], ["Column with Strings", "Column with Sdf"])
 schema2 = knext.Schema([ktype1, ktype2], ["Another column with Strings", "Another column with Sdf"])
 return schema1, schema2
```

|  All supported types of your current environment can be obtained by printing `knime.api.schema.supported_value_types()` or `knime.extension.supported_value_types()``.   
---|---  
Here is an example with two input ports and one output port. See the previous session for the definitions of `MyPortObject`, `MyPortObjectSpec` and `my_model_port_type`.
```
@knext.node("My Predictor", node_type=knext.NodeType.PREDICTOR, icon_path="icon.png", category="/")
@knext.input_port("Trained Model", "Trained fancy machine learning model", port_type=my_model_port_type)
@knext.input_table("Data", "The data on which to predict")
@knext.output_table("Output", "Resulting table")
class MyPredictor():
  def configure(self, config_context: knext.ConfigurationContext, input_spec: MyPortObjectSpec, table_schema: knext.Schema) -> knext.Schema:
    # We will add one column of type double to the table
    return table_schema.append(knext.Column(knext.double(), "Predictions"))
    # If you want to use types known to KNIME, but that have no dedicated KNIME type, you could use:
    # import datetime
    # return table_schema.append(knext.Column(datetime.datetime, "Date and Time"))
  def execute(self, exec_context: knext.ExecutionContext, trained_model: MyPortObject, input_table: knext.Table) -> knext.Table:
    predictions = trained_model.predict(input_table.to_pandas())
    output_table = input_table
    output_table["Predictions"] = predictions
    return knext.Table.from_pandas(output_table)
```

Example with two image output ports.
```
@knext.node("My Image Generator", node_type=knext.NodeType.SOURCE, icon_path="icon.png", category="/")
@knext.output_image(name="PNG Output Image", description="An example PNG output image")
@knext.output_image(name="SVG Output Image", description="An example SVG output image")
class ImageNode:
  def configure(self, config_context):
    return (
      knext.ImagePortObjectSpec(knext.ImageFormat.PNG),
      knext.ImagePortObjectSpec(knext.ImageFormat.SVG),
    )
  def execute(self, exec_context):
    x = [1, 2, 3, 4, 5]
    y = [1, 2, 3, 4, 5]
    fig, ax = plt.subplots(figsize=(5, 5), dpi=100)
    ax.plot(x, y)
    buffer_png = io.BytesIO()
    plt.savefig(buffer_png, format="png")
    buffer_svg = io.BytesIO()
    plt.savefig(buffer_svg, format="svg")
    return (
      buffer_png.getvalue(),
      buffer_svg.getvalue(),
    )
```

Alternatively, you can populate the `input_ports` and `output_ports` attributes of your node class (on class or instance level) for more fine grained control.
### Specifying the node category 
Each node in your Python node extension is assigned a category via the `category` parameter of the `@knext.node` decorator, which dictates where the node will be located in the node repository of KNIME Analytics Platform. Without an explicit category, the node will be placed in the root of the node repository, thus you should **always** specify a category for each node.
In order to define a custom category for the nodes of your extension, you can use the `knext.category` helper function. If autocompletion is enabled in your IDE, you should be able to see the list of the expected parameters of the function, together with their detailed description.
If you are a _community developer_ , you should use the **Community Nodes** category as the parent category of your Python node extensions. This is done by specifying the `path="/community"` parameter of the `knext.category` function:
```
import knime.extension as knext
my_category = knext.category(
  path="/community",
  level_id="my_extension",
  name="My Extension",
  description="My Python Node Extension.",
  icon="icon.png",
)
@knext.node(
  name="My Node",
  node_type=knext.NodeType.PREDICTOR,
  icon_path="icon.png",
  category=my_category
)
...
class MyNode():
  ...
.
```

![04 single category](https://docs.knime.com/latest/pure_python_node_extensions_guide/img/04_single_category.png)
Note that it is possible to further split your custom category into subcategories. This is useful if, for instance, nodes of your extension can be grouped based on their functionality. By first defining a parent category for the node extension, you can then use it as the `path` parameter when defining the subcategories:
```
import knime.extension as knext
# define the category and its subcategories
main_category = knext.category(
  path="/community",
  level_id="my_extension",
  name="scikit-learn Extension",
  description="Nodes implementing various scikit-learn algorithms.",
  icon="icon.png",
)
supervised_category = knext.category(
  path=main_category,
  level_id="supervised_learning",
  name="Supervised Learning",
  description="Nodes for supervised learning.",
  icon="icon.png",
)
unsupervised_category = knext.category(
  path=main_category,
  level_id="unsupervised_learning",
  name="Unsupervised Learning",
  description="Nodes for unsupervised learning.",
  icon="icon.png",
)
# define nodes of the extension
@knext.node(
  name="Logistic Regression Learner",
  node_type=knext.NodeType.SINK,
  icon_path="icon.png",
  category=supervised_category
)
...
class LogisticRegressionLearner():
  ...

@knext.node(
  name="SVM Learner",
  node_type=knext.NodeType.SINK,
  icon_path="icon.png",
  category=supervised_category
)
...
class SVMLearner():
  ...

@knext.node(
  name="K-Means Learner",
  node_type=knext.NodeType.SINK,
  icon_path="icon.png",
  category=unsupervised_category
)
...
class KMeansLearner():
  ...

@knext.node(
  name="PCA Learner",
  node_type=knext.NodeType.SINK,
  icon_path="icon.png",
  category=unsupervised_category
)
...
class PCALearner():
  ...
.
```

![04 multiple categories](https://docs.knime.com/latest/pure_python_node_extensions_guide/img/04_multiple_categories.png)
### Defining the node’s configuration dialog 
|  For the sake of brevity, in the following code snippets we omit repetitive portions of the code whose utility has already been established and demonstrated earlier.   
---|---  
In order to add parameterization to your node’s functionality, we can define and customize its configuration dialog. The user-configurable parameters that will be displayed there, and whose values can be accessed inside the `execute` method of the node via `self.param_name`, are set up using the following parameter classes available in `knext`:
  * `knext.IntParameter` for integer numbers:
    * Signature:
```
knext.IntParameter(
  label=None,
  description=None,
  default_value=0,
  min_value=None,
  max_value=None,
  since_version=None,
)
```

    * Definition within a node/parameter group class:
```
no_steps = knext.IntParameter("Number of steps", "The number of repetition steps.", 10, max_value=50)
```

    * Usage within the `execute` method of the node class:
```
for i in range(self.no_steps):
  # do something
```

  * `knext.DoubleParameter` for floating point numbers:
    * Signature:
```
knext.DoubleParameter(
  label=None,
  description=None,
  default_value=0.0,
  min_value=None,
  max_value=None,
  since_version=None,
)
```

    * Definition within a node/parameter group class:
```
learning_rate = knext.DoubleParameter("Learning rate", "The learning rate for Adam.", 0.003, min_value=0.)
```

    * Usage within the `execute` method of the node class:
```
optimizer = torch.optim.Adam(lr=self.learning_rate)
```

  * `knext.StringParameter` for string parameters and single-choice selections:
    * Signature:
```
knext.StringParameter(
  label=None,
  description=None,
  default_value="",
  enum: List[str] = None,
  since_version=None,
)
```

    * Definition within a node/parameter group class:
```
# as a text input field
search_term = knext.StringParameter("Search term", "The string to search for in the text.", "")
# as a single-choice selection
selection_param = knext.StringParameter("Selection", "The options to choose from.", "A", enum=["A", "B", "C", "D"])
```

    * Usage within the `execute` method of the node class:
```
table[table["str_column"].str.contains(self.search_term)]
```

  * `knext.BoolParameter` for boolean parameters:
    * Signature:
```
knext.BoolParameter(
  label=None,
  description=None,
  default_value=False,
  since_version=None,
)
```

    * Definition within a node/parameter group class:
```
output_image = knext.BoolParameter("Enable image output", "Option to output the node view as an image.", False)
```

    * Usage within the `execute` method of the node class:
```
if self.output_image is True:
  # generate an image of the plot
```

  * `knext.ColumnParameter` for a single column selection:
    * Signature:
```
knext.ColumnParameter(
  label=None,
  description=None,
  port_index=0, # the port from which to source the input table
  column_filter: Callable[[knext.Column], bool] = None, # a (lambda) function to filter columns
  include_row_key=False, # whether to include the table Row ID column in the list of selectable columns
  include_none_column=False, # whether to enable None as a selectable option, which returns "<none>"
  since_version=None,
)
```

    * Definition within a node/parameter group class:
```
selected_col = knext.ColumnParameter(
  "Target column",
  "Select the column containing country codes.",
  column_filter= lambda col: True if "country" in col.name else False,
  include_row_key=False,
  include_none_column=True,
)
```

    * Usage within the `execute` method of the node class:
```
if self.selected_column != "<none>":
  column = input_table[self.selected_column]
  # do something with the column
```

  * `knext.MultiColumnParameter` for a multiple column selection
    * Signature:
```
knext.MultiColumnParameter(
  label=None,
  description=None,
  port_index=0, # the port from which to source the input table
  column_filter: Callable[[knext.Column], bool] = None, # a (lambda) function to filter columns
  since_version=None,
)
```

    * Definition within a node/parameter group class:
```
selected_columns = knext.MultiColumnParameter(
  "Filter columns",
  "Select the columns that should be filtered out."
)
```

    * Setup within the `configure` method of the node class:
```
# the multiple column selection parameter needs to be provided the list of columns of an input table
self.selected_columns = input_schema_1.column_names
```

    * Usage within the `execute` method of the node class:
```
for col_name in self.selected_columns:
  # drop the column from the table
```



All of the above have arguments `label` and `description`, which are displayed in the node description in KNIME Analytics Platform, as well as in the configuration dialog itself. Additionally, all parameter classes have an optional argument `since_version`, which can be used to specify the version of the extension that the parameter was introduced in. Please refer to the Versioning your extension section below for a more detailed overview.
Parameters are defined in the form of class attributes inside the node class definition (similar to Python descriptors):
```
@knext.node(…)
…
class MyNode:
  num_repetitions = knext.IntParameter(
    label="Number of repetitions",
    description="How often to repeat an action",
    default_value=42
  )
  def configure(…):
    …
  def execute(…):
    …
```

While each parameter type listed above has default type validation, they also support custom validation via a property-like decorator notation. By wrapping a function that receives a tentative parameter value, and raises an exception should some condition be violated, with the `@some_param.validator` decorator, you are able to add an additional layer of validation to the parameter `some_param`. This should be done _below_ the definition of the parameter for which you are adding a validator, and _above_ the `configure` and `execute` methods:
```
@knext.node(…)
…
class MyNode:
  num_repetitions = knext.IntParameter(
    label="Number of repetitions",
    description="How often to repeat an action",
    default_value=42
  )
  @num_repetitions.validator
  def validate_reps(value):
    if value > 100:
      raise ValueError("Too many repetitions!")
  def configure(…):
    …
  def execute(…):
    …
```

#### Parameter Visibility 
By default, each parameter of a node is visible in the node’s configuration dialog. Parameters can be marked as advanced by setting `is_advanced=True`, which will only show them once the user has clicked “Show advanced settings” in the configuration dialog.
Sometimes a parameter should only be visible to the user if another parameter has a certain value. For this, each parameter type listed above has a method `rule`. In this method, one can specify a condition based on another parameter, and the effect that should be applied to this parameter when the condition becomes true.
```
@knext.node(args)
class MyNode:
  string_param = knext.StringParameter(
    "String Param Title",
    "String Param Title Description",
    "default value"
  )
  # this parameter gets disabled if string_param is "foo" or "bar"
  int_param = knext.IntParameter(
    "Int Param Title",
    "Int Param Description",
  ).rule(knext.OneOf(string_param, ["foo", "bar"]), knext.Effect.DISABLE)
```

|  Currently this only supports conditions where another parameter exactly matches a value. Rules can only depend on parameters on the same level, not in a child or parent parameter group.   
---|---  
See the full API documentation of the `rule` method here.
#### Parameter Groups 
It is also possible to define groups of parameters, which are displayed as separate sections in the configuration dialog UI. By using the `@knext.parameter_group` decorator with a dataclass-like class definition, you are able to encapsulate parameters and, optionally, their validators into a separate entity outside of the node class definition, keeping your code clean and maintainable. A parameter group is linked to a node just like an individual parameter would be:
```
@knext.parameter_group(label="My Settings")
class MySettings:
  name = knext.StringParameter("Name", "The name of the person", "Bario")
  num_repetitions = knext.IntParameter("NumReps", "How often do we repeat?", 1, min_value=1)
  @num_repetitions.validator
  def reps_validator(value):
    if value == 2:
      raise ValueError("I don't like the number 2")

@knext.node(…)
…
class MyNodeWithSettings:
  settings = MySettings()
  def configure(…):
    …
  def execute(…):
    …
    name = self.settings.name
    …
```

Another benefit of defining parameter groups is the ability to provide group validation. As opposed to only being able to validate a single value when attaching a validator to a parameter, group validators have access to the values of all parameters contained in the group, allowing for more complex validation routines.
We provide two ways of defining a group validator, with the `values` argument being a dictionary of `parameter_name` : `parameter_value` mappings:
  1. by implementing a `validate(self, values)` method inside the parameter group class definition:
```
@knext.parameter_group(label=''My Group'')
class MyGroup:
  first_param = knext.IntParameter(''Simple Int'',''Testing a simple int param'', 42)
  second_param = knext.StringParameter("Simple String", "Testing a simple string param", "foo")
 def validate(self, values):
   if values["first_param"] < len(values["second_param"]):
     raise ValueError("Params are unbalanced!")
```

  2. by using the familiar `@group_name.validator` decorator notation with a validator function inside the class definition of the ``parent'' of the group (e.g. the node itself, or a different parameter group):
```
@knext.parameter_group(label=``My Group'')
class MyGroup:
  first_param = knext.IntParameter(``Simple Int'', ``Testing a simple int param'', 42)
  second_param = knext.StringParameter("Simple String", "Testing a simple string param", "foo")
@knext.node(…)
…
class MyNode:
  param_group = MyGroup()
  @param_group.validator
  def validate_param_group(values):
    if values["first_param"] < len(values["second_param"]):
      raise ValueError("Params are unbalanced!")
```

|  If you define a validator using the first method, and then define another validator for the same group using the second method, the second validator will **override** the first validator. If you would like to keep **both** validators active, you can pass the optional `override=False` argument to the decorator: `@param_group.validator(override=False)`.   
---|---  


Intuitively, parameter groups can be nested inside other parameter groups, and their parameter values accessed during the parent group’s validation:
```
@knext.parameter_group(label="Inner Group")
class InnerGroup:
  inner_int = knext.IntParameter("Inner Int", "The inner int param", 1)
@knext.parameter_group(label="Outer Group")
class OuterGroup:
  outer_int = knext.IntParameter("Outer Int", "The outer int param", 2)
  inner_group = InnerGroup()
  def validate(self, values):
    if values["inner_group"]["inner_int"] > values["outer_int"]:
      raise ValueError("The inner int should not be larger than the outer!")
```

### Node view declaration 
You can use the `@knext.output_view(name="", description="")` decorator to specify that a node returns a view. In that case, the `execute` method should return a tuple of port outputs and the view (of type `knime.api.views.NodeView`).
```
from typing import List
import knime.extension as knext
import seaborn as sns

@knext.node(name="My Node", node_type=knext.NodeType.VISUALIZER, icon_path="icon.png", category="/")
@knext.input_table(name="Input Data", description="We read data from here")
@knext.output_view(name="My pretty view", description="Showing a seaborn plot")
class MyViewNode:
  """
  A view node
  This node shows a plot.
  """
  def configure(self, config_context, input_table_schema)
    pass
  def execute(self, exec_context, table):
    df = table.to_pandas()
    sns.lineplot(x="x", y="y", data=df)
    return knext.view_seaborn()
    # If the node outputs tables, the output view must
    # be the last element of the return value
    #
    # output_table = knext.from_pandas(df)
    # return output_table, knext.view_seaborn()
    #
    # For multiple table outputs use
    # return output_table_1, output_table_2, knext.view_seaborn()
```

### Accessing flow variables 
You can access the flow variables available to the node in both the `configure` and `execute` methods, via the `config_context.flow_variables` and `exec_context.flow_variables` attributes respectively. The flow variables are provided as a dictionary of `flow_variable_name` : `flow_variable_value` mappings, and support the following types:
  * bool
  * list(bool)
  * float
  * list(float)
  * int
  * list(int)
  * str
  * list(str)


By mutating the `flow_variables` dictionary, you can access, modify, and delete existing flow variables, as well as create new ones to be propagated to downstream nodes.
### Versioning your extension 
As you continue to develop your extension after the initial release, you might extend the functionality of your nodes by adding or removing certain parameters. With the versioning capabilities of Python-based node extensions for KNIME Analytics Platform, you can ensure backward compatibility for your users.
As seen in the Python Node Extension Setup section, the `knime.yml` configuration file contains a `version` field. This allows you to assign a version to each iteration of your extension. How closely you want to follow the semantic versioning scheme is completely up to you, but we do require adherence to the following formatting-related rule: versions must be composed of three non-negative numeric parts separated by dots (e.g. `1.0.0`, `0.2.1`, etc.).
|  The version numbers are compared from left to right, i.e. `1.0.1` is newer than `1.0.0`, but older than `1.1.0`.   
---|---  
When adding a new parameter to a node, you should associate it with the corresponding version of your extension. This is done using the `since_version` argument that is now available for all parameter types via the appropriate constructor (e.g. `knext.IntParameter`), as well as parameter groups via the `@knext.parameter_group` decorator. If not specified, the `since_version` argument of a parameter or parameter group defaults to `0.0.0`, which indicates that the parameter was available from the first iteration of the extension.
A common use-case of extension versioning is to facilitate backward compatibility when opening workflows that were created/saved with an older version of the extension installed on the machine. What KNIME Analytics Platform will try to achieve by default in this case, is to combine the values of the previously configured node settings that are still available in the current version of the extension with the newly added node settings, if any. The latter are then automatically set to their default values, and the node remains configured.
|  Sometimes the default value for a newly added node should be different than the default value for a node that is loaded as part of an old workflow (for an example see `double_param` below). In this scenario you can use a `DefaultValueProvider` instead of the default value. The `DefaultValueProvider` is a function that given a `Version` produces the default value of the parameter for that version of the extension. For old workflows it is called with the extension version the workflow was saved with. For new workflows it is called with the current version of the extension.   
---|---  
Here is a minimal functional example of a Python-based extension containing a single node with a single parameter. Since the parameter is available from the initial release of the extension, we can forgo setting the `since_version` argument:
```
"""
My Extension | Version: 0.1.0 | Author: Jane Doe
"""
import knime.extension as knext
@knext.node(
  "My Node",
  knext.NodeType.SOURCE,
  "..icons/icon.png",
  "/"
)
@knext.output_table("Output Data", "Data generated by this node.")
class MyNode:
  """Short node description.
  Long node description.
  """
  my_param = knext.IntParameter(
    "My Param",
    "My int parameter.",
    42,
  )
  def configure(self, config_context, input_table_schema):
    return input_table_schema
  def execute(self, exec_context, input_table):
    df = input_table.to_pandas()
    df['column1'] += self.my_param
    return knext.Table.from_pandas(df)
```

During the next few releases of the extension, `MyNode` is modified with an addition of several new parameters:
```
"""
My Extension | Version: 0.5.0 | Author: Jane Doe
"""
import knime.extension as knext
@knext.node(
  "My Node",
  knext.NodeType.SOURCE,
  "..icons/icon.png",
  "/"
)
@knext.output_table("Output Data", "Data generated by this node.")
class MyNode:
  """Short node description.
  Long node description.
  """
  my_param = knext.IntParameter(
    "My Param",
    "My int parameter.",
    42,
  )
  double_param = knext.DoubleParameter(
    "My Double",
    "Double parameter that strives to be Pi.",
    # For old workflows the value must be 1 to stay backwards compatible
    # but for new workflows we want the default to be 3.14
    lambda v: 1 if v < knext.Version(0, 3, 0) else 3.14,
    since_version="0.3.0",
  )
  string_param = knext.StringParameter(
    "My String",
    "An important string parameter to be turned into a flow variable.",
    "Foo",
    since_version="0.5.0",
  )
  def configure(self, config_context, input_table_schema):
    return input_table_schema
  def execute(self, exec_context, input_table):
    df = input_table.to_pandas()
    df['column1'] += self.my_param * self.double_param
    exec_context.flow_variables['important_string'] = self.string_param
    return knext.Table.from_pandas(df)
```

Now, if a user whose version of `My Extension` is `0.5.0` opens a workflow containing `MyNode` that was configured/saved on a machine where the version of `My Extension` was, for instance, `0.2.0`, the node settings will automatically be adapted to contain the previously configured value for `my_param`, and the default values for `double_param` and `string_param`. If the user were to execute the node without first reconfiguring it, the `execute` method would use those default values for the corresponding parameters.
Note how the default value of `double_param` depends on the version in order to ensure that the node’s output does not change if the workflow is of an older version.
If the behaviour/functionality of the node has changed throughout the various releases of the extension, and you would like to require users to reconfigure the node if certain conditions are met, you can use the `config_context.set_warning()` or `exec_context.set_warning()` methods in the `configure` and `execute` methods of your node respectively to display a yellow "warning" sign in the node status. Additionally, you can raise an exception to further direct the user to reconfigure the node. For example:
```
import knime.extension as knext
@knext.node(
  "My Node",
  knext.NodeType.SOURCE,
  "..icons/icon.png",
  "/"
)
@knext.output_table("Output Data", "Data generated by this node.")
class MyNode:
  """Short node description.
  Long node description.
  """
  my_param = knext.IntParameter(
    "My Param",
    "My int parameter.",
    42,
  )
  double_param = knext.DoubleParameter(
    "My Double",
    "Double parameter that strives to be Pi.",
    lambda v: 1 if v < knext.Version(0, 3, 0) else 3.14,
    since_version="0.3.0",
  )
  def configure(self, config_context, input_table_schema):
    if self.my_param < 10:
      config_context.set_warning("Please reconfigure the node.")
      raise ValueError("My Param cannot be less than 10.")
    return input_table_schema
  def execute(self, exec_context, input_table):
    df = input_table.to_pandas()
    df['column1'] += self.my_param * self.double_param
    return knext.Table.from_pandas(df)
```

### Deprecation of nodes 
Sometimes it is not possible to change a node and stay backwards compatible e.g. if an input or output port is added. If you find yourself in this scenario do the following:
  * Deprecate the old node by setting the `is_deprecated` argument to `true` in the `knime.extension.node` decorator. The node is then no longer listed in the node repository but it can still be loaded in existing KNIME workflows in which it then is also marked as deprecated.
  * Implement a new version of the node that has the same `name` argument in the `knime.extension.node` decorator as the old node.


|  Don’t change the name of the Python class that implements your old node because this name is used as ID by the Analytics Platform to find the node.   
---|---  
### Improving the node description with Markdown 
The description of your node, which is displayed in the _Description_ area of KNIME Analytics Platform when a node is selected, is composed of multiple components. These components come from the descriptions you, as the developer, provide when defining the building blocks of the node, such as the input ports or the configuration parameters.
|  Keep in mind that at the first line of the description docstring, next to the three double quotes, you can provide a short description, which will be shown in the overview when clicking on a category in the node repository of the KNIME Analytics Platform.   
---|---  
By including the `markdown` Python package in the `conda` environment associated with your node extension, you can make use of Markdown syntax when writing these descriptions to improve readability and the overall look of your nodes' documentation.
Below you can find a list of which Markdown syntax is supported for each node description element.
|  As KNIME Analytics Platform transitions to the Modern UI, we will work on extending our support for additional Markdown syntax.   
---|---  
Table 1. The supported Markdown syntax for the available node description components Element | Node description | Port description | Parameter description | Top-level parameter group description  
---|---|---|---|---  
Heading |  ✓ |  ✗ |  ✗ |  ✗  
Bold |  ✓ |  ✓ |  ✓ |  ✓  
Italic |  ✓ |  ✓ |  ✓ |  ✓  
Ordered List |  ✓ |  ✓ |  ✓ |  ✓  
Unordered List |  ✓ |  ✓ |  ✓ |  ✓  
Code |  ✓ |  ✓ |  ✓ |  ✓  
Fenced code blocks |  ✓ |  ✓ |  ✓ |  ✗  
Horizontal Rule |  ✓ |  ✓ |  ✗ |  ✗  
Link |  ✓ |  ✓ |  ✓ |  ✓  
Table |  ✓ |  ✗ |  ✗ |  ✗  
Here is a functional example of using Markdown when writing a Python node:
```
import knime.extension as knext
@knext.parameter_group("Node settings")
class Settings:
  """
  Settings to configure how the node should work with the provided **JSON** strings.
  """
  class LoggingOptions(knext.EnumParameterOptions):
    NONE = ("None", "Logging *disabled*.")
    INFO = ("Info", "Allow *some* logging messaged to be displayed.")
    VERBOSE = ("Verbose", "Log *everything*.")
  logging_verbosity = knext.EnumParameter(
    "Logging verbosity",
    "Set the node logging verbosity during execution.",
    LoggingOptions.INFO.name,
    LoggingOptions,
  )
  discard_missing = knext.BoolParameter(
    "Discard rows with missing values",
    """
    Use this option to discard rows with missing values.
    - If **enabled**, the node will ignore rows where an attribute of the JSON strings has missing value.
    - If **disabled**, the node will keep such rows with the corresponding missing values.
    """,
    True,
  )

@knext.node("JSON Parser", knext.NodeType.MANIPULATOR, "icon.png", main_category)
@knext.input_table(
  "Input table",
  """
  Input table containing JSON-encoded strings in each row.
  Example format of the expected input:
  ```
  {
    "Konstanz": {
      "population": 90,000,
      "region": "Baden-Württemberg",
      ...
    },
    ...
  }
  ```
  """,
)
@knext.output_table(
  "Parsed JSON",
  "Output table containing columns with the information extracted from the provided JSON string.",
)
class JsonParser:
  """Node for parsing JSON strings.
  Given a table containing [JSON](https://developer.mozilla.org/en-US/docs/Glossary/JSON) strings, this node attempts to parse them and
  outputs the extracted information in a new table.
  | Allowed | Not allowed |
  | ------- | ----------- |
  | JSON | YAML |
  """
  settings = Settings()
  def configure(self, config_context, input_table_schema):
    # configuration routine
    # ...
    return input_table_schema
  def execute(self, exec_context, input_table):
    # execution routine
    # ...
    return input_table
```

Below is the resulting node description as seen in KNIME Analytics Platform:
![04 full markdown description](https://docs.knime.com/latest/pure_python_node_extensions_guide/img/04_full_markdown_description.png)
The descriptions of individual node parameters can additionally be accessed from within the configuration dialog of the node:
![04 markdown in config dialog](https://docs.knime.com/latest/pure_python_node_extensions_guide/img/04_markdown_in_config_dialog.png)
## Share your extension 
You can share your extension in two ways. One is to bundle the extension to get a local update site which can be shared with your team or used for testing. The other is to publish it on KNIME Community Hub and make it available for the community. Either of the two options need some setup details. In this section, the setup and the two options will be explained.
### Setup 
To ensure that the users you have shared your extension with are able to utilise its functionality fully and error-free, we bundle the source files together with the required packages using `conda` as the bundling channel.
The `knime.yml` file (refer to the Python Node Extension Setup section for an example of this configuration file) contains the information required to bundle your extension, including:
  * `extension_module`: the name of the `.py` file containing the node definitions of your extension.
  * `env_yml_path`: the path to the `.yml` file containing the configuration of the `conda` environment that is used with your extension (see example below).


These YAML files can be automatically generated by activating the desired environment, and running one of the following commands, which will result in configuration files of various strictness:
  * `conda env export > <env_yml_filename.yml>`, which will contain all the dependencies with their full version and build numbers. **Not recommended**
  * `conda env export --from-history > <env_yml_filename.yml>`, which will reduce the list of dependencies down to the packages that you have manually installed in the environment. Note that this option does not preserve the list of manually specified channels when installing packages (e.g. _conda-forge_), so you might have to add them yourself.
  * `conda env export | cut -f 1 -d "=" | grep -v "prefix" > <env_yml_filename.yml>`, which will preserve the list of custom channels used when installing packages, as well as the full list of dependencies, **without** strict versions specified.
  * `conda env export --no-builds | grep -v "prefix" > <env_yml_filename.yml>`, same as the above command, but with package versions specified (excluding build numbers).


Note that, in addition to packages installed with `conda`, you are also able to install packages from _PyPI_ via `pip`. When done from within your activated `conda` environment, such packages are also automatically included in the YAML configuration file generated with the above commands.
#### `environment.yml`: 
```
name: knime-python-scripting
channels:
- conda-forge
- knime
dependencies:
- python=3.11       # base dependency
- knime-python-base>=5.4 # base dependency
- knime-extension>=5.4  # base dependency
- another-package=1.0.1  # example
- yet-another-package   # example
- pip:
  - img2text      # example
  - pillow       # example
```

#### OS-specific environments 
Since KNIME Analytics Platform is available on Windows, Linux, and macOS, you should try your best to ensure that your Python extension performs as expected on all platforms. To achieve this, you can generate OS-specific YAML files that include versions/replacements of packages that are guaranteed to be available on this particular OS by, for instance, searching the Anaconda package repository with the _Platform_ filter set to the desired OS (e.g. `osx-64` for Intel-based Mac machines), and correspondingly building the environment YAML file.
When specifying the environment YAMLs in the `knime.yml` file of your Python extension, you can use the following format to include different environment configuration files for different operating systems, which the KNIME Analytics Platform will then appropriately use together with your extension:
|  Support for Apple Silicon-specific environments is available starting from the 4.7 release of KNIME Analytics Platform.   
---|---  
#### `knime.yml`: 
```
...
env_yml_path:
 osx-64: <env_for_intel_mac>
 osx-arm64: <env_for_arm_mac> # available starting from KNIME Analytics Platform 4.7
 linux-64: <env_for_linux>
 win-64: <env_for_win>
...
```

Lastly, a new extension needs a `LICENSE.TXT` that will be displayed during the installation process.
### Option 1: Bundling a Python extension to share a zipped update site 
Once you have finished implementing your Python extension, you can bundle it, together with the appropriate `conda` environment, into a local update site. This allows other users to install your extension in the KNIME Analytics Platform.
Follow the steps of `extension setup`. Once you have prepared the YAML configuration file for the environment used by your extension, and have set up the `knime.yml` file, you can proceed to generating the local update site.
We provide a special `conda` package, `knime-extension-bundling`, which contains the necessary tools to automatically build your extension. Run the following commands in your terminal (Linux/macOS) or Anaconda Prompt (Windows). They will setup a `conda` environment, which gives the tools to bundle extensions. Then the extension will be bundled.
|  By default, the conda environment will bundle the extension for the latest KNIME Analytics Platform version. If you want to bundle the extension for a specific KNIME version, you have to install the corresponding conda package. You can specify the version when you create the environment , e.g. `knime-extension-bundling=5.4`. When building an older version, the environment YAML files **must** contain the corresponding versions of the `knime-python-base` and `knime-extension` packages, e.g.`- knime-python-base=5.3` when bundling for version 5.3.   
---|---  
  1. Create a fresh environment prepopulated with the `knime-extension-bundling` package:
```
conda create -n knime-ext-bundling -c knime -c conda-forge knime-extension-bundling=5.4
```

  2. Activate the environment:
```
conda activate knime-ext-bundling
```

  3. With the environment activated, run the following command to bundle your Python extension:
     * macOS/Linux:
```
build_python_extension.py <path/to/directoryof/myextension/> <path/to/directoryof/output>
```

     * Windows:
```
build_python_extension.bat <path/to/directoryof/myextension/> <path/to/directoryof/output>
```

where `<path/to/directoryof/myextension/>` is the path to the directory containing the `knime.yml` file, and `<path/to/directoryof/output>` is the path to the directory where the bundled extension **repository** will be stored.
Further instructions are given by `build_python_extension.py --help` (macOS, Linux) or `build_python_extension.bat --help` (Windows) and will be outlined upon execution of the script.
|  The bundling process can take several minutes to complete.   
---|---  
  4. Add the generated **repository** folder to KNIME AP as a Software Site in _File → Preferences → Install/Update → Available Software Sites_
  5. Install it via _File → Install KNIME Extensions_


The generated repository can now be shared with and installed by other users.
### Option 2: Publish your extension on KNIME Community Hub 
Once you have finished implementing your Python extension, you can share it, together with the appropriate `conda` environment, to KNIME Community Hub.
#### Provide the extension 
Follow the steps of `extension setup` to prepare the `environment.yml` or some other `yml` defining your Python environment and the `knime.yml`.
Upload your extension into a Git repository, where the `knime.yml` is found top-level. A `config.yml` is not needed.
Some recommended project structure:
```
https://github.com/user/my_knime_extension
├── icons
│  └── my_node_icon.png
├── knime.yml
├── LICENSE.txt
├── environment.yml
└── my_extension.py
```

#### Write a test workflow 
  1. Install the `KNIME Testing Framework` to your KNIME Analytics Platform (KAP)
  2. Create a test workflow (see https://www.knime.com/blog/enter-the-era-of-automated-workflow-testing-and-validation for details)
  3. Test your extension against the test workflow: does it check your functionality and behaves as expected?


#### Contribute 
Follow the steps in the following guide: Publish Your Extension on KNIME Community Hub
#### Lean back, clean up 
  1. Wait for us to come back to you
  2. If it is available on the nightly experimental community extension Hub, please test it again (with your test workflow) by using the nightly experimental update site: https://update.knime.com/community-contributions/trunk (for now, every Python extension will stay on that site)
  3. Upload the test workflow onto the Community Workflow Server. You can access the server via the KNIME Explorer view. If you don’t have a mount point entry for the community server yet, click on the button at the top-right of the view and then on _Configure Explorer settings_ in the appearing dialog. Now create a new mount point with a custom ID and _KNIME Community Server_ as mount point type. You can log into the server using your forum credentials, if you got your requested `community contributor status`. Create a new workflow group inside _Testflows/trunk_ , give it a meaningful name, and finally upload your workflow(s) into this group. Please make sure that the permissions on the group and the workflow(s) allow read access for everyone.


## Customizing the Python executable 
Some extensions might have additional requirements that are not part of the bundled environment e.g. in case of third party models. For these extensions, it is possible to overwrite the Python executable used for execution. This can be done via the system property `knime.python.extension.config` that has to point to a special YAML file on disc. Add it to your knime.ini with the following line:
`-Dknime.python.extension.config=path/to/your/config.yml`
|  The forward slash `/` has to be used on all OS, also on Windows.   
---|---  
The format of the YAML is:
```
id.of.first.extension:
 conda_env_path: path/to/conda/env
id.of.second.extension:
 python_executable: path/to/python/executable
```

You have two options to specify a custom Python exectuable:
  * Via the `conda_env_path` property (recommended) that points to a `conda` environment on your machine.
  * Via the `python_executable` property that points to an executable script that starts Python (see Manually configured Python environments section in KNIME Python Integration Guide for more details).


If you specify both, then `conda_env_path` will take precedence. It is your responsibility to ensure that the Python you specified in this file has the necessary dependencies to run the extension. As illustrated above, you can overwrite the Python executable of multiple extensions.
## Registering Python extensions during development 
In order to register a Python extension you are developing, you can add it to the `knime.python.extension.config` YAML explained above by adding a src property:
```
id.of.your.dev.extension:
 src: path/to/your/extension
 conda_env_path: path/to/conda/env
 debug_mode: true
```

Note that you have to specify either `conda_env_path` or `python_executable` because the Analytics Platform doesn’t have a bundled environment for your extension installed. For debugging it is also advisable to enable the debug mode by setting `debug_mode: true`. The debug mode disables caching of Python processes which allows some of your code changes to be immediately shown in the Analytics Platform. Those changes include:
  * Changes to the execute and configure runtime logic.
  * Changes to existing parameters e.g. changing the `label` argument.
  * Other changes, such as adding a node or changing a node description, require a restart of the Analytics Platform to take effect.
  * Last but not least, fully enabling and disabling the debug mode also requires a restart.


## Other Topics 
### Logging 
You can use the logging Python module to send warnings and errors to the KNIME Analytics Platform console. By going to _File → Preferences → KNIME → KNIME GUI_ , you can choose the Console View Log Level. Each consecutive level includes the previous levels (i.e. `DEBUG` will also allow message from `INFO`, `WARN`, and `ERROR` to come through in the console, whereas `WARN` will only allow `WARN` and `ERROR` levels of messages).
In your Python script, you can initiate the logger, and use it to send out messages to the KNIME Analytics Platform console as follows:
```
# other various imports including knime.extension
import logging
LOGGER = logging.getLogger(__name__)
# your node definition via the knext decorators
class MyNode:
  # your configuration dialog parameter definitions
  def configure(…):
    …
    LOGGER.debug("This message will be displayed in the KNIME Analytics Platform console at the DEBUG level")
    LOGGER.info("This one will be displayed at the INFO level.")
    LOGGER.warning("This one at the WARN level.")
    LOGGER.error("And this will be displayed as an ERROR message.")
    …
  def execute(…):
    …
    LOGGER.info("Logger messages can be inserted anywhere in your code.")
    …
```

### Gateway caching 
In order to allow for a smooth user experience, the Analytics Platform caches the gateways used for non-execution tasks (such as the spec propagation or settings validation) of the last used Python extensions. This cache can be configured via two system properties:
  * `knime.python.extension.gateway.cache.size`: controls for how many extensions the gateway is cached. If the cache is full and a gateway for a new extension is requested, then the gateway of the least recently used extension is evicted from the cache. The default value is 3.
  * `knime.python.extension.gateway.cache.expiration`: controls the time period in seconds after which an unused gateway is removed from the cache. The default is 300 seconds.


The `debug_mode: true` propertly of `config.yml` discussed before effectively disables caching for individual extensions. By default, all extensions use caching.
## Troubleshooting 
In case you run into issues while developing pure-Python nodes, here are some useful tips to help you gather more information and maybe even resolve the issue yourself. In case the issues persist and you ask for help, please include the gathered information.
|  Please have a look at the KNIME Log.   
---|---  
|  Have also a look at the troubleshoot section of the Python integration guide.   
---|---  
### Find debug information 
_Resourceful information helps in understanding issues. Relevant information can be obtained in the following ways._
#### Accessing the KNIME Log 
The `knime.log` contains information logged during the execution of nodes. To obtain it, there are two ways:
  * In the KNIME Analytics Platform: `View → Open KNIME log`
  * In the file explorer: `<path-to-knime-workspace>/.metadata/knime/knime.log`


Not all logged information is required. Please restrict the information you provide to the issue. If the log file does not contain sufficient information, you can change the logging verbosity in `File → Preferences → KNIME`. You can even log the information to the console in the KNIME Analytics Program: `File → Preferences → KNIME → KNIME GUI`.
#### Information about the Python environment 
If `conda` is used, obtain the information about the used Python environment `<python_env>` via:
  1. `conda activate <python_env>`
  2. `conda env export`


### How to update Python version 
In step 4 of the tutorial an environment is created which you use for your extension.
Three modules are specified for the installation:
  1. `knime-extension` brings in all the necessary API files such that you can use code-completion in your editor, if the environment is activated there.
  2. `knime-python-base` is a metapackage which brings in dependencies like pyarrow and pandas etc, which are necessary for interacting with the KNIME Analytics Platform. If you look at the files on Anaconda.org you see that we provide _knime-python-base_ up to Python 3.11.
  3. Python lets you specify the version. as you can see in 2., the version range made available by `knime-python-base` is 3.8-3.11.


You can create an environment with a more recent Python version as follows:
`conda create -n my_python_env python=3.11 knime-python-base knime-extension -c knime -c conda-forge`
### Develop multiple extensions at once 
If you want to develop and test multiple extensions simultaneously in your KNIME Analytics Platform, you can alter the `config.yml` (see step 5 of the tutorial)to contain the necessary information of additional extensions like this:
```
<first_extension_id>:
  src: <path/to/folder/of/first_extension>
  conda_env_path: <path/to/my_python_env>
  debug_mode: true
<second_extension_id>:
  src: <path/to/folder/of/second_extension>
  conda_env_path: <path/to/my_other_python_env>
  debug_mode: true
```

|  The indentation is necessary and needs to be the same in every indented line, e.g. 2 or 4 spaces.   
---|---  
### Errors during load 
If during development you get an error similar to
```
Loading model settigns failed: Parameter missing for key <some_key>
```

then this is probably because you freshly introduced the parameter. Re-executing the node should solve this. Alternatively, drag and drop the node again from the node repository.
|  During devleopment, you need to drag and drop the nodes always into your workflow whenever you change someting outside the `execute` or `configure` method.   
---|---  
### Column is of type long, but int was wanted 
Due to inconsistencies of the different Operating Systems, integer columns in the output table can be of type long. To prevent that, follow this example:
```
def execute(self, exec_context, input_table):
  import numpy as np
  df = input_table.to_pandas()
  # Let's assume df has a column 'column1'
  df['column1'] = df['column1'].astype(np.int32)
  return knext.Table.from_pandas(df)
```

### LZ4/jnijavacpp.dll/Columnar Table Backend error 
On Windows, the following two errors can happen if you have two KNIME Analytics Platform versions open and both use the Columnar table backend. Close both and start only one.
```
ArrowColumnStoreFactory : : : Failed to initialize LZ4 libraries. The Columnar Table Backend won't work properly.
java.lang.UnsatisfiedLinkError: java.io.FileNotFoundException: C:...\.javacpp\cache\windows-x86_64\jnijavacpp.dll (The process cannot access the file because it is being used by another process)
```

```
ERROR : KNIME-Worker-3-Data Generator 3:18 : : Node : Data Generator : 3:18 : Execute failed: Unable to create DataContainerDelegate for ColumnarTableBackend.
java.lang.IllegalStateException: Unable to create DataContainerDelegate for ColumnarTableBackend.
	at org.knime.core.data.columnar.ColumnarTableBackend.create(ColumnarTableBackend.java:115)
...
...
...
Caused by: java.lang.UnsatisfiedLinkError: java.io.FileNotFoundException: C:...\.javacpp\cache\windows-x86_64\jnijavacpp.dll (The process cannot access the file because it is being used by another process)
```

### Could not create instance error 
The following error can appear if the extension was built with `build_python_extension` for a newer KNIME Analytics Platform (KAP) version. Run the `build_python_extension` script with the parameter for the specific KAP version or an older KAP version, e.g. `build_python_extension.py --knime-version 5.4`.
```
ERROR CoreUtil Could not create instance of node org.knime.python3.nodes.extension.ExtensionNodeSetFactory$DynamicExtensionNodeFactory: Could not initialize class org.knime.python3.nodes.CloseablePythonNodeProxy
```

### SSL error during execution 
If you encounter an SSL error during the execution of a node, this might be due to the use of a self-signed certificate. If other nodes such as the GET Request node work, but the Python node does not, you can configure the Python nodes to trust the same certificates as the KNIME Analytics Platform. To do this, add the following line to your `knime.ini` file:
```
-Dknime.python.cacerts=AP
```

This will point the `CA_CERTS` and `REQUESTS_CA_BUNDLE` environment variables to a newly created CA bundle that contains the certicate authorities that the KNIME Analytics Platform trusts. The Python nodes will then trust the same certificates as the KNIME Analytics Platform.
## Installation Troubleshooting 
This chapter addresses common installation challenges associated with our Python-based KNIME Extensions. It provides solutions and advice to help users manage and resolve these issues effectively, aiming for a straightforward setup process.
### Offline installation 
For performance, we no longer bundle Python packages in Python-based extensions. Therefore, if you wish to install Python-based extensions in an offline ("air-gapped") environment, please follow these steps in addition to adding an offline update site:
  1. Install/Run a (temporary) KNIME Analytics Platform on a system that has internet access.
  2. Install all wanted extensions.
  3. Navigate to the preferences page "Python-based Extensions" via the cogwheel (or, in the classic UI: _File → Preferences → KNIME → Python-based-Extensions_) and click _"Download required packages for offline installation to"_. Select an empty folder into which the packages will be saved. After selecting a folder, KNIME will collect the required Python packages and download them to the chosen folder.
  4. After the download completes, copy this folder to the target offline system.
  5. On the target offline system set the environment variable `KNIME_PYTHON_PACKAGE_REPO_URL` to the folder with the downloaded packages.
  6. Fully close KNIME. After starting up again, KNIME will now use the provided packages for the installation of Python-based extensions.


|  If you are unsure if this procedure is necessary for the desired Python-based extensions just try to run the installation on the target offline system without setting the environment variable. The installation will fail with an error linking to this documentation section if the steps above are required. Alternatively, run steps 1-3 and check if any packages were downloaded.   
---|---  
|  _Technical detail_ : Python-based extensions set up a conda environment with the necessary conda and pip packages during installation. These packages are either bundled with the extension or downloaded during installation. If the extension bundles the packages it is possible to install it from a zipped update-site on a system that has no internet access. If the extension does not bundle the packages the extra steps described above are required for an offline installation.   
---|---  
### Custom conda environment location in case of Windows long path installation problems 
Python-based extensions install a dedicated conda environment containing the Python packages required for this extension. By default, KNIME will create these conda environments at the location: '<KNIME-HOME>/bundling/envs/<EXTENSION-NAME>' for Linux and Mac and '<KNIME-HOME>\bundling\envs\<EXTENSION-NAME>' for Windows. However, it is possible to change the directory where the conda environments are created by setting the environment variable `KNIME_PYTHON_BUNDLING_PATH` to the desired directory. This can be useful to mitigate installation problems due to the limitation of path lengths in Windows.
|  Conda environments located at this path will be overwritten when installing an extension with the same name. Also when uninstalling an extension, the conda environment will be deleted.   
---|---  
|  When changing this environment variable, previously installed extensions that rely on a Python environment may stop working. It is recommended that you only set this variable for new KNIME installations.   
---|---  
### Proxy Issues 
If you are behind a proxy, you might encounter issues when trying to install Python-based KNIME extensions.
As Python-based KNIME extensions are installed via conda, you need to set the proxy settings in the conda configuration. The conda configuration file is located in the `~/.condarc` file. More information about this file can be found in the following tutorial: How to use CondaRC.
To make sure the Python-based extension installation respects your proxy settings, we recommend adding the following lines to the `~/.condarc` file:
```
proxy_servers:
 http: http://domainname\username:password@proxyserver:port
 https: http://domainname\username:password@proxyserver:port
```

These will ensure that the proxy settings are correctly set for conda. Proxy settings from KNIME will unfortunately not be propagated for the installation, as this could overwrite the existing proxy settings for conda.
|  Do not use tabs, there must be a space between `http:` and `http://…​;`. If you don’t need a username and password for the proxy, leave out the `username:password`.   
---|---  
If you run into the following error:
```
critical libmamba Download error (35) SSL connect error [https://conda.anaconda.org/conda-forge/noarch/<some package here...>]
  schannel: next InitializeSecurityContext failed: Unknown error (0x80092012) - The revocation function was unable to check revocation for the certificate.
```

try adding `ssl_no_revoke: true` to your `~/.condarc` file. The error means that your proxy is not configured to forward SSL certificate revocation checks.
If none of these tips work, you can also perform an offline installation of the extension as described in: Offline Installation.
  * Introduction
  * Quickstart Tutorial
    * Prerequisites
    * Writing your first Python node from scratch
  * Python Node Extension Setup
    * Development and distribution
  * Defining a KNIME Node in Python: Full API
    * Defining custom port objects
    * Node port configuration
    * Specifying the node category
    * Defining the node’s configuration dialog
    * Node view declaration
    * Accessing flow variables
    * Versioning your extension
    * Deprecation of nodes
    * Improving the node description with Markdown
  * Share your extension
    * Setup
    * Option 1: Bundling a Python extension to share a zipped update site
    * Option 2: Publish your extension on KNIME Community Hub
  * Customizing the Python executable
  * Registering Python extensions during development
  * Other Topics
    * Logging
    * Gateway caching
  * Troubleshooting
    * Find debug information
    * How to update Python version
    * Develop multiple extensions at once
    * Errors during load
    * Column is of type long, but int was wanted
    * LZ4/jnijavacpp.dll/Columnar Table Backend error
    * Could not create instance error
    * SSL error during execution
  * Installation Troubleshooting
    * Offline installation
    * Custom conda environment location in case of Windows long path installation problems
    * Proxy Issues


Download PDF
## Connect
  * News
  * Blog
  * Events
  * Forum
  * KNIME Hub


## Software
  * KNIME Analytics Platform
  * KNIME Business Hub
  * KNIME Extensions
  * KNIME Integrations
  * Community Extensions
  * Partner Extensions


## Knowledge Base
  * Getting Started
  * Developer
  * White Papers
  * Learning Hub


## Quick Links
  * Download
  * KNIME Open Source Story
  * Open for Innovation


## Legal
  * Trademarks
  * Imprint
  * Privacy


## KNIME AG
  * Talacker 50
  * 8001 Zurich
  * Switzerland


© Copyright 2024 - KNIME AG - All Rights Reserved.

